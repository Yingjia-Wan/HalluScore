{"question": "I am a 21 year old girl and got my period twice in the same month. this is the first time this happened, why might it be the case.", "prompt_source": "ShareGPT", "response": "Experiencing two periods in one month can be concerning, but there are several potential reasons why this might happen. Here are some common causes:\n\n1. **Hormonal Imbalance**: Fluctuations in hormones such as estrogen and progesterone can cause irregular menstrual cycles. This can be due to stress, changes in weight, or other underlying health conditions.\n\n2. **Birth Control**: If you have recently started, stopped, or changed your method of birth control, it can affect your menstrual cycle. Hormonal contraceptives, in particular, can cause irregular bleeding.\n\n3. **Stress**: High levels of stress can impact your menstrual cycle by affecting the hormones that regulate it.\n\n4. **Polycystic Ovary Syndrome (PCOS)**: PCOS is a common condition that can cause irregular periods, including having more than one period in a month.\n\n5. **Thyroid Issues**: Both hyperthyroidism and hypothyroidism can affect your menstrual cycle.\n\n6. **Uterine Fibroids or Polyps**: These are non-cancerous growths in the uterus that can cause irregular bleeding.\n\n7. **Infections**: Pelvic inflammatory disease (PID) or other infections can cause irregular bleeding.\n\n8. **Perimenopause**: Though less common at your age, some women can start experiencing perimenopause symptoms earlier, which can lead to irregular periods.\n\n9. **Lifestyle Factors**: Significant changes in diet, exercise, or sleep patterns can also affect your menstrual cycle.\n\n10. **Medical Conditions**: Other medical conditions, such as diabetes or eating disorders, can also impact your menstrual cycle.\n\nIf this is a one-time occurrence, it might not be a cause for concern. However, if you continue to experience irregular periods or have other symptoms, it would be a good idea to consult with a healthcare provider. They can help determine the underlying cause and recommend appropriate treatment or management strategies.", "prompt_tok_cnt": 2426, "response_tok_cnt": 258, "model": "gpt-4o", "abstained": false, "predefined_stats_lst": [[1, 15, 0]], "pre_supported_claims_lst": ["Hormonal imbalances can cause irregular menstrual cycles in women.", "Changes in weight can lead to fluctuations in hormones that affect menstrual cycles.", "Recently starting, stopping, or changing a method of birth control can affect menstrual cycles.", "High levels of stress can impact menstrual cycles by affecting the hormones that regulate it.", "Polycystic Ovary Syndrome (PCOS) can cause irregular periods in women.", "Hyperthyroidism can affect menstrual cycles.", "Hypothyroidism can affect menstrual cycles.", "Uterine fibroids can cause irregular bleeding in women.", "Uterine polyps can cause irregular bleeding in women.", "Pelvic inflammatory disease (PID) can cause irregular bleeding in women.", "Significant changes in diet can affect menstrual cycles.", "Significant changes in exercise can affect menstrual cycles.", "Significant changes in sleep patterns can affect menstrual cycles.", "Medical conditions such as diabetes can impact menstrual cycles.", "Medical conditions such as eating disorders can impact menstrual cycles."], "pre_unsupported_claims_lst": [], "claim_to_verify_list": [["Perimenopause can lead to irregular periods in women."]], "all_claims_to_verify": ["Perimenopause can lead to irregular periods in women."], "claim_search_results": {"Perimenopause can lead to irregular periods in women.": [{"text": "Home/\nHealth Library/\nDiseases & Conditions/Perimenopause\n\n\nAdvertisement\nAdvertisement\n\nPerimenopause\n\n\nPerimenopause is the transitional period before menopause. During perimenopause, levels of estrogen start to decrease. You may begin having menopause-like symptoms, such as hot flashes or irregular periods. Perimenopause can last for years.\n\n\n\n\n\n\nAdvertisement\n\n\n\nCleveland Clinic is a non-profit academic medical center. Advertising on our site helps support our mission. We do not endorse non-Cleveland Clinic products or services. Policy\n\n\n\nCare at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\nContentsOverviewSymptoms and CausesDiagnosis and TestsManagement and TreatmentPreventionOutlook / PrognosisLiving WithAdditional Common Questions\n\n\n\nContentsOverviewSymptoms and CausesDiagnosis and TestsManagement and TreatmentPreventionOutlook / PrognosisLiving WithAdditional Common Questions\n\n\n\nOverview\n\nPerimenopause is when you transition to menopause. You may have symptoms like irregular periods, changes in mood and hot flashes.\n\nWhat is perimenopause?\nPerimenopause is when your body starts transitioning to menopause. During this transition, your ovaries begin producing less hormones, causing your menstrual cycle to become erratic or irregular. In perimenopause, your body is moving toward the end of your reproductive years. It\u2019s a natural and normal progression, but it comes with both physical and emotional symptoms. Some of these symptoms can disrupt your life or make you uncomfortable.\n\n\nAdvertisement\n\n\n\nCleveland Clinic is a non-profit academic medical center. Advertising on our site helps support our mission. We do not endorse non-Cleveland Clinic products or services. Policy\nPerimenopause may begin as early as your mid-30s or as late as your mid-50s. Some people are in perimenopause for only a short time, while others are in it for several years. Even though your menstrual cycles are unpredictable and your hormone levels are declining, it\u2019s still possible to get pregnant in perimenopause.\nPerimenopause is a transitional time that ends in menopause. Menopause means your periods have ended. When you have no menstrual cycle for 12 months in a row, you have officially reached menopause.\nWhat are the hormonal changes during perimenopause?\nThe hormonal changes you experience during perimenopause are mostly due to declining estrogen levels. Your ovaries make estrogen, which plays a vital role in maintaining your reproductive system. As estrogen decreases, it throws off the balance with progesterone, another hormone your ovaries produce. It\u2019s common for hormone levels to fluctuate during perimenopause \u2014 to go up and down like a rollercoaster.\nWhen you reach menopause, your body makes so little estrogen that your ovaries no longer release eggs (ovulation). At this point, you stop having your period and you lose the ability to get pregnant.\nWhat is the typical age for perimenopause to start?\nPerimenopause begins about eight to 10 years before menopause. It usually starts in your mid-40s, but it can start earlier. Completing menopause before age 40 is called premature menopause. Some medical conditions or procedures cause early menopause. If there\u2019s no medical or surgical cause for premature menopause, it\u2019s called primary ovarian insufficiency.\n\n\nAdvertisement\n\n\nHow long does perimenopause last?\nThe average length of perimenopause is about four years, but it can last up to eight years. Some people may only be in this stage for a few months, while others will be in this transition phase for several years.\nHow do you know you\u2019re in perimenopause?\nIt\u2019s important to know that everyone\u2019s body is different, and everyone experiences signs of perimenopause differently.\nGenerally, the first sign of perimenopause is irregular periods. You may go from having predictable menstrual cycles to spotting randomly or missing periods altogether. A lot of people also experience the most common signs of menopause like hot flashes and vaginal dryness early into the menopause transition.\n\n\n\n\nSymptoms and Causes\n\nWhat are the symptoms of perimenopause?\nOnce your estrogen levels begin to decline, your body must adjust to the changes in hormones. How your body reacts to changing hormones can be different from someone else\u2019s, like how your friend\u2019s body reacts. Some people don\u2019t notice any physical changes other than skipped periods, while other people have symptoms like hot flashes and mood swings.\nEven though the symptoms vary, many people experience at least one of the following:\n\nIrregular periods or skipping periods.\nPeriods that are heavier or lighter than usual.\nChanges in mood like irritability, depression or mood swings.\nLow libido.\nHot flashes.\nNight sweats.\nVaginal dryness that causes discomfort during sex.\nUrinary urgency (needing to pee more frequently).\nSleep problems (insomnia).\n\n\n\nBecause some of these symptoms resemble symptoms of other conditions, it\u2019s a good idea to talk to your healthcare provider about what you\u2019re experiencing. They can confirm that the symptoms are due to perimenopause and not another health condition.\nWhat does perimenopause anxiety feel like?\nThere are only a few studies about anxiety specific to perimenopause. But healthcare providers do know that mental health conditions like depression are common in perimenopause. Many people also report feeling more anxious in perimenopause. Anxiety is a constant worry that may interrupt your daily life. Symptoms like sweating, nausea or muscle tension can also happen with anxiety. Perimenopause anxiety can make it hard to sleep and complete your daily routine. Talk to your healthcare provider if perimenopause is affecting your emotional well-being. They can recommend treatment with medication or behavioral therapy.\nWhat are periods like during perimenopause?\nYour body is producing less of the hormones that help you ovulate, so your periods can become irregular. Your menstrual cycle could become longer or shorter than usual. Your bleeding could also be heavier or lighter than normal. Some people also notice their premenstrual syndrome (PMS) symptoms get worse.\nDo you still ovulate during perimenopause?\nIf you\u2019re still getting a period, even an irregular one, you\u2019re still ovulating. Until you haven\u2019t menstruated for 12 consecutive months, you should assume your body is still ovulating.\n\n\nAdvertisement\n\n\nHow can I reduce my risk of perimenopause complications?\nIrregular periods are common and normal during perimenopause, but other conditions can also cause irregular vaginal bleeding. If any of the following apply to you, see a healthcare provider to rule out other causes:\n\nYour periods become very heavy or contain large blood clots.\nYou change pads or tampons every one to two hours.\nYour periods last several days longer than usual.\nYou spot or bleed between periods.\nYou bleed after sex.\nYour periods occur less than 21 days apart.\nYour provider may want to rule out other causes of abnormal vaginal bleeding. Potential causes of abnormal bleeding include:\n\nInfection.\nFibroids.\nBlood clotting disorders.\nPolyps.\nCancer.\n\n\nDiagnosis and Tests\n\nHow is perimenopause diagnosed?\nA healthcare provider may determine that you\u2019re transitioning to menopause based on a physical exam and your symptoms, age and medical history.\nBut you don\u2019t always need to see a healthcare provider for an official diagnosis. Many people notice the changes in their bodies but don\u2019t have severe symptoms. Other people end up contacting their healthcare provider because symptoms become intolerable and uncomfortable.\nWhat tests can diagnose perimenopause?\nHormone testing isn\u2019t necessary to diagnose perimenopause. Hormone levels fluctuate so much that the tests aren\u2019t reliable.\n\n\nAdvertisement\n\n\nBut certain hormone tests can tell your provider you\u2019re in perimenopause. For example, testing your FSH (follicle-stimulating hormone) level is one to see if perimenopause is beginning. FSH is a hormone your pituitary gland produces. It stimulates your ovaries to release an egg during ovulation.\nA consistently high level of FSH can indicate you\u2019re getting near menopause. But FSH tests can be misleading because during perimenopause, your hormones rise and fall erratically. Certain medications and health conditions interfere with hormone levels and will affect the results of any hormone tests.\nThere aren\u2019t any imaging tests that can confirm perimenopause, but your provider may order tests to rule out other conditions.\n\n\nManagement and Treatment\n\nCan perimenopause be treated?\nThere isn\u2019t any treatment to stop perimenopause. Perimenopause is a natural part of life. It ends when your periods stop entirely, and you reach menopause.\nBut your healthcare provider may recommend ways to help ease uncomfortable symptoms. Many people don\u2019t need medication and find that their symptoms are mild enough that lifestyle changes alone make a big difference. Still, other people find that taking medication relieves their symptoms and improves their quality of life.\n\n\nAdvertisement\n\n\nYour provider may recommend the following medications:\n\nAntidepressants: These medications help with hot flashes, mood swings, anxiety or depression.\nBirth control pills: These medications stabilize your hormone levels and typically relieve symptoms.\nHormone replacement therapy (HRT): HRT uses estrogen or estrogen and progesterone to increase your hormone levels if you\u2019ve gone into menopause before age 40.\nHormone therapy (HT): Similar to HRT, this therapy uses estrogen or estrogen and progesterone to increase your hormones. Providers use this term when referring to people going through menopause at a more natural age (any time after age 45).\nGabapentin (Neurontin\u00ae): This medicine is a seizure medication that also relieves hot flashes for most people.\nOxybutynin: A medication for overactive bladder that can also relieve hot flashes.\nFezolinetant (VEOZAH\u2122): This is a newer treatment for severe hot flashes.\nVaginal creams: Your provider can tell you about prescription and over-the-counter options. Treatment with vaginal creams and lubricants can decrease pain during sex and relieve vaginal dryness.\nYour healthcare provider will discuss the risks and benefits of perimenopause treatment with you and recommend the best option based on your needs.\nWhat lifestyle changes can I make to manage perimenopause symptoms?\nYou may be able to manage perimenopause symptoms at home without needing medication from your provider. Some tips for treating your symptoms without medication include:\n\nEat lots of fruits, vegetables, whole grains, lean protein and healthy fats.\nPerform weight-bearing exercises like walking, hiking or strength training.\nDress in layers so you can remove clothing if you begin to sweat or get hot flashes.\nUse a fan or keep your house at a cooler temperature.\nImprove sleep hygiene by avoiding TV and computer screens and doing relaxing activities before bed.\nLimit alcohol and caffeine.\nPractice meditation or other stress management techniques.\nQuit using tobacco products like cigarettes.\nMaintain a weight that\u2019s healthy for you.\nWhat makes perimenopause worse?\nThere isn\u2019t one specific thing that makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse:\n\nEating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause.\nAvoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems.\nSmoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness.\nTalk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being.\n\n\n\n\nCare at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\n\nPrevention\n\nWhat can put you into perimenopause early?\nCertain factors are linked to early perimenopause. These include:\n\nSmoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t.\nA family history of early menopause.\nA history of cancer treatment.\nIf you\u2019ve had surgery to remove your uterus (hysterectomy) or ovaries (oophorectomy).\n\n\nOutlook / Prognosis\n\nAre there any health risks associated with perimenopause?\nThere are health risks associated with menopause, which happens right after perimenopause.\nEstrogen plays an important role in preserving your bones. Osteoporosis is a condition where the insides of your bones become less dense and more fragile. This increases your risk for bone fractures. Your healthcare provider may recommend a multivitamin, calcium supplement, extra vitamin D or more weight-bearing exercises. People in menopause are also at an increased risk for heart disease and other cardiovascular (heart and blood vessel) health conditions.\nCan I get pregnant if I am in perimenopause?\nYes, you can still become pregnant. You may be less likely to get pregnant during perimenopause, but it\u2019s still possible. As long as you have a period, you can still get pregnant.\nWhen your periods are irregular, you may be more likely to get pregnant unexpectedly. If you don\u2019t want to expand your family, use birth control until your healthcare provider tells you it\u2019s safe to stop.\n\n\nLiving With\n\nWhat questions should I ask my healthcare provider?\nDiscuss your perimenopause symptoms with your healthcare provider. It might help to keep a journal of your menstrual cycles, including when they start and stop and the amount of bleeding.\nSome questions you should ask are:\n\nAre these symptoms of perimenopause?\nWhat can I do to relieve my symptoms?\nHow long do you think I will have these symptoms?\nWould hormone therapy be an option for me?\nDo I need to start taking medication or vitamins?\nAre there any tests that should be done?\nWhen should I call my provider?\nIf your perimenopausal symptoms are unbearable or interfering with your quality of life, it might be time to contact a healthcare provider. They can recommend a treatment to reduce the intensity of your symptoms and get you feeling better.\n\n\nAdditional Common Questions\n\nCan perimenopause affect my sex life?\nPossibly. Certain symptoms of perimenopause like vaginal dryness and discomfort during sex can make having sex less desirable. Vaginal lubricants can help with dryness. Speak with your healthcare provider if you have issues with your sex drive so they can recommend ways to help you.\nDoes perimenopause affect my sleep?\nYes, perimenopause can affect your sleep. People in perimenopause may experience insomnia and interrupted sleep due to night sweats or other menopausal symptoms.\nWhat do hot flashes feel like during perimenopause?\nA hot flash feels like a sudden warmth all over your body. It\u2019s often accompanied by sweating and a red, flushed face. Low estrogen levels cause hot flashes, and they can last months or years.\nWhy am I gaining weight during perimenopause?\nThe shift in hormones slows down your metabolism. It\u2019s very common to gain weight once your estrogen levels start to decline. Eating healthy and nutritious foods and getting regular physical activity can help prevent weight gain during the transition to menopause.\nA note from Cleveland Clinic\n\nPerimenopause is the transition to menopause. It\u2019s a gradual process that can last years and it\u2019s a different experience for everyone. Starting this new chapter of your life, where your reproductive years are ending, can be emotionally and physically challenging. You may start having menopause-like symptoms, such as irregular periods, weight changes or mood swings. Let your healthcare provider know if you believe you\u2019ve begun perimenopause. They can help reassure you that your symptoms are normal and talk to you about the health risks of menopause like osteoporosis.\nYour healthcare provider can also help you manage symptoms of perimenopause if they start to interfere with your life. They can recommend medications or other treatments to make the transition easier on you. \n\n\n\n\n\n\nMedically Reviewed\nLast reviewed on 08/08/2024.\nLearn more about the Health Library and our editorial process.\n\nReferences\n\nAdvertisement\nAdvertisement\n\nAd\n\n\n\nAppointments 216.444.6601\nAppointments & Locations\nRequest an Appointment\nRendered: Fri Jan 10 2025 13:25:25 GMT+0000 (Coordinated Universal Time)\n\n\n\nActionsAppointments & AccessAccepted InsuranceEvents CalendarFinancial AssistanceGive to Cleveland ClinicPay Your Bill OnlineRefer a PatientPhone DirectoryVirtual Second OpinionsVirtual Visits\n\nBlog, News & AppsConsult QDHealth EssentialsNewsroomMyClevelandClinicMyChart\n\nAbout Cleveland Clinic100 Years of Cleveland ClinicAbout UsLocationsQuality & Patient SafetyOffice of Diversity & InclusionPatient ExperienceResearch & InnovationsCommunity CommitmentCareersFor EmployeesResources for Medical Professionals\n\nSite Information & PoliciesSend Us FeedbackSite MapAbout this WebsiteCopyright, Reprint & LicensingWebsite Terms of UsePrivacy PolicyNotice of Privacy PracticesNon-Discrimination Notice\n9500 Euclid Avenue, Cleveland, Ohio 44195 | 800.223.2273 | \u00a9 2025 Cleveland Clinic. All Rights Reserved. ", "title": "Perimenopause: Age, Stages, Signs, Symptoms & Treatment", "description": "<strong>Perimenopause</strong> is the transition to menopause when hormone levels decline and symptoms like hot flashes and <strong>irregular</strong> <strong>periods</strong> start. <strong>Perimenopause</strong> <strong>can</strong> last years.", "usage": {"tokens": 3532}, "retrieved_text": ["makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse: Eating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause. Avoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems. Smoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being. Care at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\n\nPrevention\n\nWhat can put you into perimenopause early? Certain factors are linked to early perimenopause. These include:\n\nSmoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t. A family history of early menopause. A history of cancer treatment.", "a cooler temperature. Improve sleep hygiene by avoiding TV and computer screens and doing relaxing activities before bed. Limit alcohol and caffeine. Practice meditation or other stress management techniques. Quit using tobacco products like cigarettes. Maintain a weight that\u2019s healthy for you. What makes perimenopause worse? There isn\u2019t one specific thing that makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse: Eating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause. Avoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems. Smoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being.", "lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being. Care at Cleveland Clinic Menopause Treatment Find a Doctor and Specialists Make an Appointment Prevention What can put you into perimenopause early? Certain factors are linked to early perimenopause. These include: Smoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t. A family history of early menopause. A history of cancer treatment. If you\u2019ve had surgery to remove your uterus (hysterectomy) or ovaries (oophorectomy). Outlook / Prognosis\n\nAre there any health risks associated with perimenopause? There are health risks associated with menopause, which happens right after perimenopause. Estrogen plays an important role in preserving your bones. Osteoporosis is a condition where the insides of your bones become less dense and more fragile. This increases your risk for bone fractures. Your healthcare provider may recommend a multivitamin, calcium supplement, extra vitamin D or more weight-bearing exercises."]}, {"text": "  \n \n   \n     \n\nThis content does not have an English version.\n\nThis content does not have an Arabic version. \n \n\n\n\n\n\n  \n\n\n\n\n\n\n    \n    \n \n \n \n \nPrint  \n \nOverview\nPerimenopause means \"around menopause\" and refers to the time during which your body makes the natural transition to menopause, marking the end of the reproductive years. Perimenopause is also called the menopausal transition. \nWomen start perimenopause at different ages. You may notice signs of progression toward menopause, such as menstrual irregularity, sometime in your 40s. But some women notice changes as early as their mid-30s.\n\n \nThe level of estrogen \u2014 the main female hormone \u2014 in your body rises and falls unevenly during perimenopause. Your menstrual cycles may lengthen or shorten, and you may begin having menstrual cycles in which your ovaries don't release an egg (ovulate). You may also experience menopause-like symptoms, such as hot flashes, sleep problems and vaginal dryness. Treatments are available to help ease these symptoms. \nOnce you've gone through 12 consecutive months without a menstrual period, you've officially reached menopause, and the perimenopause period is over.\n\nProducts & Services\n\n\nA Book: The New Rules of Menopause \n \n \n   \nSymptoms\nThroughout the menopausal transition, some subtle \u2014 and some not-so-subtle \u2014 changes in your body may take place. You might experience: \n \nIrregular periods. As ovulation becomes more unpredictable, the length of time between periods may be longer or shorter, your flow may be light to heavy, and you may skip some periods. If you have a persistent change of seven days or more in the length of your menstrual cycle, you may be in early perimenopause. If you have a space of 60 days or more between periods, you're likely in late perimenopause. \nHot flashes and sleep problems. Hot flashes are common during perimenopause. The intensity, length and frequency vary. Sleep problems are often due to hot flashes or night sweats, but sometimes sleep becomes unpredictable even without them. \nMood changes. Mood swings, irritability or increased risk of depression may happen during perimenopause. The cause of these symptoms may be sleep disruption associated with hot flashes. Mood changes may also be caused by factors not related to the hormonal changes of perimenopause. \nVaginal and bladder problems. When estrogen levels diminish, your vaginal tissues may lose lubrication and elasticity, making intercourse painful. Low estrogen may also leave you more vulnerable to urinary or vaginal infections. Loss of tissue tone may contribute to urinary incontinence. \nDecreasing fertility. As ovulation becomes irregular, your ability to conceive decreases. However, as long as you're having periods, pregnancy is still possible. If you wish to avoid pregnancy, use birth control until you've had no periods for 12 months. \nChanges in sexual function. During perimenopause, sexual arousal and desire may change. But if you had satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. \nLoss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. \nChanging cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease.  \nWhen to see a doctor\nSome women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention. Because symptoms may be subtle and come on gradually, you may not realize at first that they're all connected to the same thing \u2014 hormone fluctuations of the menopausal transition. \nIf you have symptoms that interfere with your life or well-being, such as hot flashes, mood swings or changes in sexual function that concern you, see your doctor. \nRequest an appointment \n \n  \n \n \n \n    \n \n \n \n \nThere is a problem with information submitted for this request. Review/update the information highlighted below and resubmit the form.  \nFrom Mayo Clinic to your inbox  \nSign up for free and stay up to date on research advancements, health tips, current health topics, and expertise on managing health. Click here for an email preview. \n \n           Email   \n \n Address 1    \nErrorEmail field is required \nErrorInclude a valid email address  \nLearn more about Mayo Clinic\u2019s use of data. \n \nTo provide you with the most relevant and helpful information, and understand which information is beneficial, we may combine your email and website usage information with other information we have about you. If you are a Mayo Clinic patient, this could include protected health information. If we combine this information with your protected health information, we will treat all of that information as protected health information and will only use or disclose that information as set forth in our notice of privacy practices. You may opt-out of email communications at any time by clicking on the unsubscribe link in the e-mail.  \n \nSubscribe!   \n \n \nThank you for subscribing! \n \nYou'll soon start receiving the latest Mayo Clinic health information you requested in your inbox.  \n \nSorry something went wrong with your subscription \nPlease, try again in a couple of minutes \n \nRetry      \n \n \n \n \n \n        \nCauses\nAs you go through perimenopause, your body's production of estrogen and progesterone, key female hormones, rises and falls. Many of the changes you experience during perimenopause are a result of decreasing estrogen. \nRisk factors\nMenopause is a normal phase in life. But it may occur earlier in some women than in others. Although not always conclusive, some evidence suggests that certain factors may make it more likely that you start perimenopause at an earlier age, including: \n \nSmoking. The onset of menopause occurs 1 to 2 years earlier in women who smoke than in women who don't smoke. \nFamily history. Women with a family history of early menopause may experience early menopause themselves. \nCancer treatment. Treatment for cancer with chemotherapy or pelvic radiation therapy has been linked to early menopause. \nHysterectomy. A hysterectomy that removes your uterus, but not your ovaries, usually doesn't cause menopause. Although you no longer have periods, your ovaries still produce estrogen. But such surgery may cause menopause to occur earlier than average. Also, if you have one ovary removed, the remaining ovary might stop working sooner than expected.  \nComplications\nIrregular periods are a hallmark of perimenopause. Most of the time this is normal and nothing to be concerned about. However, see your doctor if: \n \nBleeding is extremely heavy \u2014 you're changing tampons or pads every hour or two for two or more hours \nBleeding lasts longer than seven days \nBleeding occurs between periods \nPeriods regularly occur less than 21 days apart  \nSigns such as these may mean there's a problem with your reproductive system that requires diagnosis and treatment. \n \n   \n \n \n    \nBy Mayo Clinic Staff  \n \n Perimenopause care at Mayo Clinic   \nRequest an appointment \n \nDiagnosis & treatment \n May 25, 2023  \nPrint  \n\nShow references\n\n \nLobo RA, et al. Menopause and care of the mature woman: Endocrinology, consequences of estrogen deficiency, effects of hormone therapy, and other treatment options. In: Comprehensive Gynecology. 7th ed. Elsevier; 2017. https://www.clinicalkey.com. Accessed March 5, 2021. \nAskMayoExpert. Menopausal hormone therapy (adult). Mayo Clinic; 2020. \nAskMayoExpert. Bioidentical hormones. Mayo Clinic; 2019. \nDHEA. Natural Medicines. https://naturalmedicines.therapeuticresearch.com/. Accessed March 5, 2021. \nBlack cohosh. Natural Medicines. https://naturalmedicines.therapeuticresearch.com/. Accessed March 5, 2021. \nFerri FF. Menopause. In: Ferri's Clinical Advisor 2021. Elsevier; 2021. https://www.clinicalkey.com. Accessed March 5, 2021. \nMenopausal symptoms: In depth. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/menopausal-symptoms-in-depth. Accessed March 5, 2021. \nDelamater L, et al. Management of the perimenopause. Clinical Obstetrics and Gynecology. 2018; doi:10.1097/GRF.0000000000000389. \nSuss H, et al. Psychological resilience during the perimenopause. Maturitas. 2020; doi:10.1016/j.maturitas.2019.10.015. \nRaglan GB, et al. Depression during perimenopause: The role of the obstetrician-gynecologist. Archives of Women's Mental Health. 2020; doi:10.1007/s00737-019-0950-6. \nBacon JL. The menopausal transition. Obstetrics and Gynecology Clinics of North America. 2017; doi:10.1016/j.ogc.2018.02.008. \nTop questions about menopause. Office on Women's Health. https://www.womenshealth.gov/menopause. Accessed March 5, 2021. \nWarner KJ. Allscripts EPSi. Mayo Clinic. Feb. 25, 2021. \nJohnson A, et al. Complementary and alternative medicine for menopause. Journal of Evidence-Based Integrative Medicine. 2019; doi:10.1177/2515690X19829380. \nMinkin MJ. Menopause: Hormones, lifestyle, and optimizing aging. Obstetrics and Gynecology Clinics of North America. 2019; doi:10.1016/j.ogc.2019.04.008. \nAmerican College of Obstetricians and Gynecologists. Practice Bulletin No. 128: Diagnosis of abnormal uterine bleeding in reproductive-aged women. Obstetrics and Gynecology. 2012; doi:10.1097/AOG.0b013e318262e320. Reaffirmed 2016. \nAmerican College of Obstetricians and Gynecologists. Practice Bulletin No. 141: Management of menopausal symptoms. Obstetrics and Gynecology. 2014; doi:10.1097/01.AOG.0000441353.20693.78. Reaffirmed 2018. \nBlack cohosh. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/black-cohosh. Accessed March 8, 2021. \nBasu P, et al. Phytoestrogens and breast cancer: In vitro anticancer activities of isoflavones, lignans, coumestans, stilbenes and their analogs and derivatives. Biomedicine and Pharmacotherapy. 2018; doi:10.1016/j.biopha.2018.08.100. \nBurnett TL (expert opinion). Mayo Clinic. May 5, 2021. \nZhao T-T, et al. Dietary isoflavones or isoflavoine-rich food intake and breast cancer risk: A meta-analysis of prospective cohort studies. Clinical Nutrition. 2019; doi:10.1016/j.clnu.2017.12.006. \nFDA approves novel drug to treat moderate to severe hot flashes caused by menopause. U.S. Food and Drug Administration. https://www.fda.gov/news-events/press-announcements/fda-approves-novel-drug-treat-moderate-severe-hot-flashes-caused-menopause. Accessed May 16, 2023. \nVeozah (prescribing information). https://www.accessdata.fda.gov/drugsatfda_docs/label/2023/216578s000lbl.pdf. Accessed May 16, 2023.    \n \nRelated \n \nAssociated Procedures \n \n Endometrial ablation    \n \nNews from Mayo Clinic \n \n Perimenopause, menopause and ... weightlifting? Expert explains value for bone health Sept. 04, 2024, 03:15 p.m. CDT  \n Mayo Clinic Minute: Perimenopause and menopause May 13, 2024, 04:00 p.m. CDT  \n Mayo Clinic Q and A: Perimenopause transitions and concerns July 20, 2023, 03:00 p.m. CDT    \n \nProducts & Services \n \n A Book: The New Rules of Menopause        \n \nPerimenopause\n\nSymptoms\n&\ncauses\nDiagnosis\n&\ntreatment\nDoctors\n&\ndepartments\nCare at\nMayo\nClinic   \n  \n \n Advertisement \n  \n \nMayo Clinic does not endorse companies or products. Advertising revenue supports our not-for-profit mission. Advertising & Sponsorship \n \nPolicy \nOpportunities \nAd Choices    \n \nMayo Clinic Press \nCheck out these best-sellers and special offers on books and newsletters from Mayo Clinic Press. \n\nNEW: Listen to Health Matters Podcast - Mayo Clinic PressNEW: Listen to Health Matters Podcast\nMayo Clinic on Incontinence - Mayo Clinic PressMayo Clinic on Incontinence\nThe Essential Diabetes Book - Mayo Clinic PressThe Essential Diabetes Book\nMayo Clinic on Hearing and Balance - Mayo Clinic PressMayo Clinic on Hearing and Balance\nFREE Mayo Clinic Diet Assessment - Mayo Clinic PressFREE Mayo Clinic Diet Assessment\nMayo Clinic Health Letter - FREE book - Mayo Clinic PressMayo Clinic Health Letter - FREE book  \n     \n   \n  \nCON-20208696 \n \n \nPatient Care & Health Information \nDiseases & Conditions \nPerimenopause         \n \n\n\n\n\n\n\n\n\n\n\n\nDon\u2019t miss your chance!\n\n Give Now\n\n\n\nGive by Dec. 31 to multiply your impact 3X. \n\n Give Now\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Find a doctor\n\n Explore careers\n\n Sign up for free e-newsletters\n\n\n\n\n\n\n\n\nAbout Mayo Clinic  \n\n\n\n\n About this Site\n\n Contact Us\n\n Locations\n\n Health Information Policy\n\n Medicare Accountable Care Organization (ACO)\n\n Media Requests\n\n News Network\n\n Price Transparency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedical Professionals \n\n\n\n\n AskMayoExpert\n\n Clinical Trials\n\n Mayo Clinic Alumni Association\n\n Refer a Patient\n\n\n\n\n\n\n\n\nBusinesses  \n\n\n\n\n Executive Health Program\n\n International Business Collaborations\n\n Facilities & Real Estate\n\n Supplier Information\n\n\n\n\n\n\n\n\nStudents   \n\n\n\n\n Admissions Requirements\n\n Degree Programs\n\n Student & Faculty Portal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearchers  \n\n\n\n\n Research Faculty\n\n Laboratories\n\n\n\n\n\n\n\n\nInternational Patients \n\n\n\n\n Appointments\n\n Financial Services\n\n International Locations & Offices\n\n\n\n\n\n\n\n\nCharitable Care & Financial Assistance \n\n Community Health Needs Assessment\n\n\n\n\n Financial Assistance Documents \u2013 Arizona\n\n Financial Assistance Documents \u2013 Florida\n\n Financial Assistance Documents \u2013 Minnesota\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollow Mayo Clinic\n\n\n\n\n X\n\n YouTube\n\n Facebook\n\n LinkedIn\n\n Instagram\n\n\n\n\n\n\n\n\n\n\n\nGet the Mayo Clinic app\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Terms & Conditions\n\n Privacy Policy\n\n Notice of Privacy Practices\n\n Notice of Nondiscrimination\n\n Accessibility Statement\n\n Advertising & Sponsorship Policy\n\n Site Map\n\n Manage Cookies\n\n\n\n\n\n Terms and Conditions\n\n Privacy Policy\n\n Notice of Privacy Practices\n\n Notice of Nondiscrimination\n\n Accessibility Statement\n\n Advertising and Sponsorship Policy\n\n Site Map\n\n Manage Cookies\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage:\nEnglish\n\n\n\n English\n\n Espa\u00f1ol\n\n \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n\n \u7b80\u4f53\u4e2d\u6587\n\n\n\n\n\u00a9 1998-2025 Mayo Foundation for Medical Education and Research (MFMER). All rights reserved. \n\nLanguage:\nEnglish\n\n\n\n English\n\n Espa\u00f1ol\n\n \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n\n \u7b80\u4f53\u4e2d\u6587\n\n\n\n    \n       \n \n \n    \n \nMayo Clinic Privacy Policy \n Mayo Clinic and our partners use technologies such as cookies to collect information from your browser to deliver relevant advertising on our site, in emails and across the Internet, personalize content and perform site analytics. Please visit our Privacy Policy for more information about our use of data and your rights. \nClick here to update your preferences \n \nI Agree  \n \nx \nCustomize Cookie Settings \n \n Trusted \nThese technologies are used in operating the site, including remembering your preferences, ensuring security and providing certain features. They do not track your activity online.   \n \n   \n Website Analytics \nThese technologies collect information to help us understand how our websites are being used and to make improvements.   \n \n   \n Advertising \nThese technologies are used to make advertising messages more relevant to you. They perform functions like preventing the same ad from continuously reappearing, ensuring that ads are properly displayed for advertisers and selecting advertisements that are based on your interests.   \n \nSave   ", "title": "Perimenopause - Symptoms and causes - Mayo Clinic", "description": "Discover effective treatments and learn self-care strategies for hot flashes, night sweats, vaginal dryness and other <strong>perimenopause</strong> symptoms.", "usage": {"tokens": 3510}, "retrieved_text": ["you wish to avoid pregnancy, use birth control until you've had no periods for 12 months. Changes in sexual function. During perimenopause, sexual arousal and desire may change. But if you had satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. Loss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. Changing cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease. When to see a doctor\nSome women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention.", "satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. Loss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. Changing cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease. When to see a doctor Some women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention. Because symptoms may be subtle and come on gradually, you may not realize at first that they're all connected to the same thing \u2014 hormone fluctuations of the menopausal transition. If you have symptoms that interfere with your life or well-being, such as hot flashes, mood swings or changes in sexual function that concern you, see your doctor.", "Gynecology Clinics of North America. 2019; doi:10.1016/j.ogc.2019.04.008. American College of Obstetricians and Gynecologists. Practice Bulletin No. 128: Diagnosis of abnormal uterine bleeding in reproductive-aged women. Obstetrics and Gynecology. 2012; doi:10.1097/AOG.0b013e318262e320. Reaffirmed 2016. American College of Obstetricians and Gynecologists. Practice Bulletin No. 141: Management of menopausal symptoms. Obstetrics and Gynecology. 2014; doi:10.1097/01.AOG.0000441353.20693.78. Reaffirmed 2018. Black cohosh. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/black-cohosh. Accessed March 8, 2021. Basu P, et al. Phytoestrogens and breast cancer: In vitro anticancer activities of isoflavones, lignans, coumestans, stilbenes and their analogs and derivatives. Biomedicine and Pharmacotherapy. 2018; doi:10.1016/j.biopha.2018.08.100. Burnett TL (expert opinion). Mayo Clinic. May 5, 2021. Zhao T-T, et al. Dietary isoflavones or isoflavoine-rich food intake and breast cancer risk: A meta-analysis of prospective cohort studies. Clinical Nutrition. 2019; doi:10.1016/j.clnu.2017.12.006. FDA approves novel drug to treat moderate to severe hot flashes caused by menopause. U.S. Food and Drug Administration. https://www.fda.gov/news-events/press-announcements/fda-approves-novel-drug-treat-moderate-severe-hot-flashes-caused-menopause. Accessed May 16, 2023. Veozah (prescribing information). https://www.accessdata.fda.gov/drugsatfda_docs/label/2023/216578s000lbl.pdf. Accessed May 16, 2023. Related \n \nAssociated Procedures \n \n Endometrial ablation    \n \nNews from Mayo Clinic \n \n Perimenopause, menopause and ... weightlifting?"]}, {"text": "Health Conditions\n\n\nHealth ConditionsAll\n\nBreast Cancer\nCancer Care\nCaregiving for Alzheimer's Disease\nChronic Kidney Disease\nChronic Obstructive Pulmonary Disease (COPD)\nDigestive Health\nEye Health\nHeart Health\nMenopause\nMental Health\nMigraine\nMultiple Sclerosis (MS)\nPsoriasis\nRheumatoid Arthritis (RA)\nSleep Health\nType 2 Diabetes\nWeight Management\n\nCondition SpotlightAll\n\n\n\n\nControlling Ulcerative Colitis\n\n\n\nNavigating Life with Bipolar Disorder\n\n\n\nMastering Geographic Atrophy\n\n\n\nManaging Type 2 Diabetes\n\nWellness\n\n\nWellness TopicsAll\n\nCBD\nFitness\nHearing\nMental Well-Being\nNutrition\nParenthood\nRecipes\nSexual Health\nSkin Care\nSleep Health\nVitamins and Supplements\nWomen's Wellness\n\nProduct ReviewsAll\n\nAt-Home Testing\nMen's Health\nMental Health\nNutrition\nSleep\nVitamins and Supplements\nWomen's Health\n\nFeatured ProgramsAll\n\n\n\n\nYour Eczema Skin Care Roadmap\n\n\n\nInflammation and Aging\n\n\n\nCold & Flu Season Survival Guide\n\n\n\nShe\u2019s Good for Real\n\nTools\n\n\nFeatured\n\nVideo Series\nPill Identifier\nFindCare\nDrugs A-Z\n\nLessonsAll\n\nCrohn\u2019s and Ulcerative Colitis Essentials\nDiabetes Nutrition\nHigh Cholesterol\nTaming Inflammation in Psoriasis\nTaming Inflammation in Psoriatic Arthritis\n\nNewslettersAll\n\nAnxiety and Depression\nDigestive Health\nHeart Health\nMigraine\nNutrition Edition\nType 2 Diabetes\nWellness Wire\n\nLifestyle Quizzes\n\nFind a Diet\nFind Healthy Snacks\nWeight Management\nHow Well Do You Sleep?\nAre You a Workaholic?\n\nConnect\n\n\nFind Your Bezzy Community\nBezzy communities provide meaningful connections with others living with chronic conditions. Join Bezzy on the web or mobile app.All\n\n\n\n\nBreast Cancer\n\n\n\nMultiple Sclerosis\n\n\n\nDepression\n\n\n\nInflammatory Bowel Disease\n\n\n\nType 2 Diabetes\n\n\n\nPsoriasis\n\nFollow us on social media\nCan't get enough? Connect with us for all things health.\n\n\n\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Perimenopause Can Affect Your Periods and What You Can Do\n\n\n\n\nMedically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014  Updated on March 27, 2023\n\n\nSpotting\nHeavy bleeding\nDark blood\nShort cycles\nLong cycles\nMissed cycles\nIrregularity\nWhen to see your doctor\n\n\n\n\n\n\nDuring perimenopause, your levels of the hormones estrogen and progesterone fluctuate. These changes can affect your cycle, leading to irregular or missed periods, and more.\n\n\n\n\n\n\n\n\n\nUnderstanding perimenopause\n\nMenopause refers to the end of your menstrual cycle. Once you\u2019ve gone 12 months without a period, you\u2019ve reached menopause.\n\nThe average woman goes through menopause at 51 years old. The time period before menopause is called perimenopause.\n\nPerimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. \n\nThese shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns.\n\nOther symptoms of perimenopause include:\n\n\nhot flashes\nnight sweats\nsleep troubles\nmemory issues\ndifficulty urinating\nvaginal dryness\nchanges in sexual desire or satisfaction\n\nHere\u2019s what you can expect from perimenopause and what you can do.\n\n\n\n\n1. Spotting between periods\n\nIf you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting.\n\nSpotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining.\n\nMany women spot before their period starts or as it ends. Mid-cycle spotting around ovulation is also common.\n\nIf you\u2019re regularly spotting every 2 weeks, it may be a sign of a hormonal imbalance. You may want to speak with your healthcare provider.\n\nWhat you can do\n\nConsider keeping a journal to track your periods. Include information such as: \n\n\nwhen they start\nhow long they last\nhow heavy they are\nwhether you have any in-between spotting\n\nYou can also log this information in an app, like Eve.\n\nWorried about leaks and stains? Consider wearing panty liners. Disposable panty liners are available at most drugstores. They come in a variety of lengths and materials. \n\nYou can even buy reusable liners that are made of fabric and can be washed over and over again.\n\n\n\n\nProducts to try\n\nIf you deal with spotting between periods, using certain products can help you track your symptoms and avoid leaks and stains. Shop for them online:\n\nperiod journal\npanty liners\nreusable panty liners\n\n\n\n\n\n\n2. Abnormally heavy bleeding\n\nWhen your estrogen levels are high in comparison to your progesterone levels, your uterine lining builds. This results in heavier bleeding during your period as your lining sheds.\n\nA skipped period can also cause the lining to build up, leading to heavy bleeding.\n\nBleeding is considered heavy if it:\n\n\nsoaks through one tampon or pad an hour for several hours\nrequires double protection \u2014 such as a tampon and pad \u2014 to control menstrual flow\ncauses you to interrupt your sleep to change your pad or tampon\nlasts longer than 7 days\n\nWhen bleeding is heavy, it may last longer, disrupting your everyday life. You may find it uncomfortable to exercise or carry on with your normal tasks. \n\nHeavy bleeding can also cause fatigue and increase your risk for other health concerns, such as anemia.\n\nWhat you can do\n\nAs you may know, taking ibuprofen (Advil, Midol, Motrin) during your period can help with menstrual cramps. \n\nIf you take it when you\u2019re bleeding heavily, it may also reduce your flow. Try taking 200 milligrams (mg) every 4 to 6 hours during the day.\n\nIf cramps and pain continue, talk to your healthcare provider about hormonal approaches to treatment. Some women have a medical or family history that discourages the use of hormones in the perimenopausal period.\n\n\n\n\n3. Brown or dark blood\n\nThe colors you see in your menstrual flow can range from bright red to dark brown, especially toward the end of your period. Brown or dark blood is a sign of old blood exiting the body.\n\nWomen in perimenopause may also see brown spotting or discharge at other times throughout the month.\n\nYou may also notice changes in discharge texture. Your discharge may be thin and watery, or it may be clumpy and thick.\n\nWhat you can do\n\nIf you\u2019re concerned about your menstrual flow, you may want to schedule an appointment to see your doctor.\n\nThe variation in color is usually due to the amount of time it takes for the blood and tissue to cycle out of the body, but it can sometimes be a sign of another underlying condition. \n\nIf there\u2019s a foul odor to the vaginal discharge, it may be a sign of infection. See your healthcare provider.\n\n\n\n4. Shorter cycles\n\nWhen your estrogen levels are low, your uterine lining is thinner. Bleeding, as a result, may be lighter and last fewer days. Short cycles are more common in the earlier stages of perimenopause.\n\nFor example, you may have a period that\u2019s 2 or 3 days shorter than normal. Your whole cycle may also last 2 or 3 weeks instead of 4. It isn\u2019t uncommon to feel like your period just ended when the next one comes.\n\nWhat you can do\n\nIf you\u2019re worried about short, unpredictable cycles, consider leakage protection such as liners, pads, or period underwear like Thinx.\n\nPass on tampons and menstrual cups unless you have a menstrual flow. Insertion can be difficult or uncomfortable without this lubrication. You\u2019re also more likely to forget to change your tampon or cup, increasing your risk for complications.\n\n\n\n\nProducts to try\n\nIf your periods are unpredictable, you can protect yourself from stains with leakage protection products. Shop for them online:\n\npanty liners\npads\nperiod underwear\n\n\n\n\n\n\n\n\n\n\n\n\n5. Longer cycles\n\nIn the later stages of perimenopause, your cycles may become much longer and farther apart. Longer cycles are defined as those longer than 38 days. They\u2019re related to anovulatory cycles, or cycles in which you don\u2019t ovulate.\n\nA 2008 study suggests that women who experience anovulatory cycles may have lighter bleeding than women who experience ovulatory cycles.\n\nWhat you can do\n\nIf you\u2019re dealing with longer cycles, it may be time to invest in a good menstrual cup or a cycle set of blood-wicking underwear. You can also use pads or tampons to help you avoid leakage.\n\n\n\n\nProducts to try\n\nIf you have a long cycle, a variety of products are available to help you avoid leakage. Shop for them online:\n\nmenstrual cups\na cycle set of blood-wicking underwear, like these from Thinx and Awwa\npads\ntampons\n\n\n\n\n\n\n6. Missed cycles\n\nYour fluctuating hormones may also be to blame for a missed cycle. In fact, your cycles may become so far apart that you can\u2019t recall the last time you bled. After you\u2019ve missed 12 consecutive cycles, you\u2019ve reached menopause.\n\nIf your cycles are still making an appearance \u2014 however delayed \u2014 ovulation is still occurring. This means you can still have a period, and you can still get pregnant.\n\nAnovulatory cycles can also create delayed or missed periods.\n\nWhat you can do\n\nMissed cycles every so often usually aren\u2019t cause for concern. If you\u2019ve missed a few consecutive cycles, you may want to take a pregnancy test to determine whether your symptoms are tied to perimenopause.\n\nOther early symptoms of pregnancy include:\n\n\nnausea\nbreast tenderness\nfrequent urination\nsensitivity to smells\nheartburn\n\nYou can also make an appointment with your doctor instead of taking a home test. Your doctor can run tests to determine whether you\u2019re experiencing symptoms of perimenopause, menopause, or pregnancy.\n\nIf you aren\u2019t pregnant and don\u2019t want to conceive, use birth control every time you have sex. Fertility doesn\u2019t end until you\u2019ve completely reached menopause. \n\nUse condoms and other barrier methods to prevent sexually transmitted infections (STIs).\n\n\n\n\nProducts to try\n\nA missed period may actually be a sign of pregnancy, which can be confirmed with an at-home test. Shop for tests and condoms online:\n\npregnancy test\ncondoms\n\n\n\n\n\n\n\n7. Overall irregularity\n\nBetween long cycles, short cycles, spotting, and heavy bleeding, your cycles during perimenopause may be generally irregular. They may not settle into any discernible pattern, especially as you get closer to menopause. This can be unsettling and frustrating.\n\nWhat you can do\n\nTry your best to remember that the changes you\u2019re experiencing are part of a bigger transition. Just as it began, the process will eventually end when you stop ovulating and reach menopause.\n\nIn the meantime:\n\n\nConsider wearing black underwear or investing in period underwear to reduce your risk of stained clothing.\nConsider wearing disposable or reusable panty liners to protect from irregular leaks, spotting, and otherwise unexpected bleeding.\nTrack your periods as best you can via a calendar or an app.\nTake notes about abnormal bleeding, pain, discomfort, or other symptoms you\u2019re experiencing.\n\n\n\n\nProducts to try\n\nIf you\u2019re having irregular periods, certain products can help you avoid leaks and stains and keep track of your symptoms. Shop for them online: \n\nperiod underwear\npanty liners\nreusable panty liners\nperiod journal\n\n\n\n\n\n\nWhen to see your doctor\n\nIn some cases, irregular bleeding may be a sign of another underlying condition.\n\nSee your doctor if you\u2019re also experiencing these symptoms:\n\n\nextremely heavy bleeding that requires you to change your pad or tampon every hour or two\nbleeding that lasts longer than 7 days\nbleeding \u2014 not spotting \u2014 that happens more frequently than every 3 weeks\n\nAt your appointment, your doctor will ask about your medical history and about any symptoms you\u2019ve had. From there, they may give you a pelvic exam and order tests (such as a blood test, a biopsy, or an ultrasound) to rule out more serious issues.\n\n\n\n\nHow we reviewed this article:\n\n\nSources\n\nHistory\n\n\n\nHealthline has strict sourcing guidelines and relies on peer-reviewed studies, academic research institutions, and medical journals and associations. We only use quality, credible sources to ensure content accuracy and integrity. You can learn more about how we ensure our content is accurate and current by reading our editorial policy.\n\nLaughlin-Tommaso SK. (2018). Bleeding after menopause: Is it normal?\nhttps://www.mayoclinic.org/diseases-conditions/menopause/expert-answers/bleeding-after-menopause/faq-20058396\nMayo Clinic Staff. (2019). Perimenopause.\nhttps://www.mayoclinic.org/diseases-conditions/perimenopause/symptoms-causes/syc-20354666\nMayo Clinic Staff. (2019). Symptoms of pregnancy: What happens first.\nhttps://www.mayoclinic.org/healthy-lifestyle/getting-pregnant/in-depth/symptoms-of-pregnancy/art-20043853\nMenopause, perimenopause, and postmenopause. (2019).\nhttps://my.clevelandclinic.org/health/diseases/15224-menopause-perimenopause-and-postmenopause\nThe menopause years. (2018).\nhttps://www.acog.org/patient-resources/faqs/womens-health/the-menopause-years\nPerimenopausal bleeding and bleeding after menopause. (2018).\nhttps://www.acog.org/patient-resources/faqs/gynecologic-problems/perimenopausal-bleeding-and-bleeding-after-menopause\nVan Voorhis BJ, et al. (2008). The relationship of bleeding patterns to daily reproductive hormones in women approaching menopause.\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666050/\nWhat are the treatment options for heavy periods? (2017).\nhttps://www.ncbi.nlm.nih.gov/books/NBK279293/\nWhat happens during the typical 28-day menstrual cycle? (2018).\nhttps://www.womenshealth.gov/menstrual-cycle/your-menstrual-cycle\nWhat to expect in perimenopause. (2011).\nhttps://www.ourbodiesourselves.org/book-excerpts/health-article/what-to-expect-perimenopause/\n\n\nShare this article\n\n\n\n\n\n\n\n\n\n\n\n\nMedically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014  Updated on March 27, 2023\n\n\n\n\n\nrelated stories\n\n\nCan Perimenopause Cause Your Periods to Be Closer Together?\n\nPerimenopause and Discharge: What to Expect\n\nNatural Remedies for Perimenopause\n\nThe Perimenopause Diet: Must-Knows\n\nMenopause: 11 Things Every Woman Should Know\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWas this article helpful?\nYesNo\n\n\n\n\n\n\nRead this next\n\n\n\n\n\n\n\n\nCan Perimenopause Cause Your Periods to Be Closer Together?\nPerimenopause can cause a lot of menstrual changes before your period ultimately stops. But can it cause your periods to be closer together?READ MORE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerimenopause and Discharge: What to Expect\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nPerimenopause is the transitional period leading to menopause. During this stage, fluctuating estrogen levels can affect your cycle from one month to\u2026READ MORE\n\n\n\n\n\n\n\nNatural Remedies for Perimenopause\n\n\nMedically reviewed by Meredith Goodwin, MD, FAAFP\nDiscover vitamins, supplements, and other natural remedies for perimenopause.READ MORE\n\n\n\n\n\n\n\n\n\n\nThe Perimenopause Diet: Must-Knows\n\n\nMedically reviewed by Debra Sullivan, Ph.D., MSN, R.N., CNE, COI\nYou can\u2019t prevent menopause from happening, but you can decide how you take care of your body during this time. Learn about mindful living during\u2026READ MORE\n\n\n\n\n\n\n\nMenopause: 11 Things Every Woman Should Know\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nTo help you get a handle on menopause, here are 11 things you should know about this transitional stage of life.READ MORE\n\n\n\n\n\n\n\nWhat\u2019s the Connection between Estrogen and Heart Disease?\nEstrogen has protective factors against heart disease. When estrogen levels lower during menopause, the risk of heart disease increases. READ MORE\n\n\n\n\n\n\n\nUnderstanding the Connection Between Early Menopause and Heart Disease Risk\nExperiencing menopause earlier in life (before the age of 45) has been linked to increased risk factors for heart disease. Here's why. READ MORE\n\n\n\n\n\n\n\nUnderstanding Heart Attack Symptoms During Menopause\nChest pain is still the most common heart attack symptom during menopause, but there are other symptoms to be aware of as well. READ MORE\n\n\n\n\n\n\n\nUnderstanding the Connection Between Menopause and Heart Rate Changes\nA lesser known symptom of menopause can be heart rate changes. Here's what that may look like. READ MORE\n\n\n\n\n\n\n\nWhy Does Menopause Increase Your Risk of Heart Disease?\nThis article discusses the connection between menopause and heart disease, who\u2019s at risk, other changes you might experience during menopause, and\u2026READ MORE\n\n\n\n\n\n\n\n\n\n\n\nAbout Us\nContact Us\nTerms of Use\nPrivacy Policy\n\nPrivacy Settings\nAdvertising Policy\n\nHealth Topics\nSitemap\nMedical Affairs\nContent Integrity\nNewsletters\nYour Privacy Choices\n\n\u00a9 2025 Healthline Media LLC. All rights reserved. Healthline Media is an RVO Health Company. Our website services, content, and products are for informational purposes only. Healthline Media does not provide medical advice, diagnosis, or treatment. See additional information.\n\u00a9 2025 Healthline Media LLC. All rights reserved. Healthline Media is an RVO Health Company. Our website services, content, and products are for informational purposes only. Healthline Media does not provide medical advice, diagnosis, or treatment. See additional information.\n\n\n\nAboutCareersAdvertise with us\n\nOUR BRANDSHealthlineMedical News TodayGreatistPsych CentralBezzy\n\n\n\n\n   \n", "title": "Perimenopause Periods: Spotting, Heavy Bleeds, and More", "description": "During perimenopause, <strong>your levels of the hormones estrogen and progesterone fluctuate</strong>. These changes can affect your cycle, leading to irregular or missed periods, and more.", "usage": {"tokens": 3812}, "retrieved_text": ["to irregular or missed periods, and more. Understanding perimenopause Menopause refers to the end of your menstrual cycle. Once you\u2019ve gone 12 months without a period, you\u2019ve reached menopause. The average woman goes through menopause at 51 years old. The time period before menopause is called perimenopause. Perimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. These shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns. Other symptoms of perimenopause include:\n\n\nhot flashes\nnight sweats\nsleep troubles\nmemory issues\ndifficulty urinating\nvaginal dryness\nchanges in sexual desire or satisfaction\n\nHere\u2019s what you can expect from perimenopause and what you can do. 1. Spotting between periods\n\nIf you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting. Spotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining.", "at 51 years old. The time period before menopause is called perimenopause. Perimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. These shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns. Other symptoms of perimenopause include: hot flashes night sweats sleep troubles memory issues difficulty urinating vaginal dryness changes in sexual desire or satisfaction Here\u2019s what you can expect from perimenopause and what you can do. 1. Spotting between periods If you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting. Spotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining. Many women spot before their period starts or as it ends. Mid-cycle spotting around ovulation is also common. If you\u2019re regularly spotting every 2 weeks, it may be a sign of a hormonal imbalance.", "https://www.acog.org/patient-resources/faqs/gynecologic-problems/perimenopausal-bleeding-and-bleeding-after-menopause Van Voorhis BJ, et al. (2008). The relationship of bleeding patterns to daily reproductive hormones in women approaching menopause. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666050/ What are the treatment options for heavy periods? (2017). https://www.ncbi.nlm.nih.gov/books/NBK279293/ What happens during the typical 28-day menstrual cycle? (2018). https://www.womenshealth.gov/menstrual-cycle/your-menstrual-cycle What to expect in perimenopause. (2011). https://www.ourbodiesourselves.org/book-excerpts/health-article/what-to-expect-perimenopause/ Share this article Medically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014 Updated on March 27, 2023 related stories Can Perimenopause Cause Your Periods to Be Closer Together? Perimenopause and Discharge: What to Expect Natural Remedies for Perimenopause The Perimenopause Diet: Must-Knows Menopause: 11 Things Every Woman Should Know Was this article helpful? YesNo Read this next Can Perimenopause Cause Your Periods to Be Closer Together? Perimenopause can cause a lot of menstrual changes before your period ultimately stops. But can it cause your periods to be closer together?READ MORE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerimenopause and Discharge: What to Expect\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nPerimenopause is the transitional period leading to menopause."]}, {"text": "    \n  \n \n\nNews \n \n  \n\n\n\n\n\nIrregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? \nIf you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. \nThis stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. \nPerimenopause ends with menopause, at which point you have not had a period for 12 months. \nWhat Does Perimenopause Mean? \nThe word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. \nWhen Can You Expect Perimenopause? \nChances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause. The average age a woman starts the menopausal transition is 47. \nWhile we cannot predict when this will occur for you, the age your mother began to experience symptoms may be a good indicator. Smokers tend to reach menopause earlier than non smokers as well. \nShould You Get Tested for Perimenopause? \nThe short answer: No. \nThe blood tests that measure your ovarian reserve are rarely accurate during perimenopause. FSH and estrogen change by the day and throughout the day so they are generally not helpful. \nWe do consider testing these hormones if you experience perimenopausal symptoms under the age of 45. We generally will also check other pituitary hormones, like TSH and prolactin, if you are experiencing these symptoms prematurely. \nKeeping a menstrual diary is generally the best \u201ctest\u201d you can do. This will give you and your OBGYN insight into what your body is doing and for how long. \nAny time you experience abnormal uterine bleeding (i.e., changes in the amount of blood, frequency of bleeding, and length of bleeding), checking in with your doctor is a good idea to make sure it is normal and that no other work-up is needed. \nStages of Perimenopause \nWomen typically go through the menopausal transition in two phases: early and late. \nSigns of Early Perimenopause \n \nMore frequent menstrual cycles \nUnpredictable menstrual cycles \nHeavier or lighter bleeding  \nSigns of Late Perimenopause \n \nHot flashes or mood changes \nLess frequent menstrual periods \nHeavier or lighter bleeding  \nWhat to Expect During Perimenopause \nJust as every woman\u2019s period is different, the way you experience perimenopause will be different. The intensity of symptoms varies from one woman to the next, but they follow a typical progression until you have reached menopause and do not have a period for 12 months. \nIrregular Periods \nThis is the hallmark of perimenopause. The frustrating part is the unpredictability. Periods may change gradually from one cycle to the next, or you may notice more abrupt changes. \nHot Flashes & Night Sweats \nHot flashes and night sweats are common symptoms of perimenopause and menopause, with over 85% of women reporting hot flashes. Hormone changes affect your body\u2019s \u201cinternal thermostat.\u201d A hot flash feels like a wave or sensation of heat across your face, neck, and chest. It can last for several minutes. Hot flashes can happen a few times a day, a few times a week, or less often. \nHot flashes that happen at night are called night sweats, which can cause women to wake up drenched in sweat and disturb sleep.  Women are more likely to report hot flashes at night. \nTrouble Sleeping \nWhen hot flashes wake you up, it can be hard to fall back to sleep. You may feel like you are in \u201cfight or flight\u201d mode.  In addition, mood trouble can often make sleep tricky. \nIrritability & Depression \nPerimenopause increases the risk of depression by about 30%. The rise in depression may come from the changes and lack of control over what\u2019s happening to your body, sleeplessness, and other life events that typically occur at this point as we age. There is good news! Studies have shown that the risk of depression decreases after menopause. \nTreating Perimenopausal Symptoms \nPerimenopause itself does not need to be treated. It is a natural, normal transition as you move to the next stage of your life. \nTreating perimenopausal symptoms, however, is definitely possible. A discussion with your OBGYN will help you determine the best path for you. \n \nBirth control pills that contain low doses of estrogen can be used to help control the uncomfortable symptoms of perimenopause by regulating your body\u2019s fluctuating hormones. \nAntidepressants can be prescribed to help with depression. They can also help treat hot flashes. \nSupplements, such as multi-vitamins, support your overall health. Herbal and hormonal therapies, however, are not recommended. Studies consistently show they are as equally effective as a placebo at managing symptoms and some are associated with risks. \nMindfulness and meditation are tools that you can learn to use to manage irritability, depression, and anxiety. \nCounseling can help you learn how to cope with the changes your body and life are experiencing. \nExercise (strength training, cardiovascular routines, yoga) is beneficial throughout perimenopause as it can help you maintain a healthy weight, improve your mood, strengthen your bones, and reduce your risk of cardiovascular and other diseases.  \nFrequently Asked Questions about Perimenopause \nQ. Can I still get pregnant during perimenopause? \n \nThe chance that you could become pregnant in your mid- to late-40s is very low. However, as long as you are menstruating, it is certainly possible that you could get pregnant. We recommend contraception into your 40s if you do not want to become pregnant.  \nQ. When should I stop taking my birth control? \n \nBirth control can mask the transition to menopause, which is why it is used for treating symptoms such as hot flashes and irregular bleeding. Birth control also makes lab tests for perimenopause unreliable. We recommend stopping birth control around age 50-51 and then monitoring your symptoms.  \nQ. What can I do about perimenopausal weight gain? \n \nAs you age, your metabolism naturally slows down. You may also lose muscle mass and gain unwanted fat. It\u2019s important to maintain a healthy diet and shoot for 30 minutes of exercise each day. Carrying excess weight increases your risks of heart disease, Type 2 diabetes, and even various types of cancers. Talk to your doctor about ways you can obtain (and maintain) a healthy body weight.  \nQ. When should I call a doctor about my perimenopausal symptoms? \n \nIf you are experiencing hot flashes and night sweats under the age of 45, contact your OBGYN to see what else might be causing them. When you have abnormal uterine bleeding, it is important to alert us regardless of age as we may recommend an ultrasound or endometrial biopsy to rule out abnormal changes in the uterus.  \nIf you have not had a period for 12 months and then experience vaginal bleeding, contact your doctor. It is not normal for bleeding to recur after this period of time. Read our article about when you should see your OBGYN. \n\u2014 \nRemember, perimenopause and menopause are natural and normal transitions, but they can be stressful. Many symptoms can be managed which can help you regain a sense of control, well-being, and confidence to thrive in your next stage of life. \nWe want you to feel supported, heard, and cared for as you go through this change. \nSometimes, the biggest help is simply confirmation that what you\u2019re experiencing is normal! \nDr. Ashley Durward has been providing healthcare to women in Madison since 2015 and joined Madison Women\u2019s Health in 2019, specializing in high and low risk obstetrics, contraception and preconception counseling, management of abnormal uterine bleeding, pelvic floor disorders, and minimally invasive gynecologic surgery. September 28, 2020/by Dr. Ashley Durward\nTags: abnormal uterine bleeding, hot flashes, irregular periods, menopause, perimenopause\n\n\nShare this entry\n\nShare on Facebook\nShare on X\nShare on Pinterest\nShare on LinkedIn\nShare on Reddit\nShare by Mail\n\n  https://madisonwomenshealth.com/wp-content/uploads/2020/09/perimenopause-irregular-bleeding-in-40s.jpg 799 1200   Dr. Ashley Durward  https://madisonwomenshealth.com/wp-content/uploads/2017/09/mwh_logo-300x177.png  Dr. Ashley Durward2020-09-28 09:11:392023-05-31 12:17:11What is Perimenopause?\n\n\nYou might also like\n\nHormone Imbalance: What are the Symptoms and Treatments?\nMenopausal Symptoms: How to Treat The Ones That Bother You \n    \n\n\nFind Answers to Your Questions  \n \n        \nRecent Posts \n \n Constipation During Pregnancy: Tips for Finding Relief and Peace of Mind  \n The Truth About Infections During Pregnancy: What Every Mom-to-Be Needs to Know  \n Frequently Asked Questions About Changing Health Insurance in 2024  \n Understanding and Overcoming Yeast Infections During Pregnancy  \n Why STI Testing Should Be Part of Your Self-Care Routine   \nBrowse by Category\nBrowse by Category Select Category Baby Story Billing News Blog Clinic News Family Family Planning Fertility Health News Healthcare 101 Kids Labor & Delivery Pregnancy Uncategorized Women\u2019s Health   \nTags\nbaby weight birth control birth control pills birth control shots breast cancer breastfeeding C-section clogged duct Clomid colostrum conceiving condoms Depo Provera diaphragms doula epidural exercising during pregnancy fertility tips genetic counselling genetic screening gestational diabetes gestational hypertension giving birth Glow App gynecologist HPV screening labor tips Letrozole mammograms mastitis menopause mini-pill miscarriage NuvaRing OBGYN perimenopause Polycystic Ovarian Syndrome (PCOS) pregnancy pregnancy tips pregnancy weight gain pregnancy weight loss prenatal genetic testing STI ultrasound yeast infections    \n\n\n\n\n \n\n \n\n\n\n\n\n\n5801 Research Park Blvd, Suite 400, Madison WI 53719 \n608-729-6300   \n \n      Reviews \n  \n \n\n\n  \n  \n\n\n    \n \n \u00a9 2025 All rights reserved. | WordPress Web Design by Bizzy Bizzy      Food During Pregnancy: Cravings, Aversions, Foods to Eat and What to AvoidHormone Imbalance: What are the Symptoms and Treatments? Scroll to top \n \n\n\n\nThis site uses cookies. By continuing to browse the site, you are agreeing to our use of cookies.OKLearn more\u00d7\n  \n\nCookie and Privacy Settings\n\n\n  \n\n\n\nHow we use cookies\n\n\nWe may request cookies to be set on your device. We use cookies to let us know when you visit our websites, how you interact with us, to enrich your user experience, and to customize your relationship with our website.  \nClick on the different category headings to find out more. You can also change some of your preferences. Note that blocking some types of cookies may impact your experience on our websites and the services we are able to offer. \nEssential Website Cookies\n\n\nThese cookies are strictly necessary to provide you with services available through our website and to use some of its features. \nBecause these cookies are strictly necessary to deliver the website, refusing them will have impact how our site functions. You always can block or delete cookies by changing your browser settings and force blocking all cookies on this website. But this will always prompt you to accept/refuse cookies when revisiting our site. \nWe fully respect if you want to refuse cookies but to avoid asking you again and again kindly allow us to store a cookie for that. You are free to opt out any time or opt in for other cookies to get a better experience. If you refuse cookies we will remove all set cookies in our domain. \nWe provide you with a list of stored cookies on your computer in our domain so you can check what we stored. Due to security reasons we are not able to show or modify cookies from other domains. You can check these in your browser security settings. \nCheck to enable permanent hiding of message bar and refuse all cookies if you do not opt in. We need 2 cookies to store this setting. Otherwise you will be prompted again when opening a new browser window or new a tab. \nClick to enable/disable essential site cookies. \nGoogle Analytics Cookies\n\n\nThese cookies collect information that is used either in aggregate form to help us understand how our website is being used or how effective our marketing campaigns are, or to help us customize our website and application for you in order to enhance your experience. \nIf you do not want that we track your visit to our site you can disable tracking in your browser here: \nClick to enable/disable Google Analytics tracking. \nOther external services\n\n\nWe also use different external services like Google Webfonts, Google Maps, and external Video providers. Since these providers may collect personal data like your IP address we allow you to block them here. Please be aware that this might heavily reduce the functionality and appearance of our site. Changes will take effect once you reload the page. \nGoogle Webfont Settings: \nClick to enable/disable Google Webfonts. \nGoogle Map Settings: \nClick to enable/disable Google Maps. \nGoogle reCaptcha Settings: \nClick to enable/disable Google reCaptcha. \nVimeo and Youtube video embeds: \nClick to enable/disable video embeds. \nOther cookies\n\n\nThe following cookies are also needed - You can choose if you want to allow them: \nClick to enable/disable _ga - Google Analytics Cookie. \nClick to enable/disable _gid - Google Analytics Cookie. \nClick to enable/disable _gat_* - Google Analytics Cookie. \nAccept settingsHide notification only         ", "title": "Irregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else?", "description": "It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. <strong>Your estrogen levels drop and you may have markedly irregular menstrual cycles</strong>. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood ...", "usage": {"tokens": 3017}, "retrieved_text": ["    \n  \n \n\nNews \n \n  \n\n\n\n\n\nIrregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? If you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. This stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. Perimenopause ends with menopause, at which point you have not had a period for 12 months. What Does Perimenopause Mean? The word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. When Can You Expect Perimenopause? Chances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause.", "News Irregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? If you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. This stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. Perimenopause ends with menopause, at which point you have not had a period for 12 months. What Does Perimenopause Mean? The word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. When Can You Expect Perimenopause? Chances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause. The average age a woman starts the menopausal transition is 47.", "We generally will also check other pituitary hormones, like TSH and prolactin, if you are experiencing these symptoms prematurely. Keeping a menstrual diary is generally the best \u201ctest\u201d you can do. This will give you and your OBGYN insight into what your body is doing and for how long. Any time you experience abnormal uterine bleeding (i.e., changes in the amount of blood, frequency of bleeding, and length of bleeding), checking in with your doctor is a good idea to make sure it is normal and that no other work-up is needed. Stages of Perimenopause \nWomen typically go through the menopausal transition in two phases: early and late. Signs of Early Perimenopause \n \nMore frequent menstrual cycles \nUnpredictable menstrual cycles \nHeavier or lighter bleeding  \nSigns of Late Perimenopause \n \nHot flashes or mood changes \nLess frequent menstrual periods \nHeavier or lighter bleeding  \nWhat to Expect During Perimenopause \nJust as every woman\u2019s period is different, the way you experience perimenopause will be different. The intensity of symptoms varies from one woman to the next, but they follow a typical progression until you have reached menopause and do not have a period for 12 months. Irregular Periods \nThis is the hallmark of perimenopause."]}, {"text": "  \n   \n \n \nRecent Articles \n \n  \n   \n \nLet's not call it cancer    \n   \n \nChair exercises for seniors: Boosting strength, flexibility, and stamina    \n   \n \nWhy all the buzz about inflammation \u2014 and just how bad is it?    \n   \n \nChronic kidney disease: What to know about this common, serious condition    \n   \n \nBeyond the usual suspects for healthy resolutions    \n   \n \nCOVID tests: Do at-home tests work on newer variants?    \n   \n \nA liquid biopsy for metastatic prostate cancer    \n   \n \nOne way to combat loneliness? Strengthen relationships you already have    \n   \n \nFeel like you should be drinking less? Start here    \n   \n \nGot a pinched nerve? Strategies and treatments for pain relief    \n  /      \n \n \n \n  \n \n  August 9, 2022    \n \n   \nWhat are the signs of perimenopause? You're in your 40s, you wake up in a sweat at night, and your periods are erratic and often accompanied by heavy bleeding: Chances are, you're going through perimenopause. Many women experience an array of symptoms as their hormones shift during the months or years leading up to menopause \u2014 that is, the natural end of menstruation. Menopause is a point in time, but perimenopause (peri, Greek for \"around\" or \"near\" + menopause) is an extended transitional state. It's also sometimes referred to as the menopausal transition, although technically, the transition ends 12 months earlier than perimenopause (see \"Stages of reproductive aging\" below). \nWhat is perimenopause? \nPerimenopause has been variously defined, but experts generally agree that it begins with irregular menstrual cycles \u2014 courtesy of declining ovarian function \u2014 and ends a year after the last menstrual period. \nPerimenopause varies greatly from one woman to the next. The average duration is three to four years, although it can last just a few months or extend as long as a decade. Some women feel buffeted by hot flashes and wiped out by heavy periods; many have no bothersome symptoms. Periods may end more or less abruptly for some, while others may menstruate erratically for years. Fortunately, as knowledge about reproductive aging has grown, so have the options for treating some of its more distressing features. \nPerimenopause and estrogen \nThe physical changes of perimenopause are rooted in hormonal alterations, particularly variations in the level of circulating estrogen. \nDuring our peak reproductive years, the amount of estrogen in circulation rises and falls fairly predictably throughout the menstrual cycle. Estrogen levels are largely controlled by two hormones, follicle-stimulating hormone (FSH) and luteinizing hormone (LH). FSH stimulates the follicles \u2014 the fluid-filled sacs in the ovaries that contain the eggs \u2014 to produce estrogen. When estrogen reaches a certain level, the brain signals the pituitary to turn off the FSH and produce a surge of LH. This in turn stimulates the ovary to release the egg from its follicle (ovulation). The leftover follicle produces progesterone, in addition to estrogen, in preparation for pregnancy. As these hormone levels rise, the levels of FSH and LH drop. If pregnancy doesn't occur, progesterone falls, menstruation takes place, and the cycle begins again. \nHot flashes during perimenopause \nMost women don't expect to have hot flashes until menopause, so it can be a big surprise when they show up earlier, during perimenopause. Hot flashes \u2014 sometimes called hot flushes and given the scientific name of vasomotor symptoms \u2014 are the most commonly reported symptom of perimenopause. They're also a regular feature of sudden menopause due to surgery or treatment with certain medications, such as chemotherapy drugs. \nHot flashes tend to come on rapidly and can last from one to five minutes. They range in severity from a fleeting sense of warmth to a feeling of being consumed by fire \"from the inside out.\" A major hot flash can induce facial and upper-body flushing, sweating, chills, and sometimes confusion. Having one of these at an inconvenient time (such as during a speech, job interview, or romantic interlude) can be quite disconcerting. Hot flash frequency varies widely. Some women have a few over the course of a week; others may experience 10 or more in the daytime, plus some at night. \nMost American women have hot flashes around the time of menopause, but studies of other cultures suggest this experience is not universal. Far fewer Japanese, Korean, and Southeast Asian women report having hot flashes. In Mexico's Yucatan peninsula, women appear not to have any at all. These differences may reflect cultural variations in perceptions, semantics, and lifestyle factors, such as diet. \nAlthough the physiology of hot flashes has been studied for more than 30 years, no one is certain why or how they occur. Estrogen is involved \u2014 if it weren't, estrogen therapy wouldn't relieve vasomotor symptoms as well as it does \u2014 but it's not the whole story. For example, researchers have found no differences in estrogen levels in women who have hot flash symptoms and those who don't. A better understanding of the causes of hot flashes in perimenopause could open the way to new, nonhormonal treatments. Hormone therapy quells hot flashes, but it's not risk-free. \nBy our late 30s, we don't produce as much progesterone. The number and quality of follicles also diminishes, causing a decline in estrogen production and fewer ovulations. As a result, by our 40s, cycle length and menstrual flow may vary and periods may become irregular. Estrogen may drop precipitously or spike higher than normal. Over time, FSH levels rise in a vain attempt to prod the ovaries into producing more estrogen. \nAlthough a high FSH can be a sign that perimenopause has begun, a single FSH reading isn't a reliable indicator because day-to-day hormone levels can fluctuate dramatically. \nPerimenopause symptoms \nIt can be difficult to distinguish the hormonally based symptoms of perimenopause from more general changes due to aging or common midlife events \u2014 such as children leaving home, changes in relationships or careers, or the death or illness of parents. Given the range of women's experience of perimenopause, it's unlikely that symptoms depend on hormonal fluctuations alone. \n \nHot flashes and night sweats. An estimated 35%\u201350% of perimenopausal women suffer sudden waves of body heat with sweating and flushing that last 5\u201310 minutes, often at night as well as during the day. They typically begin in the scalp, face, neck, or chest and can differ dramatically among women who have them; some women feel only slightly warm, while others end up wringing wet. Hot flashes often continue for a year or two after menopause. In up to 10% of women, they persist for years beyond that. \nVaginal dryness. During late perimenopause, falling estrogen levels can cause vaginal tissue to become thinner and drier. Vaginal dryness (which usually becomes even worse after menopause) can cause itching and irritation. It may also be a source of pain during intercourse, contributing to a decline in sexual desire at midlife. \nUterine bleeding problems. With less progesterone to regulate the growth of the endometrium, the uterine lining may become thicker before it's shed, resulting in very heavy periods. Also, fibroids (benign tumors of the uterine wall) and endometriosis (the migration of endometrial tissue to other pelvic structures), both of which are fueled by estrogen, may become more troublesome. \nSleep disturbances. About 40% of perimenopausal women have sleep problems. Some studies have shown a relationship between night sweats and disrupted sleep; others have not. The problem is too complex to blame on hormone oscillations alone. Sleep cycles change as we age, and insomnia is a common age-related complaint in both sexes. \nMood symptoms. Estimates put the number of women who experience mood symptoms during perimenopause at 10%\u201320%. Some studies have linked estrogen to depression during the menopausal transition, but there's no proof that depression in women at midlife reflects declining hormone levels. In fact, women actually have a lower rate of depression after age 45 than before. Menopause-related hormone changes are also unlikely to make women anxious or chronically irritable, although the unpredictability of perimenopause can be stressful and provoke some episodes of irritability. Also, some women may be more vulnerable than others to hormone-related mood changes. The best predictors of mood symptoms at midlife are life stress, poor overall health, and a history of depression. \nOther problems. Many women complain of short-term memory problems and difficulty concentrating during the menopausal transition. Although estrogen and progesterone are players in maintaining brain function, there's too little information to separate the effects of aging and psychosocial factors from those related to hormone changes.  \nWhat to do about perimenopause symptoms \nSeveral treatments have been studied for managing perimenopausal symptoms. Complementary therapies are also available, but research on them is limited and the results are inconsistent. \n \nVasomotor symptoms. The first rule is to avoid possible triggers of hot flashes, which include warm air temperatures, hot beverages, and spicy foods. You know your triggers best. Dress in layers so you can take off clothes as needed. There's clear evidence that paced respiration, a deep breathing technique, helps alleviate hot flashes. The most effective treatment for severe hot flashes and night sweats is estrogen. Unless you've had a hysterectomy, you'll likely need to take a progestin to reduce the risk of developing endometrial cancer. Low-dose estrogen by pill or patch \u2014 for example, doses that are less than or equal to 0.3 milligrams (mg) conjugated equine estrogen, 0.5 mg oral micronized estradiol, 25 micrograms (mcg) transdermal (patch) estradiol, or 2.5 mcg ethinyl estradiol \u2014 works for many women. \nHormonal irregularities. If you need contraception and don't smoke, you can take low-dose birth control pills until menopause (see \"Irregular periods and heavy bleeding\"). Another advantage of these pills is that they regulate your menses and suppress the erratic hormonal ups and downs of perimenopause; some women report feeling more even-tempered while taking them. \nSevere hot flashes. Women with severe hot flashes who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. \nIrregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist.\n Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. \nVaginal dryness. Low-dose contraceptives or vaginal estrogen (in a cream, ring, tablet, or gel) can help relieve vaginal dryness, but hormonal treatment is not the only approach. Vaginal moisturizers such as Replens, applied twice weekly, increase vaginal moisture, elasticity, and acidity. Continued sexual activity also seems to improve vaginal tone and helps maintain the acidic environment that protects it against infections. Lubricants such as K-Y Jelly, Astroglide, and K-Y Silk-E can make intercourse less painful.  \n(This article was first printed in a previous issue of the Harvard Women's Health Watch. For more information or to order, please go to /womens.) \n  Image: Highwaystarz-Photography/Getty Images    \n \n Share This Page  Share this page to Facebook    Share this page to Twitter    Share this page via Email    \n Print This Page \n Click to Print     \nDisclaimer: \n \n As a service to our readers, Harvard Health Publishing provides access to our library of archived content. Please note the date of last review or update on all articles.  \n No content on this site, regardless of date, should ever be used as a substitute for direct medical advice from your doctor or other qualified clinician.       \n \n \nRecent Articles \n \n  \n   \n \nLet's not call it cancer    \n   \n \nChair exercises for seniors: Boosting strength, flexibility, and stamina    \n   \n \nWhy all the buzz about inflammation \u2014 and just how bad is it?    \n   \n \nChronic kidney disease: What to know about this common, serious condition    \n   \n \nBeyond the usual suspects for healthy resolutions    \n   \n \nCOVID tests: Do at-home tests work on newer variants?    \n   \n \nA liquid biopsy for metastatic prostate cancer    \n   \n \nOne way to combat loneliness? Strengthen relationships you already have    \n   \n \nFeel like you should be drinking less? Start here    \n   \n \nGot a pinched nerve? Strategies and treatments for pain relief    \n  /      \n \n \nFree Healthbeat Signup \n \nGet the latest in health news delivered to your inbox!    Sign Up         \n \n \nFooter \n  Harvard Health Publishing   \n  Facebook    Twitter    Linkedin    YouTube     \n \n \n \nMy Account \n \n  Customer Service    \n  Log in      \n \nOrder Now \n \n  Online Learning Courses    \n  Digital Subscriptions    \n  Special Health Reports    \n  Print Subscriptions      \n \nMore \n \n  About Us    \n  Permissions    \n  Content Licensing    \n  Topics    \n  Trademark Notice        \n \n \n \u00a9 2025 Harvard Health Publishing\u00ae of The President and Fellows of Harvard College  \nDo not sell my personal information | Privacy Policy and Terms of Use    \n  Scroll To Top      \n \n \n \n  Close   \n \nThanks for visiting. Don't miss your FREE gift. \nThe Best Diets for Cognitive Fitness, is yours absolutely FREE when you sign up to receive Health Alerts from Harvard Medical School \nSign up to get tips for living a healthy lifestyle, with ways to  fight inflammation and improve cognitive health, plus the latest advances in preventative medicine, diet and exercise, pain relief, blood pressure and cholesterol management, and more.  \n \n   \n    \n I want to get healthier \n     \n \n This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.       \n \n \n  Close   \nHealth Alerts from Harvard Medical School \nGet helpful tips and guidance for everything from fighting inflammation to finding the best diets for weight loss...from exercises to build a stronger core to advice on treating cataracts. PLUS, the latest news on medical advances and breakthroughs from Harvard Medical School experts.\n \n \n \nBONUS! Sign up now and\n get a FREE copy of the\nBest Diets for Cognitive Fitness    \n \n   \n    \n I want to get healthier \n     \n \n This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.       \n \n \n  Close  \n  \n \nStay on top of latest health news from Harvard Medical School.   \nPlus, get a FREE copy of the Best Diets for Cognitive Fitness. \n \n   \n    \n Sign me up \n          \n \n \n  Close  \n \n  \n \nStay on top of latest health news from Harvard Medical School. \nPlus, get a FREE copy of the Best Diets for Cognitive Fitness.   \n \n   \n    \n Sign me up \n     \n \n This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.             ", "title": "Perimenopause: Rocky road to menopause - Harvard Health", "description": "By our late 30s, we don&#x27;t produce as much progesterone. The number and quality of follicles also diminishes, causing a decline in estrogen production and fewer ovulations. As a result, <strong>by our 40s, cycle length and menstrual flow may vary and periods may become irregular</strong>.", "usage": {"tokens": 3407}, "retrieved_text": ["equine estrogen, 0.5 mg oral micronized estradiol, 25 micrograms (mcg) transdermal (patch) estradiol, or 2.5 mcg ethinyl estradiol \u2014 works for many women. Hormonal irregularities. If you need contraception and don't smoke, you can take low-dose birth control pills until menopause (see \"Irregular periods and heavy bleeding\"). Another advantage of these pills is that they regulate your menses and suppress the erratic hormonal ups and downs of perimenopause; some women report feeling more even-tempered while taking them. Severe hot flashes. Women with severe hot flashes who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis.", "with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist. Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. Vaginal dryness. Low-dose contraceptives or vaginal estrogen (in a cream, ring, tablet, or gel) can help relieve vaginal dryness, but hormonal treatment is not the only approach. Vaginal moisturizers such as Replens, applied twice weekly, increase vaginal moisture, elasticity, and acidity. Continued sexual activity also seems to improve vaginal tone and helps maintain the acidic environment that protects it against infections. Lubricants such as K-Y Jelly, Astroglide, and K-Y Silk-E can make intercourse less painful.", "who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist. Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. Vaginal dryness."]}]}, "retrieved_search_results": {"Perimenopause can lead to irregular periods in women.": [{"text": "Home/\nHealth Library/\nDiseases & Conditions/Perimenopause\n\n\nAdvertisement\nAdvertisement\n\nPerimenopause\n\n\nPerimenopause is the transitional period before menopause. During perimenopause, levels of estrogen start to decrease. You may begin having menopause-like symptoms, such as hot flashes or irregular periods. Perimenopause can last for years.\n\n\n\n\n\n\nAdvertisement\n\n\n\nCleveland Clinic is a non-profit academic medical center. Advertising on our site helps support our mission. We do not endorse non-Cleveland Clinic products or services. Policy\n\n\n\nCare at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\nContentsOverviewSymptoms and CausesDiagnosis and TestsManagement and TreatmentPreventionOutlook / PrognosisLiving WithAdditional Common Questions\n\n\n\nContentsOverviewSymptoms and CausesDiagnosis and TestsManagement and TreatmentPreventionOutlook / PrognosisLiving WithAdditional Common Questions\n\n\n\nOverview\n\nPerimenopause is when you transition to menopause. You may have symptoms like irregular periods, changes in mood and hot flashes.\n\nWhat is perimenopause?\nPerimenopause is when your body starts transitioning to menopause. During this transition, your ovaries begin producing less hormones, causing your menstrual cycle to become erratic or irregular. In perimenopause, your body is moving toward the end of your reproductive years. It\u2019s a natural and normal progression, but it comes with both physical and emotional symptoms. Some of these symptoms can disrupt your life or make you uncomfortable.\n\n\nAdvertisement\n\n\n\nCleveland Clinic is a non-profit academic medical center. Advertising on our site helps support our mission. We do not endorse non-Cleveland Clinic products or services. Policy\nPerimenopause may begin as early as your mid-30s or as late as your mid-50s. Some people are in perimenopause for only a short time, while others are in it for several years. Even though your menstrual cycles are unpredictable and your hormone levels are declining, it\u2019s still possible to get pregnant in perimenopause.\nPerimenopause is a transitional time that ends in menopause. Menopause means your periods have ended. When you have no menstrual cycle for 12 months in a row, you have officially reached menopause.\nWhat are the hormonal changes during perimenopause?\nThe hormonal changes you experience during perimenopause are mostly due to declining estrogen levels. Your ovaries make estrogen, which plays a vital role in maintaining your reproductive system. As estrogen decreases, it throws off the balance with progesterone, another hormone your ovaries produce. It\u2019s common for hormone levels to fluctuate during perimenopause \u2014 to go up and down like a rollercoaster.\nWhen you reach menopause, your body makes so little estrogen that your ovaries no longer release eggs (ovulation). At this point, you stop having your period and you lose the ability to get pregnant.\nWhat is the typical age for perimenopause to start?\nPerimenopause begins about eight to 10 years before menopause. It usually starts in your mid-40s, but it can start earlier. Completing menopause before age 40 is called premature menopause. Some medical conditions or procedures cause early menopause. If there\u2019s no medical or surgical cause for premature menopause, it\u2019s called primary ovarian insufficiency.\n\n\nAdvertisement\n\n\nHow long does perimenopause last?\nThe average length of perimenopause is about four years, but it can last up to eight years. Some people may only be in this stage for a few months, while others will be in this transition phase for several years.\nHow do you know you\u2019re in perimenopause?\nIt\u2019s important to know that everyone\u2019s body is different, and everyone experiences signs of perimenopause differently.\nGenerally, the first sign of perimenopause is irregular periods. You may go from having predictable menstrual cycles to spotting randomly or missing periods altogether. A lot of people also experience the most common signs of menopause like hot flashes and vaginal dryness early into the menopause transition.\n\n\n\n\nSymptoms and Causes\n\nWhat are the symptoms of perimenopause?\nOnce your estrogen levels begin to decline, your body must adjust to the changes in hormones. How your body reacts to changing hormones can be different from someone else\u2019s, like how your friend\u2019s body reacts. Some people don\u2019t notice any physical changes other than skipped periods, while other people have symptoms like hot flashes and mood swings.\nEven though the symptoms vary, many people experience at least one of the following:\n\nIrregular periods or skipping periods.\nPeriods that are heavier or lighter than usual.\nChanges in mood like irritability, depression or mood swings.\nLow libido.\nHot flashes.\nNight sweats.\nVaginal dryness that causes discomfort during sex.\nUrinary urgency (needing to pee more frequently).\nSleep problems (insomnia).\n\n\n\nBecause some of these symptoms resemble symptoms of other conditions, it\u2019s a good idea to talk to your healthcare provider about what you\u2019re experiencing. They can confirm that the symptoms are due to perimenopause and not another health condition.\nWhat does perimenopause anxiety feel like?\nThere are only a few studies about anxiety specific to perimenopause. But healthcare providers do know that mental health conditions like depression are common in perimenopause. Many people also report feeling more anxious in perimenopause. Anxiety is a constant worry that may interrupt your daily life. Symptoms like sweating, nausea or muscle tension can also happen with anxiety. Perimenopause anxiety can make it hard to sleep and complete your daily routine. Talk to your healthcare provider if perimenopause is affecting your emotional well-being. They can recommend treatment with medication or behavioral therapy.\nWhat are periods like during perimenopause?\nYour body is producing less of the hormones that help you ovulate, so your periods can become irregular. Your menstrual cycle could become longer or shorter than usual. Your bleeding could also be heavier or lighter than normal. Some people also notice their premenstrual syndrome (PMS) symptoms get worse.\nDo you still ovulate during perimenopause?\nIf you\u2019re still getting a period, even an irregular one, you\u2019re still ovulating. Until you haven\u2019t menstruated for 12 consecutive months, you should assume your body is still ovulating.\n\n\nAdvertisement\n\n\nHow can I reduce my risk of perimenopause complications?\nIrregular periods are common and normal during perimenopause, but other conditions can also cause irregular vaginal bleeding. If any of the following apply to you, see a healthcare provider to rule out other causes:\n\nYour periods become very heavy or contain large blood clots.\nYou change pads or tampons every one to two hours.\nYour periods last several days longer than usual.\nYou spot or bleed between periods.\nYou bleed after sex.\nYour periods occur less than 21 days apart.\nYour provider may want to rule out other causes of abnormal vaginal bleeding. Potential causes of abnormal bleeding include:\n\nInfection.\nFibroids.\nBlood clotting disorders.\nPolyps.\nCancer.\n\n\nDiagnosis and Tests\n\nHow is perimenopause diagnosed?\nA healthcare provider may determine that you\u2019re transitioning to menopause based on a physical exam and your symptoms, age and medical history.\nBut you don\u2019t always need to see a healthcare provider for an official diagnosis. Many people notice the changes in their bodies but don\u2019t have severe symptoms. Other people end up contacting their healthcare provider because symptoms become intolerable and uncomfortable.\nWhat tests can diagnose perimenopause?\nHormone testing isn\u2019t necessary to diagnose perimenopause. Hormone levels fluctuate so much that the tests aren\u2019t reliable.\n\n\nAdvertisement\n\n\nBut certain hormone tests can tell your provider you\u2019re in perimenopause. For example, testing your FSH (follicle-stimulating hormone) level is one to see if perimenopause is beginning. FSH is a hormone your pituitary gland produces. It stimulates your ovaries to release an egg during ovulation.\nA consistently high level of FSH can indicate you\u2019re getting near menopause. But FSH tests can be misleading because during perimenopause, your hormones rise and fall erratically. Certain medications and health conditions interfere with hormone levels and will affect the results of any hormone tests.\nThere aren\u2019t any imaging tests that can confirm perimenopause, but your provider may order tests to rule out other conditions.\n\n\nManagement and Treatment\n\nCan perimenopause be treated?\nThere isn\u2019t any treatment to stop perimenopause. Perimenopause is a natural part of life. It ends when your periods stop entirely, and you reach menopause.\nBut your healthcare provider may recommend ways to help ease uncomfortable symptoms. Many people don\u2019t need medication and find that their symptoms are mild enough that lifestyle changes alone make a big difference. Still, other people find that taking medication relieves their symptoms and improves their quality of life.\n\n\nAdvertisement\n\n\nYour provider may recommend the following medications:\n\nAntidepressants: These medications help with hot flashes, mood swings, anxiety or depression.\nBirth control pills: These medications stabilize your hormone levels and typically relieve symptoms.\nHormone replacement therapy (HRT): HRT uses estrogen or estrogen and progesterone to increase your hormone levels if you\u2019ve gone into menopause before age 40.\nHormone therapy (HT): Similar to HRT, this therapy uses estrogen or estrogen and progesterone to increase your hormones. Providers use this term when referring to people going through menopause at a more natural age (any time after age 45).\nGabapentin (Neurontin\u00ae): This medicine is a seizure medication that also relieves hot flashes for most people.\nOxybutynin: A medication for overactive bladder that can also relieve hot flashes.\nFezolinetant (VEOZAH\u2122): This is a newer treatment for severe hot flashes.\nVaginal creams: Your provider can tell you about prescription and over-the-counter options. Treatment with vaginal creams and lubricants can decrease pain during sex and relieve vaginal dryness.\nYour healthcare provider will discuss the risks and benefits of perimenopause treatment with you and recommend the best option based on your needs.\nWhat lifestyle changes can I make to manage perimenopause symptoms?\nYou may be able to manage perimenopause symptoms at home without needing medication from your provider. Some tips for treating your symptoms without medication include:\n\nEat lots of fruits, vegetables, whole grains, lean protein and healthy fats.\nPerform weight-bearing exercises like walking, hiking or strength training.\nDress in layers so you can remove clothing if you begin to sweat or get hot flashes.\nUse a fan or keep your house at a cooler temperature.\nImprove sleep hygiene by avoiding TV and computer screens and doing relaxing activities before bed.\nLimit alcohol and caffeine.\nPractice meditation or other stress management techniques.\nQuit using tobacco products like cigarettes.\nMaintain a weight that\u2019s healthy for you.\nWhat makes perimenopause worse?\nThere isn\u2019t one specific thing that makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse:\n\nEating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause.\nAvoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems.\nSmoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness.\nTalk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being.\n\n\n\n\nCare at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\n\nPrevention\n\nWhat can put you into perimenopause early?\nCertain factors are linked to early perimenopause. These include:\n\nSmoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t.\nA family history of early menopause.\nA history of cancer treatment.\nIf you\u2019ve had surgery to remove your uterus (hysterectomy) or ovaries (oophorectomy).\n\n\nOutlook / Prognosis\n\nAre there any health risks associated with perimenopause?\nThere are health risks associated with menopause, which happens right after perimenopause.\nEstrogen plays an important role in preserving your bones. Osteoporosis is a condition where the insides of your bones become less dense and more fragile. This increases your risk for bone fractures. Your healthcare provider may recommend a multivitamin, calcium supplement, extra vitamin D or more weight-bearing exercises. People in menopause are also at an increased risk for heart disease and other cardiovascular (heart and blood vessel) health conditions.\nCan I get pregnant if I am in perimenopause?\nYes, you can still become pregnant. You may be less likely to get pregnant during perimenopause, but it\u2019s still possible. As long as you have a period, you can still get pregnant.\nWhen your periods are irregular, you may be more likely to get pregnant unexpectedly. If you don\u2019t want to expand your family, use birth control until your healthcare provider tells you it\u2019s safe to stop.\n\n\nLiving With\n\nWhat questions should I ask my healthcare provider?\nDiscuss your perimenopause symptoms with your healthcare provider. It might help to keep a journal of your menstrual cycles, including when they start and stop and the amount of bleeding.\nSome questions you should ask are:\n\nAre these symptoms of perimenopause?\nWhat can I do to relieve my symptoms?\nHow long do you think I will have these symptoms?\nWould hormone therapy be an option for me?\nDo I need to start taking medication or vitamins?\nAre there any tests that should be done?\nWhen should I call my provider?\nIf your perimenopausal symptoms are unbearable or interfering with your quality of life, it might be time to contact a healthcare provider. They can recommend a treatment to reduce the intensity of your symptoms and get you feeling better.\n\n\nAdditional Common Questions\n\nCan perimenopause affect my sex life?\nPossibly. Certain symptoms of perimenopause like vaginal dryness and discomfort during sex can make having sex less desirable. Vaginal lubricants can help with dryness. Speak with your healthcare provider if you have issues with your sex drive so they can recommend ways to help you.\nDoes perimenopause affect my sleep?\nYes, perimenopause can affect your sleep. People in perimenopause may experience insomnia and interrupted sleep due to night sweats or other menopausal symptoms.\nWhat do hot flashes feel like during perimenopause?\nA hot flash feels like a sudden warmth all over your body. It\u2019s often accompanied by sweating and a red, flushed face. Low estrogen levels cause hot flashes, and they can last months or years.\nWhy am I gaining weight during perimenopause?\nThe shift in hormones slows down your metabolism. It\u2019s very common to gain weight once your estrogen levels start to decline. Eating healthy and nutritious foods and getting regular physical activity can help prevent weight gain during the transition to menopause.\nA note from Cleveland Clinic\n\nPerimenopause is the transition to menopause. It\u2019s a gradual process that can last years and it\u2019s a different experience for everyone. Starting this new chapter of your life, where your reproductive years are ending, can be emotionally and physically challenging. You may start having menopause-like symptoms, such as irregular periods, weight changes or mood swings. Let your healthcare provider know if you believe you\u2019ve begun perimenopause. They can help reassure you that your symptoms are normal and talk to you about the health risks of menopause like osteoporosis.\nYour healthcare provider can also help you manage symptoms of perimenopause if they start to interfere with your life. They can recommend medications or other treatments to make the transition easier on you. \n\n\n\n\n\n\nMedically Reviewed\nLast reviewed on 08/08/2024.\nLearn more about the Health Library and our editorial process.\n\nReferences\n\nAdvertisement\nAdvertisement\n\nAd\n\n\n\nAppointments 216.444.6601\nAppointments & Locations\nRequest an Appointment\nRendered: Fri Jan 10 2025 13:25:25 GMT+0000 (Coordinated Universal Time)\n\n\n\nActionsAppointments & AccessAccepted InsuranceEvents CalendarFinancial AssistanceGive to Cleveland ClinicPay Your Bill OnlineRefer a PatientPhone DirectoryVirtual Second OpinionsVirtual Visits\n\nBlog, News & AppsConsult QDHealth EssentialsNewsroomMyClevelandClinicMyChart\n\nAbout Cleveland Clinic100 Years of Cleveland ClinicAbout UsLocationsQuality & Patient SafetyOffice of Diversity & InclusionPatient ExperienceResearch & InnovationsCommunity CommitmentCareersFor EmployeesResources for Medical Professionals\n\nSite Information & PoliciesSend Us FeedbackSite MapAbout this WebsiteCopyright, Reprint & LicensingWebsite Terms of UsePrivacy PolicyNotice of Privacy PracticesNon-Discrimination Notice\n9500 Euclid Avenue, Cleveland, Ohio 44195 | 800.223.2273 | \u00a9 2025 Cleveland Clinic. All Rights Reserved. ", "title": "Perimenopause: Age, Stages, Signs, Symptoms & Treatment", "description": "<strong>Perimenopause</strong> is the transition to menopause when hormone levels decline and symptoms like hot flashes and <strong>irregular</strong> <strong>periods</strong> start. <strong>Perimenopause</strong> <strong>can</strong> last years.", "usage": {"tokens": 3532}, "retrieved_text": ["makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse: Eating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause. Avoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems. Smoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being. Care at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\n\nPrevention\n\nWhat can put you into perimenopause early? Certain factors are linked to early perimenopause. These include:\n\nSmoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t. A family history of early menopause. A history of cancer treatment.", "a cooler temperature. Improve sleep hygiene by avoiding TV and computer screens and doing relaxing activities before bed. Limit alcohol and caffeine. Practice meditation or other stress management techniques. Quit using tobacco products like cigarettes. Maintain a weight that\u2019s healthy for you. What makes perimenopause worse? There isn\u2019t one specific thing that makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse: Eating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause. Avoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems. Smoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being.", "lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being. Care at Cleveland Clinic Menopause Treatment Find a Doctor and Specialists Make an Appointment Prevention What can put you into perimenopause early? Certain factors are linked to early perimenopause. These include: Smoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t. A family history of early menopause. A history of cancer treatment. If you\u2019ve had surgery to remove your uterus (hysterectomy) or ovaries (oophorectomy). Outlook / Prognosis\n\nAre there any health risks associated with perimenopause? There are health risks associated with menopause, which happens right after perimenopause. Estrogen plays an important role in preserving your bones. Osteoporosis is a condition where the insides of your bones become less dense and more fragile. This increases your risk for bone fractures. Your healthcare provider may recommend a multivitamin, calcium supplement, extra vitamin D or more weight-bearing exercises."]}, {"text": "  \n \n   \n     \n\nThis content does not have an English version.\n\nThis content does not have an Arabic version. \n \n\n\n\n\n\n  \n\n\n\n\n\n\n    \n    \n \n \n \n \nPrint  \n \nOverview\nPerimenopause means \"around menopause\" and refers to the time during which your body makes the natural transition to menopause, marking the end of the reproductive years. Perimenopause is also called the menopausal transition. \nWomen start perimenopause at different ages. You may notice signs of progression toward menopause, such as menstrual irregularity, sometime in your 40s. But some women notice changes as early as their mid-30s.\n\n \nThe level of estrogen \u2014 the main female hormone \u2014 in your body rises and falls unevenly during perimenopause. Your menstrual cycles may lengthen or shorten, and you may begin having menstrual cycles in which your ovaries don't release an egg (ovulate). You may also experience menopause-like symptoms, such as hot flashes, sleep problems and vaginal dryness. Treatments are available to help ease these symptoms. \nOnce you've gone through 12 consecutive months without a menstrual period, you've officially reached menopause, and the perimenopause period is over.\n\nProducts & Services\n\n\nA Book: The New Rules of Menopause \n \n \n   \nSymptoms\nThroughout the menopausal transition, some subtle \u2014 and some not-so-subtle \u2014 changes in your body may take place. You might experience: \n \nIrregular periods. As ovulation becomes more unpredictable, the length of time between periods may be longer or shorter, your flow may be light to heavy, and you may skip some periods. If you have a persistent change of seven days or more in the length of your menstrual cycle, you may be in early perimenopause. If you have a space of 60 days or more between periods, you're likely in late perimenopause. \nHot flashes and sleep problems. Hot flashes are common during perimenopause. The intensity, length and frequency vary. Sleep problems are often due to hot flashes or night sweats, but sometimes sleep becomes unpredictable even without them. \nMood changes. Mood swings, irritability or increased risk of depression may happen during perimenopause. The cause of these symptoms may be sleep disruption associated with hot flashes. Mood changes may also be caused by factors not related to the hormonal changes of perimenopause. \nVaginal and bladder problems. When estrogen levels diminish, your vaginal tissues may lose lubrication and elasticity, making intercourse painful. Low estrogen may also leave you more vulnerable to urinary or vaginal infections. Loss of tissue tone may contribute to urinary incontinence. \nDecreasing fertility. As ovulation becomes irregular, your ability to conceive decreases. However, as long as you're having periods, pregnancy is still possible. If you wish to avoid pregnancy, use birth control until you've had no periods for 12 months. \nChanges in sexual function. During perimenopause, sexual arousal and desire may change. But if you had satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. \nLoss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. \nChanging cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease.  \nWhen to see a doctor\nSome women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention. Because symptoms may be subtle and come on gradually, you may not realize at first that they're all connected to the same thing \u2014 hormone fluctuations of the menopausal transition. \nIf you have symptoms that interfere with your life or well-being, such as hot flashes, mood swings or changes in sexual function that concern you, see your doctor. \nRequest an appointment \n \n  \n \n \n \n    \n \n \n \n \nThere is a problem with information submitted for this request. Review/update the information highlighted below and resubmit the form.  \nFrom Mayo Clinic to your inbox  \nSign up for free and stay up to date on research advancements, health tips, current health topics, and expertise on managing health. Click here for an email preview. \n \n           Email   \n \n Address 1    \nErrorEmail field is required \nErrorInclude a valid email address  \nLearn more about Mayo Clinic\u2019s use of data. \n \nTo provide you with the most relevant and helpful information, and understand which information is beneficial, we may combine your email and website usage information with other information we have about you. If you are a Mayo Clinic patient, this could include protected health information. If we combine this information with your protected health information, we will treat all of that information as protected health information and will only use or disclose that information as set forth in our notice of privacy practices. You may opt-out of email communications at any time by clicking on the unsubscribe link in the e-mail.  \n \nSubscribe!   \n \n \nThank you for subscribing! \n \nYou'll soon start receiving the latest Mayo Clinic health information you requested in your inbox.  \n \nSorry something went wrong with your subscription \nPlease, try again in a couple of minutes \n \nRetry      \n \n \n \n \n \n        \nCauses\nAs you go through perimenopause, your body's production of estrogen and progesterone, key female hormones, rises and falls. Many of the changes you experience during perimenopause are a result of decreasing estrogen. \nRisk factors\nMenopause is a normal phase in life. But it may occur earlier in some women than in others. Although not always conclusive, some evidence suggests that certain factors may make it more likely that you start perimenopause at an earlier age, including: \n \nSmoking. The onset of menopause occurs 1 to 2 years earlier in women who smoke than in women who don't smoke. \nFamily history. Women with a family history of early menopause may experience early menopause themselves. \nCancer treatment. Treatment for cancer with chemotherapy or pelvic radiation therapy has been linked to early menopause. \nHysterectomy. A hysterectomy that removes your uterus, but not your ovaries, usually doesn't cause menopause. Although you no longer have periods, your ovaries still produce estrogen. But such surgery may cause menopause to occur earlier than average. Also, if you have one ovary removed, the remaining ovary might stop working sooner than expected.  \nComplications\nIrregular periods are a hallmark of perimenopause. Most of the time this is normal and nothing to be concerned about. However, see your doctor if: \n \nBleeding is extremely heavy \u2014 you're changing tampons or pads every hour or two for two or more hours \nBleeding lasts longer than seven days \nBleeding occurs between periods \nPeriods regularly occur less than 21 days apart  \nSigns such as these may mean there's a problem with your reproductive system that requires diagnosis and treatment. \n \n   \n \n \n    \nBy Mayo Clinic Staff  \n \n Perimenopause care at Mayo Clinic   \nRequest an appointment \n \nDiagnosis & treatment \n May 25, 2023  \nPrint  \n\nShow references\n\n \nLobo RA, et al. Menopause and care of the mature woman: Endocrinology, consequences of estrogen deficiency, effects of hormone therapy, and other treatment options. In: Comprehensive Gynecology. 7th ed. Elsevier; 2017. https://www.clinicalkey.com. Accessed March 5, 2021. \nAskMayoExpert. Menopausal hormone therapy (adult). Mayo Clinic; 2020. \nAskMayoExpert. Bioidentical hormones. Mayo Clinic; 2019. \nDHEA. Natural Medicines. https://naturalmedicines.therapeuticresearch.com/. Accessed March 5, 2021. \nBlack cohosh. Natural Medicines. https://naturalmedicines.therapeuticresearch.com/. Accessed March 5, 2021. \nFerri FF. Menopause. In: Ferri's Clinical Advisor 2021. Elsevier; 2021. https://www.clinicalkey.com. Accessed March 5, 2021. \nMenopausal symptoms: In depth. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/menopausal-symptoms-in-depth. Accessed March 5, 2021. \nDelamater L, et al. Management of the perimenopause. Clinical Obstetrics and Gynecology. 2018; doi:10.1097/GRF.0000000000000389. \nSuss H, et al. Psychological resilience during the perimenopause. Maturitas. 2020; doi:10.1016/j.maturitas.2019.10.015. \nRaglan GB, et al. Depression during perimenopause: The role of the obstetrician-gynecologist. Archives of Women's Mental Health. 2020; doi:10.1007/s00737-019-0950-6. \nBacon JL. The menopausal transition. Obstetrics and Gynecology Clinics of North America. 2017; doi:10.1016/j.ogc.2018.02.008. \nTop questions about menopause. Office on Women's Health. https://www.womenshealth.gov/menopause. Accessed March 5, 2021. \nWarner KJ. Allscripts EPSi. Mayo Clinic. Feb. 25, 2021. \nJohnson A, et al. Complementary and alternative medicine for menopause. Journal of Evidence-Based Integrative Medicine. 2019; doi:10.1177/2515690X19829380. \nMinkin MJ. Menopause: Hormones, lifestyle, and optimizing aging. Obstetrics and Gynecology Clinics of North America. 2019; doi:10.1016/j.ogc.2019.04.008. \nAmerican College of Obstetricians and Gynecologists. Practice Bulletin No. 128: Diagnosis of abnormal uterine bleeding in reproductive-aged women. Obstetrics and Gynecology. 2012; doi:10.1097/AOG.0b013e318262e320. Reaffirmed 2016. \nAmerican College of Obstetricians and Gynecologists. Practice Bulletin No. 141: Management of menopausal symptoms. Obstetrics and Gynecology. 2014; doi:10.1097/01.AOG.0000441353.20693.78. Reaffirmed 2018. \nBlack cohosh. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/black-cohosh. Accessed March 8, 2021. \nBasu P, et al. Phytoestrogens and breast cancer: In vitro anticancer activities of isoflavones, lignans, coumestans, stilbenes and their analogs and derivatives. Biomedicine and Pharmacotherapy. 2018; doi:10.1016/j.biopha.2018.08.100. \nBurnett TL (expert opinion). Mayo Clinic. May 5, 2021. \nZhao T-T, et al. Dietary isoflavones or isoflavoine-rich food intake and breast cancer risk: A meta-analysis of prospective cohort studies. Clinical Nutrition. 2019; doi:10.1016/j.clnu.2017.12.006. \nFDA approves novel drug to treat moderate to severe hot flashes caused by menopause. U.S. Food and Drug Administration. https://www.fda.gov/news-events/press-announcements/fda-approves-novel-drug-treat-moderate-severe-hot-flashes-caused-menopause. Accessed May 16, 2023. \nVeozah (prescribing information). https://www.accessdata.fda.gov/drugsatfda_docs/label/2023/216578s000lbl.pdf. Accessed May 16, 2023.    \n \nRelated \n \nAssociated Procedures \n \n Endometrial ablation    \n \nNews from Mayo Clinic \n \n Perimenopause, menopause and ... weightlifting? Expert explains value for bone health Sept. 04, 2024, 03:15 p.m. CDT  \n Mayo Clinic Minute: Perimenopause and menopause May 13, 2024, 04:00 p.m. CDT  \n Mayo Clinic Q and A: Perimenopause transitions and concerns July 20, 2023, 03:00 p.m. CDT    \n \nProducts & Services \n \n A Book: The New Rules of Menopause        \n \nPerimenopause\n\nSymptoms\n&\ncauses\nDiagnosis\n&\ntreatment\nDoctors\n&\ndepartments\nCare at\nMayo\nClinic   \n  \n \n Advertisement \n  \n \nMayo Clinic does not endorse companies or products. Advertising revenue supports our not-for-profit mission. Advertising & Sponsorship \n \nPolicy \nOpportunities \nAd Choices    \n \nMayo Clinic Press \nCheck out these best-sellers and special offers on books and newsletters from Mayo Clinic Press. \n\nNEW: Listen to Health Matters Podcast - Mayo Clinic PressNEW: Listen to Health Matters Podcast\nMayo Clinic on Incontinence - Mayo Clinic PressMayo Clinic on Incontinence\nThe Essential Diabetes Book - Mayo Clinic PressThe Essential Diabetes Book\nMayo Clinic on Hearing and Balance - Mayo Clinic PressMayo Clinic on Hearing and Balance\nFREE Mayo Clinic Diet Assessment - Mayo Clinic PressFREE Mayo Clinic Diet Assessment\nMayo Clinic Health Letter - FREE book - Mayo Clinic PressMayo Clinic Health Letter - FREE book  \n     \n   \n  \nCON-20208696 \n \n \nPatient Care & Health Information \nDiseases & Conditions \nPerimenopause         \n \n\n\n\n\n\n\n\n\n\n\n\nDon\u2019t miss your chance!\n\n Give Now\n\n\n\nGive by Dec. 31 to multiply your impact 3X. \n\n Give Now\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Find a doctor\n\n Explore careers\n\n Sign up for free e-newsletters\n\n\n\n\n\n\n\n\nAbout Mayo Clinic  \n\n\n\n\n About this Site\n\n Contact Us\n\n Locations\n\n Health Information Policy\n\n Medicare Accountable Care Organization (ACO)\n\n Media Requests\n\n News Network\n\n Price Transparency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedical Professionals \n\n\n\n\n AskMayoExpert\n\n Clinical Trials\n\n Mayo Clinic Alumni Association\n\n Refer a Patient\n\n\n\n\n\n\n\n\nBusinesses  \n\n\n\n\n Executive Health Program\n\n International Business Collaborations\n\n Facilities & Real Estate\n\n Supplier Information\n\n\n\n\n\n\n\n\nStudents   \n\n\n\n\n Admissions Requirements\n\n Degree Programs\n\n Student & Faculty Portal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearchers  \n\n\n\n\n Research Faculty\n\n Laboratories\n\n\n\n\n\n\n\n\nInternational Patients \n\n\n\n\n Appointments\n\n Financial Services\n\n International Locations & Offices\n\n\n\n\n\n\n\n\nCharitable Care & Financial Assistance \n\n Community Health Needs Assessment\n\n\n\n\n Financial Assistance Documents \u2013 Arizona\n\n Financial Assistance Documents \u2013 Florida\n\n Financial Assistance Documents \u2013 Minnesota\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollow Mayo Clinic\n\n\n\n\n X\n\n YouTube\n\n Facebook\n\n LinkedIn\n\n Instagram\n\n\n\n\n\n\n\n\n\n\n\nGet the Mayo Clinic app\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Terms & Conditions\n\n Privacy Policy\n\n Notice of Privacy Practices\n\n Notice of Nondiscrimination\n\n Accessibility Statement\n\n Advertising & Sponsorship Policy\n\n Site Map\n\n Manage Cookies\n\n\n\n\n\n Terms and Conditions\n\n Privacy Policy\n\n Notice of Privacy Practices\n\n Notice of Nondiscrimination\n\n Accessibility Statement\n\n Advertising and Sponsorship Policy\n\n Site Map\n\n Manage Cookies\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage:\nEnglish\n\n\n\n English\n\n Espa\u00f1ol\n\n \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n\n \u7b80\u4f53\u4e2d\u6587\n\n\n\n\n\u00a9 1998-2025 Mayo Foundation for Medical Education and Research (MFMER). All rights reserved. \n\nLanguage:\nEnglish\n\n\n\n English\n\n Espa\u00f1ol\n\n \u0627\u0644\u0639\u0631\u0628\u064a\u0629\n\n \u7b80\u4f53\u4e2d\u6587\n\n\n\n    \n       \n \n \n    \n \nMayo Clinic Privacy Policy \n Mayo Clinic and our partners use technologies such as cookies to collect information from your browser to deliver relevant advertising on our site, in emails and across the Internet, personalize content and perform site analytics. Please visit our Privacy Policy for more information about our use of data and your rights. \nClick here to update your preferences \n \nI Agree  \n \nx \nCustomize Cookie Settings \n \n Trusted \nThese technologies are used in operating the site, including remembering your preferences, ensuring security and providing certain features. They do not track your activity online.   \n \n   \n Website Analytics \nThese technologies collect information to help us understand how our websites are being used and to make improvements.   \n \n   \n Advertising \nThese technologies are used to make advertising messages more relevant to you. They perform functions like preventing the same ad from continuously reappearing, ensuring that ads are properly displayed for advertisers and selecting advertisements that are based on your interests.   \n \nSave   ", "title": "Perimenopause - Symptoms and causes - Mayo Clinic", "description": "Discover effective treatments and learn self-care strategies for hot flashes, night sweats, vaginal dryness and other <strong>perimenopause</strong> symptoms.", "usage": {"tokens": 3510}, "retrieved_text": ["you wish to avoid pregnancy, use birth control until you've had no periods for 12 months. Changes in sexual function. During perimenopause, sexual arousal and desire may change. But if you had satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. Loss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. Changing cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease. When to see a doctor\nSome women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention.", "satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. Loss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. Changing cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease. When to see a doctor Some women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention. Because symptoms may be subtle and come on gradually, you may not realize at first that they're all connected to the same thing \u2014 hormone fluctuations of the menopausal transition. If you have symptoms that interfere with your life or well-being, such as hot flashes, mood swings or changes in sexual function that concern you, see your doctor.", "Gynecology Clinics of North America. 2019; doi:10.1016/j.ogc.2019.04.008. American College of Obstetricians and Gynecologists. Practice Bulletin No. 128: Diagnosis of abnormal uterine bleeding in reproductive-aged women. Obstetrics and Gynecology. 2012; doi:10.1097/AOG.0b013e318262e320. Reaffirmed 2016. American College of Obstetricians and Gynecologists. Practice Bulletin No. 141: Management of menopausal symptoms. Obstetrics and Gynecology. 2014; doi:10.1097/01.AOG.0000441353.20693.78. Reaffirmed 2018. Black cohosh. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/black-cohosh. Accessed March 8, 2021. Basu P, et al. Phytoestrogens and breast cancer: In vitro anticancer activities of isoflavones, lignans, coumestans, stilbenes and their analogs and derivatives. Biomedicine and Pharmacotherapy. 2018; doi:10.1016/j.biopha.2018.08.100. Burnett TL (expert opinion). Mayo Clinic. May 5, 2021. Zhao T-T, et al. Dietary isoflavones or isoflavoine-rich food intake and breast cancer risk: A meta-analysis of prospective cohort studies. Clinical Nutrition. 2019; doi:10.1016/j.clnu.2017.12.006. FDA approves novel drug to treat moderate to severe hot flashes caused by menopause. U.S. Food and Drug Administration. https://www.fda.gov/news-events/press-announcements/fda-approves-novel-drug-treat-moderate-severe-hot-flashes-caused-menopause. Accessed May 16, 2023. Veozah (prescribing information). https://www.accessdata.fda.gov/drugsatfda_docs/label/2023/216578s000lbl.pdf. Accessed May 16, 2023. Related \n \nAssociated Procedures \n \n Endometrial ablation    \n \nNews from Mayo Clinic \n \n Perimenopause, menopause and ... weightlifting?"]}, {"text": "Health Conditions\n\n\nHealth ConditionsAll\n\nBreast Cancer\nCancer Care\nCaregiving for Alzheimer's Disease\nChronic Kidney Disease\nChronic Obstructive Pulmonary Disease (COPD)\nDigestive Health\nEye Health\nHeart Health\nMenopause\nMental Health\nMigraine\nMultiple Sclerosis (MS)\nPsoriasis\nRheumatoid Arthritis (RA)\nSleep Health\nType 2 Diabetes\nWeight Management\n\nCondition SpotlightAll\n\n\n\n\nControlling Ulcerative Colitis\n\n\n\nNavigating Life with Bipolar Disorder\n\n\n\nMastering Geographic Atrophy\n\n\n\nManaging Type 2 Diabetes\n\nWellness\n\n\nWellness TopicsAll\n\nCBD\nFitness\nHearing\nMental Well-Being\nNutrition\nParenthood\nRecipes\nSexual Health\nSkin Care\nSleep Health\nVitamins and Supplements\nWomen's Wellness\n\nProduct ReviewsAll\n\nAt-Home Testing\nMen's Health\nMental Health\nNutrition\nSleep\nVitamins and Supplements\nWomen's Health\n\nFeatured ProgramsAll\n\n\n\n\nYour Eczema Skin Care Roadmap\n\n\n\nInflammation and Aging\n\n\n\nCold & Flu Season Survival Guide\n\n\n\nShe\u2019s Good for Real\n\nTools\n\n\nFeatured\n\nVideo Series\nPill Identifier\nFindCare\nDrugs A-Z\n\nLessonsAll\n\nCrohn\u2019s and Ulcerative Colitis Essentials\nDiabetes Nutrition\nHigh Cholesterol\nTaming Inflammation in Psoriasis\nTaming Inflammation in Psoriatic Arthritis\n\nNewslettersAll\n\nAnxiety and Depression\nDigestive Health\nHeart Health\nMigraine\nNutrition Edition\nType 2 Diabetes\nWellness Wire\n\nLifestyle Quizzes\n\nFind a Diet\nFind Healthy Snacks\nWeight Management\nHow Well Do You Sleep?\nAre You a Workaholic?\n\nConnect\n\n\nFind Your Bezzy Community\nBezzy communities provide meaningful connections with others living with chronic conditions. Join Bezzy on the web or mobile app.All\n\n\n\n\nBreast Cancer\n\n\n\nMultiple Sclerosis\n\n\n\nDepression\n\n\n\nInflammatory Bowel Disease\n\n\n\nType 2 Diabetes\n\n\n\nPsoriasis\n\nFollow us on social media\nCan't get enough? Connect with us for all things health.\n\n\n\nSubscribe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Perimenopause Can Affect Your Periods and What You Can Do\n\n\n\n\nMedically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014  Updated on March 27, 2023\n\n\nSpotting\nHeavy bleeding\nDark blood\nShort cycles\nLong cycles\nMissed cycles\nIrregularity\nWhen to see your doctor\n\n\n\n\n\n\nDuring perimenopause, your levels of the hormones estrogen and progesterone fluctuate. These changes can affect your cycle, leading to irregular or missed periods, and more.\n\n\n\n\n\n\n\n\n\nUnderstanding perimenopause\n\nMenopause refers to the end of your menstrual cycle. Once you\u2019ve gone 12 months without a period, you\u2019ve reached menopause.\n\nThe average woman goes through menopause at 51 years old. The time period before menopause is called perimenopause.\n\nPerimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. \n\nThese shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns.\n\nOther symptoms of perimenopause include:\n\n\nhot flashes\nnight sweats\nsleep troubles\nmemory issues\ndifficulty urinating\nvaginal dryness\nchanges in sexual desire or satisfaction\n\nHere\u2019s what you can expect from perimenopause and what you can do.\n\n\n\n\n1. Spotting between periods\n\nIf you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting.\n\nSpotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining.\n\nMany women spot before their period starts or as it ends. Mid-cycle spotting around ovulation is also common.\n\nIf you\u2019re regularly spotting every 2 weeks, it may be a sign of a hormonal imbalance. You may want to speak with your healthcare provider.\n\nWhat you can do\n\nConsider keeping a journal to track your periods. Include information such as: \n\n\nwhen they start\nhow long they last\nhow heavy they are\nwhether you have any in-between spotting\n\nYou can also log this information in an app, like Eve.\n\nWorried about leaks and stains? Consider wearing panty liners. Disposable panty liners are available at most drugstores. They come in a variety of lengths and materials. \n\nYou can even buy reusable liners that are made of fabric and can be washed over and over again.\n\n\n\n\nProducts to try\n\nIf you deal with spotting between periods, using certain products can help you track your symptoms and avoid leaks and stains. Shop for them online:\n\nperiod journal\npanty liners\nreusable panty liners\n\n\n\n\n\n\n2. Abnormally heavy bleeding\n\nWhen your estrogen levels are high in comparison to your progesterone levels, your uterine lining builds. This results in heavier bleeding during your period as your lining sheds.\n\nA skipped period can also cause the lining to build up, leading to heavy bleeding.\n\nBleeding is considered heavy if it:\n\n\nsoaks through one tampon or pad an hour for several hours\nrequires double protection \u2014 such as a tampon and pad \u2014 to control menstrual flow\ncauses you to interrupt your sleep to change your pad or tampon\nlasts longer than 7 days\n\nWhen bleeding is heavy, it may last longer, disrupting your everyday life. You may find it uncomfortable to exercise or carry on with your normal tasks. \n\nHeavy bleeding can also cause fatigue and increase your risk for other health concerns, such as anemia.\n\nWhat you can do\n\nAs you may know, taking ibuprofen (Advil, Midol, Motrin) during your period can help with menstrual cramps. \n\nIf you take it when you\u2019re bleeding heavily, it may also reduce your flow. Try taking 200 milligrams (mg) every 4 to 6 hours during the day.\n\nIf cramps and pain continue, talk to your healthcare provider about hormonal approaches to treatment. Some women have a medical or family history that discourages the use of hormones in the perimenopausal period.\n\n\n\n\n3. Brown or dark blood\n\nThe colors you see in your menstrual flow can range from bright red to dark brown, especially toward the end of your period. Brown or dark blood is a sign of old blood exiting the body.\n\nWomen in perimenopause may also see brown spotting or discharge at other times throughout the month.\n\nYou may also notice changes in discharge texture. Your discharge may be thin and watery, or it may be clumpy and thick.\n\nWhat you can do\n\nIf you\u2019re concerned about your menstrual flow, you may want to schedule an appointment to see your doctor.\n\nThe variation in color is usually due to the amount of time it takes for the blood and tissue to cycle out of the body, but it can sometimes be a sign of another underlying condition. \n\nIf there\u2019s a foul odor to the vaginal discharge, it may be a sign of infection. See your healthcare provider.\n\n\n\n4. Shorter cycles\n\nWhen your estrogen levels are low, your uterine lining is thinner. Bleeding, as a result, may be lighter and last fewer days. Short cycles are more common in the earlier stages of perimenopause.\n\nFor example, you may have a period that\u2019s 2 or 3 days shorter than normal. Your whole cycle may also last 2 or 3 weeks instead of 4. It isn\u2019t uncommon to feel like your period just ended when the next one comes.\n\nWhat you can do\n\nIf you\u2019re worried about short, unpredictable cycles, consider leakage protection such as liners, pads, or period underwear like Thinx.\n\nPass on tampons and menstrual cups unless you have a menstrual flow. Insertion can be difficult or uncomfortable without this lubrication. You\u2019re also more likely to forget to change your tampon or cup, increasing your risk for complications.\n\n\n\n\nProducts to try\n\nIf your periods are unpredictable, you can protect yourself from stains with leakage protection products. Shop for them online:\n\npanty liners\npads\nperiod underwear\n\n\n\n\n\n\n\n\n\n\n\n\n5. Longer cycles\n\nIn the later stages of perimenopause, your cycles may become much longer and farther apart. Longer cycles are defined as those longer than 38 days. They\u2019re related to anovulatory cycles, or cycles in which you don\u2019t ovulate.\n\nA 2008 study suggests that women who experience anovulatory cycles may have lighter bleeding than women who experience ovulatory cycles.\n\nWhat you can do\n\nIf you\u2019re dealing with longer cycles, it may be time to invest in a good menstrual cup or a cycle set of blood-wicking underwear. You can also use pads or tampons to help you avoid leakage.\n\n\n\n\nProducts to try\n\nIf you have a long cycle, a variety of products are available to help you avoid leakage. Shop for them online:\n\nmenstrual cups\na cycle set of blood-wicking underwear, like these from Thinx and Awwa\npads\ntampons\n\n\n\n\n\n\n6. Missed cycles\n\nYour fluctuating hormones may also be to blame for a missed cycle. In fact, your cycles may become so far apart that you can\u2019t recall the last time you bled. After you\u2019ve missed 12 consecutive cycles, you\u2019ve reached menopause.\n\nIf your cycles are still making an appearance \u2014 however delayed \u2014 ovulation is still occurring. This means you can still have a period, and you can still get pregnant.\n\nAnovulatory cycles can also create delayed or missed periods.\n\nWhat you can do\n\nMissed cycles every so often usually aren\u2019t cause for concern. If you\u2019ve missed a few consecutive cycles, you may want to take a pregnancy test to determine whether your symptoms are tied to perimenopause.\n\nOther early symptoms of pregnancy include:\n\n\nnausea\nbreast tenderness\nfrequent urination\nsensitivity to smells\nheartburn\n\nYou can also make an appointment with your doctor instead of taking a home test. Your doctor can run tests to determine whether you\u2019re experiencing symptoms of perimenopause, menopause, or pregnancy.\n\nIf you aren\u2019t pregnant and don\u2019t want to conceive, use birth control every time you have sex. Fertility doesn\u2019t end until you\u2019ve completely reached menopause. \n\nUse condoms and other barrier methods to prevent sexually transmitted infections (STIs).\n\n\n\n\nProducts to try\n\nA missed period may actually be a sign of pregnancy, which can be confirmed with an at-home test. Shop for tests and condoms online:\n\npregnancy test\ncondoms\n\n\n\n\n\n\n\n7. Overall irregularity\n\nBetween long cycles, short cycles, spotting, and heavy bleeding, your cycles during perimenopause may be generally irregular. They may not settle into any discernible pattern, especially as you get closer to menopause. This can be unsettling and frustrating.\n\nWhat you can do\n\nTry your best to remember that the changes you\u2019re experiencing are part of a bigger transition. Just as it began, the process will eventually end when you stop ovulating and reach menopause.\n\nIn the meantime:\n\n\nConsider wearing black underwear or investing in period underwear to reduce your risk of stained clothing.\nConsider wearing disposable or reusable panty liners to protect from irregular leaks, spotting, and otherwise unexpected bleeding.\nTrack your periods as best you can via a calendar or an app.\nTake notes about abnormal bleeding, pain, discomfort, or other symptoms you\u2019re experiencing.\n\n\n\n\nProducts to try\n\nIf you\u2019re having irregular periods, certain products can help you avoid leaks and stains and keep track of your symptoms. Shop for them online: \n\nperiod underwear\npanty liners\nreusable panty liners\nperiod journal\n\n\n\n\n\n\nWhen to see your doctor\n\nIn some cases, irregular bleeding may be a sign of another underlying condition.\n\nSee your doctor if you\u2019re also experiencing these symptoms:\n\n\nextremely heavy bleeding that requires you to change your pad or tampon every hour or two\nbleeding that lasts longer than 7 days\nbleeding \u2014 not spotting \u2014 that happens more frequently than every 3 weeks\n\nAt your appointment, your doctor will ask about your medical history and about any symptoms you\u2019ve had. From there, they may give you a pelvic exam and order tests (such as a blood test, a biopsy, or an ultrasound) to rule out more serious issues.\n\n\n\n\nHow we reviewed this article:\n\n\nSources\n\nHistory\n\n\n\nHealthline has strict sourcing guidelines and relies on peer-reviewed studies, academic research institutions, and medical journals and associations. We only use quality, credible sources to ensure content accuracy and integrity. You can learn more about how we ensure our content is accurate and current by reading our editorial policy.\n\nLaughlin-Tommaso SK. (2018). Bleeding after menopause: Is it normal?\nhttps://www.mayoclinic.org/diseases-conditions/menopause/expert-answers/bleeding-after-menopause/faq-20058396\nMayo Clinic Staff. (2019). Perimenopause.\nhttps://www.mayoclinic.org/diseases-conditions/perimenopause/symptoms-causes/syc-20354666\nMayo Clinic Staff. (2019). Symptoms of pregnancy: What happens first.\nhttps://www.mayoclinic.org/healthy-lifestyle/getting-pregnant/in-depth/symptoms-of-pregnancy/art-20043853\nMenopause, perimenopause, and postmenopause. (2019).\nhttps://my.clevelandclinic.org/health/diseases/15224-menopause-perimenopause-and-postmenopause\nThe menopause years. (2018).\nhttps://www.acog.org/patient-resources/faqs/womens-health/the-menopause-years\nPerimenopausal bleeding and bleeding after menopause. (2018).\nhttps://www.acog.org/patient-resources/faqs/gynecologic-problems/perimenopausal-bleeding-and-bleeding-after-menopause\nVan Voorhis BJ, et al. (2008). The relationship of bleeding patterns to daily reproductive hormones in women approaching menopause.\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666050/\nWhat are the treatment options for heavy periods? (2017).\nhttps://www.ncbi.nlm.nih.gov/books/NBK279293/\nWhat happens during the typical 28-day menstrual cycle? (2018).\nhttps://www.womenshealth.gov/menstrual-cycle/your-menstrual-cycle\nWhat to expect in perimenopause. (2011).\nhttps://www.ourbodiesourselves.org/book-excerpts/health-article/what-to-expect-perimenopause/\n\n\nShare this article\n\n\n\n\n\n\n\n\n\n\n\n\nMedically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014  Updated on March 27, 2023\n\n\n\n\n\nrelated stories\n\n\nCan Perimenopause Cause Your Periods to Be Closer Together?\n\nPerimenopause and Discharge: What to Expect\n\nNatural Remedies for Perimenopause\n\nThe Perimenopause Diet: Must-Knows\n\nMenopause: 11 Things Every Woman Should Know\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWas this article helpful?\nYesNo\n\n\n\n\n\n\nRead this next\n\n\n\n\n\n\n\n\nCan Perimenopause Cause Your Periods to Be Closer Together?\nPerimenopause can cause a lot of menstrual changes before your period ultimately stops. But can it cause your periods to be closer together?READ MORE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerimenopause and Discharge: What to Expect\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nPerimenopause is the transitional period leading to menopause. During this stage, fluctuating estrogen levels can affect your cycle from one month to\u2026READ MORE\n\n\n\n\n\n\n\nNatural Remedies for Perimenopause\n\n\nMedically reviewed by Meredith Goodwin, MD, FAAFP\nDiscover vitamins, supplements, and other natural remedies for perimenopause.READ MORE\n\n\n\n\n\n\n\n\n\n\nThe Perimenopause Diet: Must-Knows\n\n\nMedically reviewed by Debra Sullivan, Ph.D., MSN, R.N., CNE, COI\nYou can\u2019t prevent menopause from happening, but you can decide how you take care of your body during this time. Learn about mindful living during\u2026READ MORE\n\n\n\n\n\n\n\nMenopause: 11 Things Every Woman Should Know\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nTo help you get a handle on menopause, here are 11 things you should know about this transitional stage of life.READ MORE\n\n\n\n\n\n\n\nWhat\u2019s the Connection between Estrogen and Heart Disease?\nEstrogen has protective factors against heart disease. When estrogen levels lower during menopause, the risk of heart disease increases. READ MORE\n\n\n\n\n\n\n\nUnderstanding the Connection Between Early Menopause and Heart Disease Risk\nExperiencing menopause earlier in life (before the age of 45) has been linked to increased risk factors for heart disease. Here's why. READ MORE\n\n\n\n\n\n\n\nUnderstanding Heart Attack Symptoms During Menopause\nChest pain is still the most common heart attack symptom during menopause, but there are other symptoms to be aware of as well. READ MORE\n\n\n\n\n\n\n\nUnderstanding the Connection Between Menopause and Heart Rate Changes\nA lesser known symptom of menopause can be heart rate changes. Here's what that may look like. READ MORE\n\n\n\n\n\n\n\nWhy Does Menopause Increase Your Risk of Heart Disease?\nThis article discusses the connection between menopause and heart disease, who\u2019s at risk, other changes you might experience during menopause, and\u2026READ MORE\n\n\n\n\n\n\n\n\n\n\n\nAbout Us\nContact Us\nTerms of Use\nPrivacy Policy\n\nPrivacy Settings\nAdvertising Policy\n\nHealth Topics\nSitemap\nMedical Affairs\nContent Integrity\nNewsletters\nYour Privacy Choices\n\n\u00a9 2025 Healthline Media LLC. All rights reserved. Healthline Media is an RVO Health Company. Our website services, content, and products are for informational purposes only. Healthline Media does not provide medical advice, diagnosis, or treatment. See additional information.\n\u00a9 2025 Healthline Media LLC. All rights reserved. Healthline Media is an RVO Health Company. Our website services, content, and products are for informational purposes only. Healthline Media does not provide medical advice, diagnosis, or treatment. See additional information.\n\n\n\nAboutCareersAdvertise with us\n\nOUR BRANDSHealthlineMedical News TodayGreatistPsych CentralBezzy\n\n\n\n\n   \n", "title": "Perimenopause Periods: Spotting, Heavy Bleeds, and More", "description": "During perimenopause, <strong>your levels of the hormones estrogen and progesterone fluctuate</strong>. These changes can affect your cycle, leading to irregular or missed periods, and more.", "usage": {"tokens": 3812}, "retrieved_text": ["to irregular or missed periods, and more. Understanding perimenopause Menopause refers to the end of your menstrual cycle. Once you\u2019ve gone 12 months without a period, you\u2019ve reached menopause. The average woman goes through menopause at 51 years old. The time period before menopause is called perimenopause. Perimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. These shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns. Other symptoms of perimenopause include:\n\n\nhot flashes\nnight sweats\nsleep troubles\nmemory issues\ndifficulty urinating\nvaginal dryness\nchanges in sexual desire or satisfaction\n\nHere\u2019s what you can expect from perimenopause and what you can do. 1. Spotting between periods\n\nIf you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting. Spotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining.", "at 51 years old. The time period before menopause is called perimenopause. Perimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. These shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns. Other symptoms of perimenopause include: hot flashes night sweats sleep troubles memory issues difficulty urinating vaginal dryness changes in sexual desire or satisfaction Here\u2019s what you can expect from perimenopause and what you can do. 1. Spotting between periods If you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting. Spotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining. Many women spot before their period starts or as it ends. Mid-cycle spotting around ovulation is also common. If you\u2019re regularly spotting every 2 weeks, it may be a sign of a hormonal imbalance.", "https://www.acog.org/patient-resources/faqs/gynecologic-problems/perimenopausal-bleeding-and-bleeding-after-menopause Van Voorhis BJ, et al. (2008). The relationship of bleeding patterns to daily reproductive hormones in women approaching menopause. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666050/ What are the treatment options for heavy periods? (2017). https://www.ncbi.nlm.nih.gov/books/NBK279293/ What happens during the typical 28-day menstrual cycle? (2018). https://www.womenshealth.gov/menstrual-cycle/your-menstrual-cycle What to expect in perimenopause. (2011). https://www.ourbodiesourselves.org/book-excerpts/health-article/what-to-expect-perimenopause/ Share this article Medically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014 Updated on March 27, 2023 related stories Can Perimenopause Cause Your Periods to Be Closer Together? Perimenopause and Discharge: What to Expect Natural Remedies for Perimenopause The Perimenopause Diet: Must-Knows Menopause: 11 Things Every Woman Should Know Was this article helpful? YesNo Read this next Can Perimenopause Cause Your Periods to Be Closer Together? Perimenopause can cause a lot of menstrual changes before your period ultimately stops. But can it cause your periods to be closer together?READ MORE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerimenopause and Discharge: What to Expect\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nPerimenopause is the transitional period leading to menopause."]}, {"text": "    \n  \n \n\nNews \n \n  \n\n\n\n\n\nIrregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? \nIf you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. \nThis stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. \nPerimenopause ends with menopause, at which point you have not had a period for 12 months. \nWhat Does Perimenopause Mean? \nThe word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. \nWhen Can You Expect Perimenopause? \nChances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause. The average age a woman starts the menopausal transition is 47. \nWhile we cannot predict when this will occur for you, the age your mother began to experience symptoms may be a good indicator. Smokers tend to reach menopause earlier than non smokers as well. \nShould You Get Tested for Perimenopause? \nThe short answer: No. \nThe blood tests that measure your ovarian reserve are rarely accurate during perimenopause. FSH and estrogen change by the day and throughout the day so they are generally not helpful. \nWe do consider testing these hormones if you experience perimenopausal symptoms under the age of 45. We generally will also check other pituitary hormones, like TSH and prolactin, if you are experiencing these symptoms prematurely. \nKeeping a menstrual diary is generally the best \u201ctest\u201d you can do. This will give you and your OBGYN insight into what your body is doing and for how long. \nAny time you experience abnormal uterine bleeding (i.e., changes in the amount of blood, frequency of bleeding, and length of bleeding), checking in with your doctor is a good idea to make sure it is normal and that no other work-up is needed. \nStages of Perimenopause \nWomen typically go through the menopausal transition in two phases: early and late. \nSigns of Early Perimenopause \n \nMore frequent menstrual cycles \nUnpredictable menstrual cycles \nHeavier or lighter bleeding  \nSigns of Late Perimenopause \n \nHot flashes or mood changes \nLess frequent menstrual periods \nHeavier or lighter bleeding  \nWhat to Expect During Perimenopause \nJust as every woman\u2019s period is different, the way you experience perimenopause will be different. The intensity of symptoms varies from one woman to the next, but they follow a typical progression until you have reached menopause and do not have a period for 12 months. \nIrregular Periods \nThis is the hallmark of perimenopause. The frustrating part is the unpredictability. Periods may change gradually from one cycle to the next, or you may notice more abrupt changes. \nHot Flashes & Night Sweats \nHot flashes and night sweats are common symptoms of perimenopause and menopause, with over 85% of women reporting hot flashes. Hormone changes affect your body\u2019s \u201cinternal thermostat.\u201d A hot flash feels like a wave or sensation of heat across your face, neck, and chest. It can last for several minutes. Hot flashes can happen a few times a day, a few times a week, or less often. \nHot flashes that happen at night are called night sweats, which can cause women to wake up drenched in sweat and disturb sleep.  Women are more likely to report hot flashes at night. \nTrouble Sleeping \nWhen hot flashes wake you up, it can be hard to fall back to sleep. You may feel like you are in \u201cfight or flight\u201d mode.  In addition, mood trouble can often make sleep tricky. \nIrritability & Depression \nPerimenopause increases the risk of depression by about 30%. The rise in depression may come from the changes and lack of control over what\u2019s happening to your body, sleeplessness, and other life events that typically occur at this point as we age. There is good news! Studies have shown that the risk of depression decreases after menopause. \nTreating Perimenopausal Symptoms \nPerimenopause itself does not need to be treated. It is a natural, normal transition as you move to the next stage of your life. \nTreating perimenopausal symptoms, however, is definitely possible. A discussion with your OBGYN will help you determine the best path for you. \n \nBirth control pills that contain low doses of estrogen can be used to help control the uncomfortable symptoms of perimenopause by regulating your body\u2019s fluctuating hormones. \nAntidepressants can be prescribed to help with depression. They can also help treat hot flashes. \nSupplements, such as multi-vitamins, support your overall health. Herbal and hormonal therapies, however, are not recommended. Studies consistently show they are as equally effective as a placebo at managing symptoms and some are associated with risks. \nMindfulness and meditation are tools that you can learn to use to manage irritability, depression, and anxiety. \nCounseling can help you learn how to cope with the changes your body and life are experiencing. \nExercise (strength training, cardiovascular routines, yoga) is beneficial throughout perimenopause as it can help you maintain a healthy weight, improve your mood, strengthen your bones, and reduce your risk of cardiovascular and other diseases.  \nFrequently Asked Questions about Perimenopause \nQ. Can I still get pregnant during perimenopause? \n \nThe chance that you could become pregnant in your mid- to late-40s is very low. However, as long as you are menstruating, it is certainly possible that you could get pregnant. We recommend contraception into your 40s if you do not want to become pregnant.  \nQ. When should I stop taking my birth control? \n \nBirth control can mask the transition to menopause, which is why it is used for treating symptoms such as hot flashes and irregular bleeding. Birth control also makes lab tests for perimenopause unreliable. We recommend stopping birth control around age 50-51 and then monitoring your symptoms.  \nQ. What can I do about perimenopausal weight gain? \n \nAs you age, your metabolism naturally slows down. You may also lose muscle mass and gain unwanted fat. It\u2019s important to maintain a healthy diet and shoot for 30 minutes of exercise each day. Carrying excess weight increases your risks of heart disease, Type 2 diabetes, and even various types of cancers. Talk to your doctor about ways you can obtain (and maintain) a healthy body weight.  \nQ. When should I call a doctor about my perimenopausal symptoms? \n \nIf you are experiencing hot flashes and night sweats under the age of 45, contact your OBGYN to see what else might be causing them. When you have abnormal uterine bleeding, it is important to alert us regardless of age as we may recommend an ultrasound or endometrial biopsy to rule out abnormal changes in the uterus.  \nIf you have not had a period for 12 months and then experience vaginal bleeding, contact your doctor. It is not normal for bleeding to recur after this period of time. Read our article about when you should see your OBGYN. \n\u2014 \nRemember, perimenopause and menopause are natural and normal transitions, but they can be stressful. Many symptoms can be managed which can help you regain a sense of control, well-being, and confidence to thrive in your next stage of life. \nWe want you to feel supported, heard, and cared for as you go through this change. \nSometimes, the biggest help is simply confirmation that what you\u2019re experiencing is normal! \nDr. Ashley Durward has been providing healthcare to women in Madison since 2015 and joined Madison Women\u2019s Health in 2019, specializing in high and low risk obstetrics, contraception and preconception counseling, management of abnormal uterine bleeding, pelvic floor disorders, and minimally invasive gynecologic surgery. September 28, 2020/by Dr. Ashley Durward\nTags: abnormal uterine bleeding, hot flashes, irregular periods, menopause, perimenopause\n\n\nShare this entry\n\nShare on Facebook\nShare on X\nShare on Pinterest\nShare on LinkedIn\nShare on Reddit\nShare by Mail\n\n  https://madisonwomenshealth.com/wp-content/uploads/2020/09/perimenopause-irregular-bleeding-in-40s.jpg 799 1200   Dr. Ashley Durward  https://madisonwomenshealth.com/wp-content/uploads/2017/09/mwh_logo-300x177.png  Dr. Ashley Durward2020-09-28 09:11:392023-05-31 12:17:11What is Perimenopause?\n\n\nYou might also like\n\nHormone Imbalance: What are the Symptoms and Treatments?\nMenopausal Symptoms: How to Treat The Ones That Bother You \n    \n\n\nFind Answers to Your Questions  \n \n        \nRecent Posts \n \n Constipation During Pregnancy: Tips for Finding Relief and Peace of Mind  \n The Truth About Infections During Pregnancy: What Every Mom-to-Be Needs to Know  \n Frequently Asked Questions About Changing Health Insurance in 2024  \n Understanding and Overcoming Yeast Infections During Pregnancy  \n Why STI Testing Should Be Part of Your Self-Care Routine   \nBrowse by Category\nBrowse by Category Select Category Baby Story Billing News Blog Clinic News Family Family Planning Fertility Health News Healthcare 101 Kids Labor & Delivery Pregnancy Uncategorized Women\u2019s Health   \nTags\nbaby weight birth control birth control pills birth control shots breast cancer breastfeeding C-section clogged duct Clomid colostrum conceiving condoms Depo Provera diaphragms doula epidural exercising during pregnancy fertility tips genetic counselling genetic screening gestational diabetes gestational hypertension giving birth Glow App gynecologist HPV screening labor tips Letrozole mammograms mastitis menopause mini-pill miscarriage NuvaRing OBGYN perimenopause Polycystic Ovarian Syndrome (PCOS) pregnancy pregnancy tips pregnancy weight gain pregnancy weight loss prenatal genetic testing STI ultrasound yeast infections    \n\n\n\n\n \n\n \n\n\n\n\n\n\n5801 Research Park Blvd, Suite 400, Madison WI 53719 \n608-729-6300   \n \n      Reviews \n  \n \n\n\n  \n  \n\n\n    \n \n \u00a9 2025 All rights reserved. | WordPress Web Design by Bizzy Bizzy      Food During Pregnancy: Cravings, Aversions, Foods to Eat and What to AvoidHormone Imbalance: What are the Symptoms and Treatments? Scroll to top \n \n\n\n\nThis site uses cookies. By continuing to browse the site, you are agreeing to our use of cookies.OKLearn more\u00d7\n  \n\nCookie and Privacy Settings\n\n\n  \n\n\n\nHow we use cookies\n\n\nWe may request cookies to be set on your device. We use cookies to let us know when you visit our websites, how you interact with us, to enrich your user experience, and to customize your relationship with our website.  \nClick on the different category headings to find out more. You can also change some of your preferences. Note that blocking some types of cookies may impact your experience on our websites and the services we are able to offer. \nEssential Website Cookies\n\n\nThese cookies are strictly necessary to provide you with services available through our website and to use some of its features. \nBecause these cookies are strictly necessary to deliver the website, refusing them will have impact how our site functions. You always can block or delete cookies by changing your browser settings and force blocking all cookies on this website. But this will always prompt you to accept/refuse cookies when revisiting our site. \nWe fully respect if you want to refuse cookies but to avoid asking you again and again kindly allow us to store a cookie for that. You are free to opt out any time or opt in for other cookies to get a better experience. If you refuse cookies we will remove all set cookies in our domain. \nWe provide you with a list of stored cookies on your computer in our domain so you can check what we stored. Due to security reasons we are not able to show or modify cookies from other domains. You can check these in your browser security settings. \nCheck to enable permanent hiding of message bar and refuse all cookies if you do not opt in. We need 2 cookies to store this setting. Otherwise you will be prompted again when opening a new browser window or new a tab. \nClick to enable/disable essential site cookies. \nGoogle Analytics Cookies\n\n\nThese cookies collect information that is used either in aggregate form to help us understand how our website is being used or how effective our marketing campaigns are, or to help us customize our website and application for you in order to enhance your experience. \nIf you do not want that we track your visit to our site you can disable tracking in your browser here: \nClick to enable/disable Google Analytics tracking. \nOther external services\n\n\nWe also use different external services like Google Webfonts, Google Maps, and external Video providers. Since these providers may collect personal data like your IP address we allow you to block them here. Please be aware that this might heavily reduce the functionality and appearance of our site. Changes will take effect once you reload the page. \nGoogle Webfont Settings: \nClick to enable/disable Google Webfonts. \nGoogle Map Settings: \nClick to enable/disable Google Maps. \nGoogle reCaptcha Settings: \nClick to enable/disable Google reCaptcha. \nVimeo and Youtube video embeds: \nClick to enable/disable video embeds. \nOther cookies\n\n\nThe following cookies are also needed - You can choose if you want to allow them: \nClick to enable/disable _ga - Google Analytics Cookie. \nClick to enable/disable _gid - Google Analytics Cookie. \nClick to enable/disable _gat_* - Google Analytics Cookie. \nAccept settingsHide notification only         ", "title": "Irregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else?", "description": "It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. <strong>Your estrogen levels drop and you may have markedly irregular menstrual cycles</strong>. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood ...", "usage": {"tokens": 3017}, "retrieved_text": ["    \n  \n \n\nNews \n \n  \n\n\n\n\n\nIrregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? If you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. This stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. Perimenopause ends with menopause, at which point you have not had a period for 12 months. What Does Perimenopause Mean? The word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. When Can You Expect Perimenopause? Chances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause.", "News Irregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? If you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. This stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. Perimenopause ends with menopause, at which point you have not had a period for 12 months. What Does Perimenopause Mean? The word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. When Can You Expect Perimenopause? Chances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause. The average age a woman starts the menopausal transition is 47.", "We generally will also check other pituitary hormones, like TSH and prolactin, if you are experiencing these symptoms prematurely. Keeping a menstrual diary is generally the best \u201ctest\u201d you can do. This will give you and your OBGYN insight into what your body is doing and for how long. Any time you experience abnormal uterine bleeding (i.e., changes in the amount of blood, frequency of bleeding, and length of bleeding), checking in with your doctor is a good idea to make sure it is normal and that no other work-up is needed. Stages of Perimenopause \nWomen typically go through the menopausal transition in two phases: early and late. Signs of Early Perimenopause \n \nMore frequent menstrual cycles \nUnpredictable menstrual cycles \nHeavier or lighter bleeding  \nSigns of Late Perimenopause \n \nHot flashes or mood changes \nLess frequent menstrual periods \nHeavier or lighter bleeding  \nWhat to Expect During Perimenopause \nJust as every woman\u2019s period is different, the way you experience perimenopause will be different. The intensity of symptoms varies from one woman to the next, but they follow a typical progression until you have reached menopause and do not have a period for 12 months. Irregular Periods \nThis is the hallmark of perimenopause."]}, {"text": "  \n   \n \n \nRecent Articles \n \n  \n   \n \nLet's not call it cancer    \n   \n \nChair exercises for seniors: Boosting strength, flexibility, and stamina    \n   \n \nWhy all the buzz about inflammation \u2014 and just how bad is it?    \n   \n \nChronic kidney disease: What to know about this common, serious condition    \n   \n \nBeyond the usual suspects for healthy resolutions    \n   \n \nCOVID tests: Do at-home tests work on newer variants?    \n   \n \nA liquid biopsy for metastatic prostate cancer    \n   \n \nOne way to combat loneliness? Strengthen relationships you already have    \n   \n \nFeel like you should be drinking less? Start here    \n   \n \nGot a pinched nerve? Strategies and treatments for pain relief    \n  /      \n \n \n \n  \n \n  August 9, 2022    \n \n   \nWhat are the signs of perimenopause? You're in your 40s, you wake up in a sweat at night, and your periods are erratic and often accompanied by heavy bleeding: Chances are, you're going through perimenopause. Many women experience an array of symptoms as their hormones shift during the months or years leading up to menopause \u2014 that is, the natural end of menstruation. Menopause is a point in time, but perimenopause (peri, Greek for \"around\" or \"near\" + menopause) is an extended transitional state. It's also sometimes referred to as the menopausal transition, although technically, the transition ends 12 months earlier than perimenopause (see \"Stages of reproductive aging\" below). \nWhat is perimenopause? \nPerimenopause has been variously defined, but experts generally agree that it begins with irregular menstrual cycles \u2014 courtesy of declining ovarian function \u2014 and ends a year after the last menstrual period. \nPerimenopause varies greatly from one woman to the next. The average duration is three to four years, although it can last just a few months or extend as long as a decade. Some women feel buffeted by hot flashes and wiped out by heavy periods; many have no bothersome symptoms. Periods may end more or less abruptly for some, while others may menstruate erratically for years. Fortunately, as knowledge about reproductive aging has grown, so have the options for treating some of its more distressing features. \nPerimenopause and estrogen \nThe physical changes of perimenopause are rooted in hormonal alterations, particularly variations in the level of circulating estrogen. \nDuring our peak reproductive years, the amount of estrogen in circulation rises and falls fairly predictably throughout the menstrual cycle. Estrogen levels are largely controlled by two hormones, follicle-stimulating hormone (FSH) and luteinizing hormone (LH). FSH stimulates the follicles \u2014 the fluid-filled sacs in the ovaries that contain the eggs \u2014 to produce estrogen. When estrogen reaches a certain level, the brain signals the pituitary to turn off the FSH and produce a surge of LH. This in turn stimulates the ovary to release the egg from its follicle (ovulation). The leftover follicle produces progesterone, in addition to estrogen, in preparation for pregnancy. As these hormone levels rise, the levels of FSH and LH drop. If pregnancy doesn't occur, progesterone falls, menstruation takes place, and the cycle begins again. \nHot flashes during perimenopause \nMost women don't expect to have hot flashes until menopause, so it can be a big surprise when they show up earlier, during perimenopause. Hot flashes \u2014 sometimes called hot flushes and given the scientific name of vasomotor symptoms \u2014 are the most commonly reported symptom of perimenopause. They're also a regular feature of sudden menopause due to surgery or treatment with certain medications, such as chemotherapy drugs. \nHot flashes tend to come on rapidly and can last from one to five minutes. They range in severity from a fleeting sense of warmth to a feeling of being consumed by fire \"from the inside out.\" A major hot flash can induce facial and upper-body flushing, sweating, chills, and sometimes confusion. Having one of these at an inconvenient time (such as during a speech, job interview, or romantic interlude) can be quite disconcerting. Hot flash frequency varies widely. Some women have a few over the course of a week; others may experience 10 or more in the daytime, plus some at night. \nMost American women have hot flashes around the time of menopause, but studies of other cultures suggest this experience is not universal. Far fewer Japanese, Korean, and Southeast Asian women report having hot flashes. In Mexico's Yucatan peninsula, women appear not to have any at all. These differences may reflect cultural variations in perceptions, semantics, and lifestyle factors, such as diet. \nAlthough the physiology of hot flashes has been studied for more than 30 years, no one is certain why or how they occur. Estrogen is involved \u2014 if it weren't, estrogen therapy wouldn't relieve vasomotor symptoms as well as it does \u2014 but it's not the whole story. For example, researchers have found no differences in estrogen levels in women who have hot flash symptoms and those who don't. A better understanding of the causes of hot flashes in perimenopause could open the way to new, nonhormonal treatments. Hormone therapy quells hot flashes, but it's not risk-free. \nBy our late 30s, we don't produce as much progesterone. The number and quality of follicles also diminishes, causing a decline in estrogen production and fewer ovulations. As a result, by our 40s, cycle length and menstrual flow may vary and periods may become irregular. Estrogen may drop precipitously or spike higher than normal. Over time, FSH levels rise in a vain attempt to prod the ovaries into producing more estrogen. \nAlthough a high FSH can be a sign that perimenopause has begun, a single FSH reading isn't a reliable indicator because day-to-day hormone levels can fluctuate dramatically. \nPerimenopause symptoms \nIt can be difficult to distinguish the hormonally based symptoms of perimenopause from more general changes due to aging or common midlife events \u2014 such as children leaving home, changes in relationships or careers, or the death or illness of parents. Given the range of women's experience of perimenopause, it's unlikely that symptoms depend on hormonal fluctuations alone. \n \nHot flashes and night sweats. An estimated 35%\u201350% of perimenopausal women suffer sudden waves of body heat with sweating and flushing that last 5\u201310 minutes, often at night as well as during the day. They typically begin in the scalp, face, neck, or chest and can differ dramatically among women who have them; some women feel only slightly warm, while others end up wringing wet. Hot flashes often continue for a year or two after menopause. In up to 10% of women, they persist for years beyond that. \nVaginal dryness. During late perimenopause, falling estrogen levels can cause vaginal tissue to become thinner and drier. Vaginal dryness (which usually becomes even worse after menopause) can cause itching and irritation. It may also be a source of pain during intercourse, contributing to a decline in sexual desire at midlife. \nUterine bleeding problems. With less progesterone to regulate the growth of the endometrium, the uterine lining may become thicker before it's shed, resulting in very heavy periods. Also, fibroids (benign tumors of the uterine wall) and endometriosis (the migration of endometrial tissue to other pelvic structures), both of which are fueled by estrogen, may become more troublesome. \nSleep disturbances. About 40% of perimenopausal women have sleep problems. Some studies have shown a relationship between night sweats and disrupted sleep; others have not. The problem is too complex to blame on hormone oscillations alone. Sleep cycles change as we age, and insomnia is a common age-related complaint in both sexes. \nMood symptoms. Estimates put the number of women who experience mood symptoms during perimenopause at 10%\u201320%. Some studies have linked estrogen to depression during the menopausal transition, but there's no proof that depression in women at midlife reflects declining hormone levels. In fact, women actually have a lower rate of depression after age 45 than before. Menopause-related hormone changes are also unlikely to make women anxious or chronically irritable, although the unpredictability of perimenopause can be stressful and provoke some episodes of irritability. Also, some women may be more vulnerable than others to hormone-related mood changes. The best predictors of mood symptoms at midlife are life stress, poor overall health, and a history of depression. \nOther problems. Many women complain of short-term memory problems and difficulty concentrating during the menopausal transition. Although estrogen and progesterone are players in maintaining brain function, there's too little information to separate the effects of aging and psychosocial factors from those related to hormone changes.  \nWhat to do about perimenopause symptoms \nSeveral treatments have been studied for managing perimenopausal symptoms. Complementary therapies are also available, but research on them is limited and the results are inconsistent. \n \nVasomotor symptoms. The first rule is to avoid possible triggers of hot flashes, which include warm air temperatures, hot beverages, and spicy foods. You know your triggers best. Dress in layers so you can take off clothes as needed. There's clear evidence that paced respiration, a deep breathing technique, helps alleviate hot flashes. The most effective treatment for severe hot flashes and night sweats is estrogen. Unless you've had a hysterectomy, you'll likely need to take a progestin to reduce the risk of developing endometrial cancer. Low-dose estrogen by pill or patch \u2014 for example, doses that are less than or equal to 0.3 milligrams (mg) conjugated equine estrogen, 0.5 mg oral micronized estradiol, 25 micrograms (mcg) transdermal (patch) estradiol, or 2.5 mcg ethinyl estradiol \u2014 works for many women. \nHormonal irregularities. If you need contraception and don't smoke, you can take low-dose birth control pills until menopause (see \"Irregular periods and heavy bleeding\"). Another advantage of these pills is that they regulate your menses and suppress the erratic hormonal ups and downs of perimenopause; some women report feeling more even-tempered while taking them. \nSevere hot flashes. Women with severe hot flashes who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. \nIrregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist.\n Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. \nVaginal dryness. Low-dose contraceptives or vaginal estrogen (in a cream, ring, tablet, or gel) can help relieve vaginal dryness, but hormonal treatment is not the only approach. Vaginal moisturizers such as Replens, applied twice weekly, increase vaginal moisture, elasticity, and acidity. Continued sexual activity also seems to improve vaginal tone and helps maintain the acidic environment that protects it against infections. Lubricants such as K-Y Jelly, Astroglide, and K-Y Silk-E can make intercourse less painful.  \n(This article was first printed in a previous issue of the Harvard Women's Health Watch. For more information or to order, please go to /womens.) \n  Image: Highwaystarz-Photography/Getty Images    \n \n Share This Page  Share this page to Facebook    Share this page to Twitter    Share this page via Email    \n Print This Page \n Click to Print     \nDisclaimer: \n \n As a service to our readers, Harvard Health Publishing provides access to our library of archived content. Please note the date of last review or update on all articles.  \n No content on this site, regardless of date, should ever be used as a substitute for direct medical advice from your doctor or other qualified clinician.       \n \n \nRecent Articles \n \n  \n   \n \nLet's not call it cancer    \n   \n \nChair exercises for seniors: Boosting strength, flexibility, and stamina    \n   \n \nWhy all the buzz about inflammation \u2014 and just how bad is it?    \n   \n \nChronic kidney disease: What to know about this common, serious condition    \n   \n \nBeyond the usual suspects for healthy resolutions    \n   \n \nCOVID tests: Do at-home tests work on newer variants?    \n   \n \nA liquid biopsy for metastatic prostate cancer    \n   \n \nOne way to combat loneliness? Strengthen relationships you already have    \n   \n \nFeel like you should be drinking less? Start here    \n   \n \nGot a pinched nerve? Strategies and treatments for pain relief    \n  /      \n \n \nFree Healthbeat Signup \n \nGet the latest in health news delivered to your inbox!    Sign Up         \n \n \nFooter \n  Harvard Health Publishing   \n  Facebook    Twitter    Linkedin    YouTube     \n \n \n \nMy Account \n \n  Customer Service    \n  Log in      \n \nOrder Now \n \n  Online Learning Courses    \n  Digital Subscriptions    \n  Special Health Reports    \n  Print Subscriptions      \n \nMore \n \n  About Us    \n  Permissions    \n  Content Licensing    \n  Topics    \n  Trademark Notice        \n \n \n \u00a9 2025 Harvard Health Publishing\u00ae of The President and Fellows of Harvard College  \nDo not sell my personal information | Privacy Policy and Terms of Use    \n  Scroll To Top      \n \n \n \n  Close   \n \nThanks for visiting. Don't miss your FREE gift. \nThe Best Diets for Cognitive Fitness, is yours absolutely FREE when you sign up to receive Health Alerts from Harvard Medical School \nSign up to get tips for living a healthy lifestyle, with ways to  fight inflammation and improve cognitive health, plus the latest advances in preventative medicine, diet and exercise, pain relief, blood pressure and cholesterol management, and more.  \n \n   \n    \n I want to get healthier \n     \n \n This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.       \n \n \n  Close   \nHealth Alerts from Harvard Medical School \nGet helpful tips and guidance for everything from fighting inflammation to finding the best diets for weight loss...from exercises to build a stronger core to advice on treating cataracts. PLUS, the latest news on medical advances and breakthroughs from Harvard Medical School experts.\n \n \n \nBONUS! Sign up now and\n get a FREE copy of the\nBest Diets for Cognitive Fitness    \n \n   \n    \n I want to get healthier \n     \n \n This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.       \n \n \n  Close  \n  \n \nStay on top of latest health news from Harvard Medical School.   \nPlus, get a FREE copy of the Best Diets for Cognitive Fitness. \n \n   \n    \n Sign me up \n          \n \n \n  Close  \n \n  \n \nStay on top of latest health news from Harvard Medical School. \nPlus, get a FREE copy of the Best Diets for Cognitive Fitness.   \n \n   \n    \n Sign me up \n     \n \n This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.             ", "title": "Perimenopause: Rocky road to menopause - Harvard Health", "description": "By our late 30s, we don&#x27;t produce as much progesterone. The number and quality of follicles also diminishes, causing a decline in estrogen production and fewer ovulations. As a result, <strong>by our 40s, cycle length and menstrual flow may vary and periods may become irregular</strong>.", "usage": {"tokens": 3407}, "retrieved_text": ["equine estrogen, 0.5 mg oral micronized estradiol, 25 micrograms (mcg) transdermal (patch) estradiol, or 2.5 mcg ethinyl estradiol \u2014 works for many women. Hormonal irregularities. If you need contraception and don't smoke, you can take low-dose birth control pills until menopause (see \"Irregular periods and heavy bleeding\"). Another advantage of these pills is that they regulate your menses and suppress the erratic hormonal ups and downs of perimenopause; some women report feeling more even-tempered while taking them. Severe hot flashes. Women with severe hot flashes who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis.", "with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist. Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. Vaginal dryness. Low-dose contraceptives or vaginal estrogen (in a cream, ring, tablet, or gel) can help relieve vaginal dryness, but hormonal treatment is not the only approach. Vaginal moisturizers such as Replens, applied twice weekly, increase vaginal moisture, elasticity, and acidity. Continued sexual activity also seems to improve vaginal tone and helps maintain the acidic environment that protects it against infections. Lubricants such as K-Y Jelly, Astroglide, and K-Y Silk-E can make intercourse less painful.", "who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist. Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. Vaginal dryness."]}]}, "claim_verification_result": [{"claim": "Perimenopause can lead to irregular periods in women.", "search_results": "Evidence 1\nTitle: Perimenopause: Age, Stages, Signs, Symptoms & Treatment\nContent: makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse: Eating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause. Avoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems. Smoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being. Care at Cleveland Clinic\n\nMenopause Treatment\nFind a Doctor and Specialists\nMake an Appointment\n\n\nPrevention\n\nWhat can put you into perimenopause early? Certain factors are linked to early perimenopause. These include:\n\nSmoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t. A family history of early menopause. A history of cancer treatment.\na cooler temperature. Improve sleep hygiene by avoiding TV and computer screens and doing relaxing activities before bed. Limit alcohol and caffeine. Practice meditation or other stress management techniques. Quit using tobacco products like cigarettes. Maintain a weight that\u2019s healthy for you. What makes perimenopause worse? There isn\u2019t one specific thing that makes perimenopause worse. Everyone is different. You may find that certain activities or specific foods make your symptoms worse. In general, the following factors can make perimenopause symptoms worse: Eating unhealthy foods like sugary drinks and desserts or foods high in saturated fat. Eating more nutritious foods can help with weight changes and hot flashes from perimenopause. Avoiding exercise or staying sedentary. Getting regular exercise has been shown to improve energy, mood swings and sleep problems. Smoking cigarettes or using tobacco products. Studies show that smoking can lead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being.\nlead to early menopause and decreased bone density, as well as worsen vaginal dryness. Talk to your healthcare provider about your lifestyle and any symptoms you have. They can work with you on a treatment plan and recommend ways to improve your overall well-being. Care at Cleveland Clinic Menopause Treatment Find a Doctor and Specialists Make an Appointment Prevention What can put you into perimenopause early? Certain factors are linked to early perimenopause. These include: Smoking or using tobacco products. People who use these products regularly tend to go into menopause about one to two years earlier than people who don\u2019t. A family history of early menopause. A history of cancer treatment. If you\u2019ve had surgery to remove your uterus (hysterectomy) or ovaries (oophorectomy). Outlook / Prognosis\n\nAre there any health risks associated with perimenopause? There are health risks associated with menopause, which happens right after perimenopause. Estrogen plays an important role in preserving your bones. Osteoporosis is a condition where the insides of your bones become less dense and more fragile. This increases your risk for bone fractures. Your healthcare provider may recommend a multivitamin, calcium supplement, extra vitamin D or more weight-bearing exercises.\n\nEvidence 2\nTitle: Perimenopause - Symptoms and causes - Mayo Clinic\nContent: you wish to avoid pregnancy, use birth control until you've had no periods for 12 months. Changes in sexual function. During perimenopause, sexual arousal and desire may change. But if you had satisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. Loss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. Changing cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease. When to see a doctor\nSome women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention.\nsatisfactory sexual intimacy before menopause, this will likely continue through perimenopause and beyond. Loss of bone. With declining estrogen levels, you start to lose bone more quickly than you replace it, increasing your risk of osteoporosis \u2014 a disease that causes fragile bones. Changing cholesterol levels. Declining estrogen levels may lead to unfavorable changes in your blood cholesterol levels, including an increase in low-density lipoprotein (LDL) cholesterol \u2014 the \"bad\" cholesterol \u2014 which contributes to an increased risk of heart disease. At the same time, high-density lipoprotein (HDL) cholesterol \u2014 the \"good\" cholesterol \u2014 decreases in many women as they age, which also increases the risk of heart disease. When to see a doctor Some women seek medical attention for their perimenopausal symptoms. But others either tolerate the changes or simply don't experience symptoms severe enough to need attention. Because symptoms may be subtle and come on gradually, you may not realize at first that they're all connected to the same thing \u2014 hormone fluctuations of the menopausal transition. If you have symptoms that interfere with your life or well-being, such as hot flashes, mood swings or changes in sexual function that concern you, see your doctor.\nGynecology Clinics of North America. 2019; doi:10.1016/j.ogc.2019.04.008. American College of Obstetricians and Gynecologists. Practice Bulletin No. 128: Diagnosis of abnormal uterine bleeding in reproductive-aged women. Obstetrics and Gynecology. 2012; doi:10.1097/AOG.0b013e318262e320. Reaffirmed 2016. American College of Obstetricians and Gynecologists. Practice Bulletin No. 141: Management of menopausal symptoms. Obstetrics and Gynecology. 2014; doi:10.1097/01.AOG.0000441353.20693.78. Reaffirmed 2018. Black cohosh. National Center for Complementary and Integrative Health. https://www.nccih.nih.gov/health/black-cohosh. Accessed March 8, 2021. Basu P, et al. Phytoestrogens and breast cancer: In vitro anticancer activities of isoflavones, lignans, coumestans, stilbenes and their analogs and derivatives. Biomedicine and Pharmacotherapy. 2018; doi:10.1016/j.biopha.2018.08.100. Burnett TL (expert opinion). Mayo Clinic. May 5, 2021. Zhao T-T, et al. Dietary isoflavones or isoflavoine-rich food intake and breast cancer risk: A meta-analysis of prospective cohort studies. Clinical Nutrition. 2019; doi:10.1016/j.clnu.2017.12.006. FDA approves novel drug to treat moderate to severe hot flashes caused by menopause. U.S. Food and Drug Administration. https://www.fda.gov/news-events/press-announcements/fda-approves-novel-drug-treat-moderate-severe-hot-flashes-caused-menopause. Accessed May 16, 2023. Veozah (prescribing information). https://www.accessdata.fda.gov/drugsatfda_docs/label/2023/216578s000lbl.pdf. Accessed May 16, 2023. Related \n \nAssociated Procedures \n \n Endometrial ablation    \n \nNews from Mayo Clinic \n \n Perimenopause, menopause and ... weightlifting?\n\nEvidence 3\nTitle: Perimenopause Periods: Spotting, Heavy Bleeds, and More\nContent: to irregular or missed periods, and more. Understanding perimenopause Menopause refers to the end of your menstrual cycle. Once you\u2019ve gone 12 months without a period, you\u2019ve reached menopause. The average woman goes through menopause at 51 years old. The time period before menopause is called perimenopause. Perimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. These shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns. Other symptoms of perimenopause include:\n\n\nhot flashes\nnight sweats\nsleep troubles\nmemory issues\ndifficulty urinating\nvaginal dryness\nchanges in sexual desire or satisfaction\n\nHere\u2019s what you can expect from perimenopause and what you can do. 1. Spotting between periods\n\nIf you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting. Spotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining.\nat 51 years old. The time period before menopause is called perimenopause. Perimenopause symptoms occur for 4 years, on average. However, perimenopause can last anywhere from a few months to 10 years. During this time, the hormones estrogen and progesterone are in flux. Your levels will fluctuate from month to month. These shifts can be erratic, affecting ovulation and the rest of your cycle. You may notice anything from irregular or missed periods to different bleeding patterns. Other symptoms of perimenopause include: hot flashes night sweats sleep troubles memory issues difficulty urinating vaginal dryness changes in sexual desire or satisfaction Here\u2019s what you can expect from perimenopause and what you can do. 1. Spotting between periods If you notice some blood on your underwear between periods that doesn\u2019t require the use of a pad or tampon, it\u2019s likely spotting. Spotting is usually the result of your body\u2019s changing hormones and the buildup of your endometrium, or uterine lining. Many women spot before their period starts or as it ends. Mid-cycle spotting around ovulation is also common. If you\u2019re regularly spotting every 2 weeks, it may be a sign of a hormonal imbalance.\nhttps://www.acog.org/patient-resources/faqs/gynecologic-problems/perimenopausal-bleeding-and-bleeding-after-menopause Van Voorhis BJ, et al. (2008). The relationship of bleeding patterns to daily reproductive hormones in women approaching menopause. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666050/ What are the treatment options for heavy periods? (2017). https://www.ncbi.nlm.nih.gov/books/NBK279293/ What happens during the typical 28-day menstrual cycle? (2018). https://www.womenshealth.gov/menstrual-cycle/your-menstrual-cycle What to expect in perimenopause. (2011). https://www.ourbodiesourselves.org/book-excerpts/health-article/what-to-expect-perimenopause/ Share this article Medically reviewed by Meredith Goodwin, MD, FAAFP \u2014 Written by Ashley Marcin \u2014 Updated on March 27, 2023 related stories Can Perimenopause Cause Your Periods to Be Closer Together? Perimenopause and Discharge: What to Expect Natural Remedies for Perimenopause The Perimenopause Diet: Must-Knows Menopause: 11 Things Every Woman Should Know Was this article helpful? YesNo Read this next Can Perimenopause Cause Your Periods to Be Closer Together? Perimenopause can cause a lot of menstrual changes before your period ultimately stops. But can it cause your periods to be closer together?READ MORE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerimenopause and Discharge: What to Expect\n\n\nMedically reviewed by Valinda Riggins Nwadike, MD, MPH\nPerimenopause is the transitional period leading to menopause.\n\nEvidence 4\nTitle: Irregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else?\nContent: News \n \n  \n\n\n\n\n\nIrregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? If you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. This stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. Perimenopause ends with menopause, at which point you have not had a period for 12 months. What Does Perimenopause Mean? The word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. When Can You Expect Perimenopause? Chances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause.\nNews Irregular Periods in Your 40s \u2014 Is It Perimenopause or Something Else? If you\u2019re in your mid- to late 40s and your periods are becoming irregular, you may be in the menopausal transition, or \u201cperimenopause.\u201d This is the natural stage your body goes through as you approach menopause. This stage lasts about four years on average, although some women may experience only a few months or many more years of symptoms. It is characterized by fluctuations in hormones as your ovaries are nearly out of eggs. Your estrogen levels drop and you may have markedly irregular menstrual cycles. On top of irregular periods, hormonal changes can lead to weight gain, hot flashes, trouble sleeping, vaginal dryness, mood changes, and depression. Perimenopause ends with menopause, at which point you have not had a period for 12 months. What Does Perimenopause Mean? The word perimenopause, also called the menopausal transition, means \u201caround menopause.\u201d This refers to the period of time when your body starts to make the natural transition to menopause, which determines the end of the reproductive years. When Can You Expect Perimenopause? Chances are, if you\u2019re between the ages of 45-50, your irregular periods are a sign of perimenopause. The average age a woman starts the menopausal transition is 47.\nWe generally will also check other pituitary hormones, like TSH and prolactin, if you are experiencing these symptoms prematurely. Keeping a menstrual diary is generally the best \u201ctest\u201d you can do. This will give you and your OBGYN insight into what your body is doing and for how long. Any time you experience abnormal uterine bleeding (i.e., changes in the amount of blood, frequency of bleeding, and length of bleeding), checking in with your doctor is a good idea to make sure it is normal and that no other work-up is needed. Stages of Perimenopause \nWomen typically go through the menopausal transition in two phases: early and late. Signs of Early Perimenopause \n \nMore frequent menstrual cycles \nUnpredictable menstrual cycles \nHeavier or lighter bleeding  \nSigns of Late Perimenopause \n \nHot flashes or mood changes \nLess frequent menstrual periods \nHeavier or lighter bleeding  \nWhat to Expect During Perimenopause \nJust as every woman\u2019s period is different, the way you experience perimenopause will be different. The intensity of symptoms varies from one woman to the next, but they follow a typical progression until you have reached menopause and do not have a period for 12 months. Irregular Periods \nThis is the hallmark of perimenopause.\n\nEvidence 5\nTitle: Perimenopause: Rocky road to menopause - Harvard Health\nContent: equine estrogen, 0.5 mg oral micronized estradiol, 25 micrograms (mcg) transdermal (patch) estradiol, or 2.5 mcg ethinyl estradiol \u2014 works for many women. Hormonal irregularities. If you need contraception and don't smoke, you can take low-dose birth control pills until menopause (see \"Irregular periods and heavy bleeding\"). Another advantage of these pills is that they regulate your menses and suppress the erratic hormonal ups and downs of perimenopause; some women report feeling more even-tempered while taking them. Severe hot flashes. Women with severe hot flashes who don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis.\nwith the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist. Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. Vaginal dryness. Low-dose contraceptives or vaginal estrogen (in a cream, ring, tablet, or gel) can help relieve vaginal dryness, but hormonal treatment is not the only approach. Vaginal moisturizers such as Replens, applied twice weekly, increase vaginal moisture, elasticity, and acidity. Continued sexual activity also seems to improve vaginal tone and helps maintain the acidic environment that protects it against infections. Lubricants such as K-Y Jelly, Astroglide, and K-Y Silk-E can make intercourse less painful.\nwho don't want or can't take a hormonal therapy may get some relief from newer antidepressants such as Effexor (venlafaxine) or certain selective serotonin reuptake inhibitors (SSRIs), for example, Prozac (fluoxetine) and Paxil (paroxetine); the epilepsy drug Neurontin (gabapentin); or clonidine, a blood pressure drug. Some of these medications have side effects that may limit their usefulness. Also, some SSRIs can interfere with the metabolism of tamoxifen in certain women. Irregular periods and heavy bleeding. If you have irregular bleeding and don't want to become pregnant, low-dose birth control pills are a good choice. By suppressing ovulation, they modulate menstrual flow, regulate periods, and stabilize endometriosis. They also protect against endometrial and ovarian cancers, stave off hot flashes, reduce vaginal dryness, and prevent bone loss. If you have abnormal bleeding, such as daily or very heavy bleeding, see your gynecologist. Oral contraceptives can be taken until menopause. To help determine whether you've reached menopause, your clinician may order a blood test of your FSH level, taken after seven days off the pill. But the only wholly reliable measure is 12 months off hormones without a menstrual period. Vaginal dryness.\n\n", "verification_result": "supported"}], "response_stats": {"sentences": 1, "verified_claims": 1, "verified_supported_claims": 1, "pre_supported_claims": 15, "pre_verified_claims": 15, "all_claims": 16, "P": 1.0}}
{"question": "In which year did Taylor Swift win a Golden Globe Award?", "prompt_source": "freshQA", "response": "Taylor Swift won her first Golden Globe Award in 2020. She received the award for Best Original Song for \"Beautiful Ghosts,\" which she co-wrote with Andrew Lloyd Webber for the film \"Cats.\"", "prompt_tok_cnt": 2060, "response_tok_cnt": 82, "model": "gpt-4o", "abstained": false, "predefined_stats_lst": [[1, 1, 2]], "pre_supported_claims_lst": ["\"Beautiful Ghosts\" was written for the film \"Cats.\""], "pre_unsupported_claims_lst": ["Taylor Swift won her first Golden Globe Award in 2020.", "Taylor Swift received the Golden Globe Award for Best Original Song for \"Beautiful Ghosts\" in 2020."], "claim_to_verify_list": [["Taylor Swift co-wrote \"Beautiful Ghosts\" with Andrew Lloyd Webber."]], "all_claims_to_verify": ["Taylor Swift co-wrote \"Beautiful Ghosts\" with Andrew Lloyd Webber."], "claim_search_results": {"Taylor Swift co-wrote \"Beautiful Ghosts\" with Andrew Lloyd Webber.": [{"text": "Jump to content \n   \n \n \n \n\n  \n \n \n  \n     \n  \n     \n   \n \n \n  \n \n \n \nArticle\nTalk    \n  English  \n \n \n \n        \n  \n \n \n \nRead\nEdit\nView history      \n  Tools  \n \n \n \n \nTools \nmove to sidebar \nhide  \n \n Actions  \n \n \nRead\nEdit\nView history    \n \n General  \n \n \nWhat links here\nRelated changes\nUpload file\nSpecial pages\nPermanent link\nPage information\nCite this page\nGet shortened URL\nDownload QR code \nEdit interlanguage links\nExpand all   \n \n Print/export  \n \n \nDownload as PDF\nPrintable version    \n \n In other projects  \n \n \nWikidata item            \n \n  \n    \n     \n \n \n  \nFrom Wikipedia, the free encyclopedia  \n\n \n\n\n2019 promotional single by Taylor Swift \n  \"Beautiful Ghosts\"Promotional single by Taylor Swiftfrom the album  Cats: Highlights from the Motion Picture Soundtrack ReleasedNovember 15, 2019 (2019-11-15)Recorded2019GenreOrchestralLength4:21LabelPolydorComposer(s)\n\n\nTaylor Swift\nAndrew Lloyd WebberLyricist(s)Taylor SwiftProducer(s)\n\n\nGreg Wells\nTom Hooper\nAndrew Lloyd WebberLyric video\"Beautiful Ghosts\" on YouTube  \n\"Beautiful Ghosts\" (also subtitled \"(From the Motion Picture Cats)\") is a song by the American singer-songwriter Taylor Swift and the English composer Andrew Lloyd Webber from the 2019 Cats film adaptation, in which Swift played Bombalurina. The song was written by Swift (lyrics and music) and Cats creator Andrew Lloyd Webber (music), and produced by Greg Wells, Lloyd Webber, and Cats director Tom Hooper. It was released on November 15, 2019. Considered as its signature song, it is performed in the film by the principal character Victoria, portrayed by Francesca Hayward as its signature song. A 30-second reprise is also sung by Judi Dench as Old Deuteronomy.[1] Swift performs the official single version that is played over the ending credits.[2] \nUpon release, the song received positive reviews from music critics. It received nominations for Best Original Song at the 77th Golden Globe Awards and Best Song Written for Visual Media at the 63rd Annual Grammy Awards\u2014Swift's third nomination for the latter, following \"Safe & Sound\" (2011) and \"I Don't Wanna Live Forever\" (2016).   \n\nBackground and release[edit] \nAs a concept, the then-untitled song was first reported in early 2018, when the film adaptation was in early stages of production.[3] The song was designed to provide a way for Victoria to introduce her character to the audience, as she communicates solely through gestures and dance in the original stage show.[2] Swift described the song as such:  \n\n\"Beautiful Ghosts\" is sung from a young voice who is wondering if she will ever have glory days. Longing for the sense of belonging she sees everyone finding. Reaching for it, desperately afraid of never having beautiful ghosts of days gone by to cling to in her older years.[4]\nThe song was released on digital platforms and to streaming services on November 15, 2019, one month before the film's theatrical release.[5] A lyric video for the song was released on YouTube the same day. The lyric video has over 6 million views on YouTube.[6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats.[7] \n\nComposition[edit] \nThe song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6\n8 and 9\n8 time signatures throughout most of the song, with the ending in compound septuple meter 21\n8.[8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber.[2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics.[1][4] According to Lloyd Webber, the two \"wrote 90 percent of it pretty much over an afternoon.\"[1] He described the collaboration with Swift as \"a joy\", the collaboration as \"one of the finest of [his] 50-year career\",[9] Swift's lyrics as \"brilliant\", and her performance of the song as \"emotional\" and \"among the very best [he's] ever had\". [10] Tom Hooper, director of the film and a producer of the song, complimented the \"extraordinary beauty\" of Swift's lyrics and praised Swift for her \"profound understanding of what we're trying to do with the movie.\"[11]  \n\nCritical reception[edit] \nIn contrast to the negative reception Cats received, the song received positive reviews from music critics, who praised Swift's vocal performance. Darlene Aderoju and Joelle Goldstein of People described the song as \"chillingly-beautiful\" and a \"haunting melody\" that \"speaks to a feeling of longing to be wanted and reminiscing on better memories.\" They also praised Swift's singing, describing it as \"impressive pipes beyond her typical country-pop style\", especially the final belted note.[11] Writing for HuffPost, Ron Dicker complimented Swift's vocals, referring to the song as \"hauntingly gorgeous\" and an \"absolute showstopper\".[12] Elite Daily's Jessica Bola\u00f1os opined that the song is \"nothing short of perfection\" and stated that the lyrics are \"dripping with passion\".[13][14] Chris Willman of Variety wrote that the song is a return to the \"youthful tonality\" in Swift's voice, typical of \"her Fearless days\".[15] Brittany Spanos and Ryan Reeds of Rolling Stone described the song as \"cinematic\" and \"wistful\". They also opined that the song \"builds to a rousing climax with the singer's voice in a full roar\".[16] MTV's Madeline Roth praised Swift's vocal performance and labeled the final high note of the song as \"more powerful than anything we've heard from T. Swift in recent memory\".[17] Carolyn Droke of Uproxx wrote that the song is about \"the melancholy feeling that comes with reminiscing on old memories\" and appreciated Swift for showing off her vocal range.[18] \nOthers gave the song less favourable reviews. Adam Feldman wrote for Time Out that \"Beautiful Ghosts\" is \"terrible\", saying that \"Swift's lyrics clank with banality at nearly every turn of phrase\".[19] Johnny Oleksinski of the New York Post agreed, calling the song \"a boring 'Memory' copycat\".[20] Leah Marilla Thomas of Cosmopolitan praised the final chorus, but described the song as \"a little bonkers\" and said that listeners need to lower their expectations before listening.[21] Various Vulture staff also criticized the song, describing the lyrics as nonsensical and corny; Swift's vocal performance was criticised, with Rebecca Alter summarizing the song as Swift \"leaping for that note and falling off a cliff\".[22] In June 2022, Insider ranked \"Beautiful Ghosts\" as Swift's worst soundtrack song.[23]  \n\nAccolades[edit] \nThe song was nominated for Best Original Song at the 77th Golden Globe Awards, becoming Swift's third nomination in the category, following \"Safe & Sound\" (2013) and \"Sweeter than Fiction\" (2014).[24] At the 63rd Annual Grammy Awards, \"Beautiful Ghosts\" is nominated for Best Song Written for Visual Media, marking Swift's third nomination in the category, following \"Safe & Sound\" (2011) and \"I Don't Wanna Live Forever\" (2016).    Year Organization Award Result Ref.   \n2020  Golden Globe Awards  Best Original Song  Nominated  [25]   Huading Awards  Best Global Film Theme Song  Nominated  [26]   \n2021  Grammy Awards  Best Song Written for Visual Media  Nominated  [27]   \n\nCredits and personnel[edit] \nCredits adapted from Tidal.[28]  \n \n\nTaylor Swift \u2013 vocals, songwriter \nAndrew Lloyd Webber \u2013 producer, songwriter \nGreg Wells \u2013 producer \nTom Hooper \u2013 producer \nGus Pirelli \u2013 engineer, studio personnel \nRyan Smith \u2013 mastering engineer, studio personnel \nSerban Ghenea \u2013 mixer, studio personnel \nJohn Ashton Thomas \u2013 conductor \nEverton Nelson \u2013 orchestra leader \nSusie Gillis \u2013 orchestra contractor \nEliza Marshall, Helen Keen \u2013 alto flute, flute \nAnthony Pike \u2013 bass clarinet \nJodi Milliner \u2013 bass guitar \nAndy Wood \u2013 bass trombone \nAdrian Bradbury, Caroline Dearnley, Chris Worsey, Frank Schaefer, Ian Burdge, Paul Kegg \u2013 cello \nJohn Carnac \u2013 clarinet \nJane Marshall \u2013 cor anglais \nAllen Walley, Mary Scully, Richard Pryce, Steve Mair \u2013 double bass \nAlexei Watkins, Martin Owen, Richard Watkins, Simon Rayner \u2013 French horn \nSkaila Kanga \u2013 harp \nDavid Thomas \u2013 oboe \nChris Baron, Frank Ricotti \u2013 percussion \nJohn Ashton Thomas, Peter Murray \u2013 piano \nBill Lockhart \u2013 timpani \nEd Tarrant, Mark Nightingale \u2013 trombone \nDan Newell, Kate Moore, Pat White \u2013 trumpet \nOwen Slade \u2013 tuba \nAndy Parker, Gillianne Haddow, Gustav Clarkson, Jake Walker, Martin Humbey, Peter Lale, Polly Wiltshire, Sue Dench \u2013 viola \nDai Emanuel, Dave Williams, Debbie Preece, Debbie Widdup, Emil Chakalov, Everton Nelson, Ian Humphries, Jonathan Evans-Jones, Julian Leaper, Kate Robinson, Laura Melhuish, Lorraine McAslan, Mark Berrow, Martyn Jackson, Miranda Dale, Natalia Bonner, Odile Ollagnon, Patrick Kiernan, Paul Willey, Ralph de Souza, Richard George, Rick Koster \u2013 violin  \n\nCharts[edit]  \nChart performance for \"Beautiful Ghosts\"   Chart (2019)  Peak\nposition   Australia Digital Tracks (ARIA)[29]  22   Canada Digital Song Sales (Billboard)[30]  31   Greece Digital Songs (Billboard)[31]  5   Hungary (Single Top 40)[32]  26   New Zealand Hot Singles (RMNZ)[33]  36   Scotland (OCC)[34]  44   UK Singles Downloads (OCC)[35]  32   US Digital Song Sales (Billboard)[36]  14  \n\nRelease history[edit]  \nRelease history and formats for \"Beautiful Ghosts\"   Country  Date  Format  Label  Ref.   Various  November 15, 2019  \n\n\nDigital download\nstreaming  Polydor  [37]  \n\nReferences[edit] \n \n\n \n^ Jump up to: a b c Burlingame, Jon (October 24, 2019). \"Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for 'Cats'\". Variety. Archived from the original on October 25, 2019. Retrieved October 25, 2019.  \n^ Jump up to: a b c Vlessing, Etan (October 24, 2019). \"Andrew Lloyd Webber Talks Working With Taylor Swift on New 'Cats' Song\". The Hollywood Reporter. Archived from the original on October 25, 2019. Retrieved October 24, 2019.  \n^ Gans, Andrew (January 5, 2018). \"Andrew Lloyd Webber Pens New Song for Possible Cats Film\". Playbill. Archived from the original on October 25, 2019. Retrieved October 24, 2019.  \n^ Jump up to: a b Swift, Taylor (November 15, 2019). \"The Importance of 'Cats,' in Taylor Swift's Own Words\". Billboard. Archived from the original on November 16, 2019. Retrieved November 16, 2019.  \n^ Aniftos, Rania (November 15, 2019). \"Taylor Swift Releases 'Beautiful Ghosts,' Co-Written With Andrew Lloyd Webber for 'Cats' Film\". Billboard. Archived from the original on November 19, 2019. Retrieved November 15, 2019.  \n^ Swift, Taylor (November 15, 2019), \"Taylor Swift - Beautiful Ghosts (From The Motion Picture \"Cats\" / Lyric Video)\", YouTube, archived from the original on November 29, 2019, retrieved November 30, 2019  \n^ Rubin, Rebecca (October 12, 2021). \"Andrew Lloyd Webber Says Writing a Song With Taylor Swift Was the Only Enjoyable Part of 'Cats' Movie\". Variety. Archived from the original on October 13, 2021. Retrieved October 14, 2021.  \n^ \"Taylor Swift \"Beautiful Ghosts\"\". www.musicnotes.com. November 15, 2019. Archived from the original on April 20, 2021. Retrieved November 18, 2019.  \n^ Lloyd Webber, Andrew [@OfficialALW] (November 18, 2019). \"Thrilled at the reaction to \"Beautiful Ghosts\". This collaboration with @taylorswift13 has been a joy, one of the finest of my 50-year career\" (Tweet). Retrieved November 23, 2019 \u2013 via Twitter.  \n^ Lloyd Webber, Andrew [@OfficialALW] (December 1, 2019). \"Taylor's lyric for Beautiful Ghosts is brilliant and her emotional performance of our song must be among the very best I've ever had\" (Tweet). Retrieved December 2, 2019 \u2013 via Twitter.  \n^ Jump up to: a b Aderoju, Darlene; Goldstein, Joelle (November 15, 2019). \"Taylor Swift Shows Off Her Impressive Vocals in New Song 'Beautiful Ghosts' from Cats Musical Film\". People. Archived from the original on November 15, 2019. Retrieved November 15, 2019.  \n^ Dicker, Ron (November 15, 2019). \"Taylor Swift's New Song For 'Cats' Is An Absolute Showstopper\". HuffPost. Archived from the original on November 16, 2019. Retrieved November 15, 2019.  \n^ Bola\u00f1os, Jessica (November 15, 2019). \"Taylor Swift's \"Beautiful Ghosts\" Lyrics Will Make You So Emotional\". Elite Daily. Archived from the original on November 15, 2019. Retrieved November 15, 2019.  \n^ Bola\u00f1os, Jessica (November 15, 2019). \"Taylor Swift's \"Beautiful Ghosts\" From 'Cats' Shows Off Her English Accent\". Elite Daily. Archived from the original on November 15, 2019. Retrieved November 15, 2019.  \n^ Willman, Chris (November 15, 2019). \"Taylor Swift Releases 'Beautiful Ghosts,' Her Andrew Lloyd Webber Collaboration for 'Cats'\". Variety. Archived from the original on November 15, 2019. Retrieved November 16, 2019.  \n^ Spanos, Brittany; Reeds, Ryan (November 15, 2019). \"Hear Taylor Swift's New 'Cats' Soundtrack Song 'Beautiful Ghosts'\". Rolling Stone. Archived from the original on November 15, 2019. Retrieved November 16, 2019.  \n^ Roth, Madeline (November 15, 2019). \"Taylor Swift Drops Her Pitch-Purrfect Cats Ballad 'Beautiful Ghosts'\". MTV. Archived from the original on November 16, 2019. Retrieved November 16, 2019.  \n^ Droke, Carolyn (November 19, 2019). \"All The Best New Pop Music From This Week\". Uproxx. Archived from the original on November 20, 2019. Retrieved November 20, 2019.  \n^ Feldman, Adam (November 15, 2019). \"The Taylor Swift song from Cats has dropped and it's terrible\". Time Out New York. Archived from the original on December 9, 2019. Retrieved December 10, 2019.  \n^ Oleksinski, Johnny (November 15, 2019). \"Taylor Swift's 'Beautiful Ghosts' is a boring 'Memory' copycat\". New York Post. Archived from the original on December 9, 2019. Retrieved December 10, 2019.  \n^ Thomas, Leah Marilla (November 18, 2019). \"Taylor Swift's 'Cats' Song Is Here and There's Sooo Much to Unpack\". Cosmopolitan. Archived from the original on December 9, 2019. Retrieved December 10, 2019.  \n^ \"Let Us Gather to Discuss Taylor Swift and Andrew Lloyd Webber's Cats Song\". Vulture. December 9, 2019. Archived from the original on December 12, 2019. Retrieved December 11, 2019.  \n^ Ahlgrim, Callie; Larocca, Courteney (June 26, 2022). \"Taylor Swift's 8 soundtrack songs, ranked from worst to best\". Insider. Archived from the original on February 7, 2023. Retrieved June 26, 2022.  \n^ Hipes, Patrick (December 9, 2019). \"Golden Globes Nominations\". Deadline. Archived from the original on December 11, 2019. Retrieved December 10, 2019.  \n^ Babiana, Ana Maria (December 9, 2019). \"Nominations for the 77th Golden Globes Have Been Announced\". Hollywood Foreign Press Association. Archived from the original on December 11, 2019. Retrieved December 10, 2019.  \n^ \"Nominations Announced for the 28th Huading Awards -- \"The Global Film Satisfaction Survey Release Ceremony\"\". Tyler Morning Telegraph. October 23, 2020. Archived from the original on November 15, 2020. Retrieved November 14, 2020.  \n^ \"2021 GRAMMYs: Complete Nominees List\". GRAMMY.com. November 24, 2020. Archived from the original on November 28, 2020. Retrieved November 24, 2020.  \n^ \"\"Beautiful Ghosts\" (From The Motion Picture Cats) by Taylor Swift\". Tidal. Archived from the original on November 16, 2019. Retrieved November 16, 2019.  \n^ \"ARIA Australian Top 40 Digital Tracks\" (PDF). Australian Recording Industry Association. November 25, 2019. Archived (PDF) from the original on November 23, 2019. Retrieved November 23, 2019.  \n^ \"Taylor Swift Chart History (Hot Canadian Digital Song Sales)\". Billboard. Archived from the original on December 13, 2020. Retrieved December 11, 2019.  \n^ \"Greece Digital Song Sales\". Billboard. Archived from the original on December 6, 2019. Retrieved December 6, 2019.  \n^ \"Arch\u00edvum \u2013 Sl\u00e1gerlist\u00e1k \u2013 MAHASZ\" (in Hungarian). Single (track) Top 40 lista. Magyar Hanglemezkiad\u00f3k Sz\u00f6vets\u00e9ge. Retrieved November 29, 2019.  \n^ \"NZ Hot Singles Chart\". Recorded Music NZ. November 25, 2019. Archived from the original on November 22, 2019. Retrieved November 22, 2019.  \n^ \"Official Scottish Singles Sales Chart Top 100\". Official Charts Company. Retrieved November 23, 2019.  \n^ \"Official Singles Downloads Chart Top 100\". Official Charts Company. Retrieved December 11, 2019.  \n^ \"Taylor Swift Chart History (Digital Song Sales)\". Billboard. Retrieved December 5, 2019.  \n^ \"Beautiful Ghosts (From the Motion Picture \"Cats\") - Single by Taylor Swift on Apple Music\". Apple Music. Archived from the original on November 15, 2019. Retrieved November 15, 2019.   \n\n\nshow\n\n\nv\nt\ne\nTaylor Swift songs\n \n\nSingles discography Taylor Swift\n \n\n\"Tim McGraw\" \n\"Picture to Burn\" \n\"Teardrops on My Guitar\" \n\"Should've Said No\" \n\"Our Song\" Fearless\n \n\n\"Fearless\" \n\"Fifteen\" \n\"Love Story\" \n\"Hey Stephen\" \n\"White Horse\" \n\"You Belong with Me\" \n\"Breathe\" \n\"You're Not Sorry\" \n\"The Way I Loved You\" \n\"Forever & Always\" \n\"The Best Day\" \n\"Change\" Taylor's Version\n \n\n\"You All Over Me\" \n\"Mr. Perfectly Fine\" \n\"That's When\" \n Speak Now\n \n\n\"Mine\" \n\"Sparks Fly\" \n\"Back to December\" \n\"Speak Now\" \n\"Dear John\" \n\"Mean\" \n\"The Story of Us\" \n\"Never Grow Up\" \n\"Enchanted\" \n\"Better than Revenge\" \n\"Innocent\" \n\"Haunted\" \n\"Long Live\" \n\"Ours\" Taylor's Version\n \n\n\"Electric Touch\" \n\"When Emma Falls in Love\" \n\"I Can See You\" \n\"Castles Crumbling\" \n Red\n \n\n\"State of Grace\" \n\"Red\" \n\"Treacherous\" \n\"I Knew You Were Trouble\" \n\"All Too Well\" \n\"22\" \n\"We Are Never Ever Getting Back Together\" \n\"The Last Time\" \n\"Holy Ground\" \n\"Everything Has Changed\" \n\"Begin Again\" Taylor's Version\n \n\n\"Nothing New\" \n\"Message in a Bottle\" \n\"I Bet You Think About Me\" \n\"Forever Winter\" \n 1989\n \n\n\"Welcome to New York\" \n\"Blank Space\" \n\"Style\" \n\"Out of the Woods\" \n\"All You Had to Do Was Stay\" \n\"Shake It Off\" \n\"I Wish You Would\" \n\"Bad Blood\" \n\"Wildest Dreams\" \n\"How You Get the Girl\" \n\"This Love\" \n\"I Know Places\" \n\"Clean\" \n\"You Are in Love\" \n\"New Romantics\" Taylor's Version\n \n\n\"'Slut!'\" \n\"Say Don't Go\" \n\"Now That We Don't Talk\" \n\"Suburban Legends\" \n\"Is It Over Now?\" \n Reputation\n \n\n\"...Ready for It?\" \n\"End Game\" \n\"I Did Something Bad\" \n\"Don't Blame Me\" \n\"Delicate\" \n\"Look What You Made Me Do\" \n\"So It Goes...\" \n\"Gorgeous\" \n\"Getaway Car\" \n\"Dress\" \n\"Call It What You Want\" \n\"New Year's Day\" Lover\n \n\n\"I Forgot That You Existed\" \n\"Cruel Summer\" \n\"Lover\" \n\"The Man\" \n\"The Archer\" \n\"Miss Americana & the Heartbreak Prince\" \n\"Paper Rings\" \n\"Cornelia Street\" \n\"Death by a Thousand Cuts\" \n\"London Boy\" \n\"Soon You'll Get Better\" \n\"False God\" \n\"You Need to Calm Down\" \n\"Me!\" Folklore\n \n\n\"The 1\" \n\"Cardigan\" \n\"The Last Great American Dynasty\" \n\"Exile\" \n\"My Tears Ricochet\" \n\"Mirrorball\" \n\"Seven\" \n\"August\" \n\"This Is Me Trying\" \n\"Illicit Affairs\" \n\"Invisible String\" \n\"Mad Woman\" \n\"Epiphany\" \n\"Betty\" \n\"Peace\" \n\"Hoax\" \n\"The Lakes\" Evermore\n \n\n\"Willow\" \n\"Champagne Problems\" \n\"'Tis the Damn Season\" \n\"Tolerate It\" \n\"No Body, No Crime\" \n\"Happiness\" \n\"Dorothea\" \n\"Coney Island\" \n\"Long Story Short\" \n\"Marjorie\" Midnights\n \n\n\"Lavender Haze\" \n\"Maroon\" \n\"Anti-Hero\" \n\"Snow on the Beach\" \n\"You're on Your Own, Kid\" \n\"Midnight Rain\" \n\"Question...?\" \n\"Vigilante Shit\" \n\"Bejeweled\" \n\"Labyrinth\" \n\"Karma\" \n\"Sweet Nothing\" \n\"Mastermind\" \n\"Hits Different\" \n\"Bigger Than the Whole Sky\" \n\"Would've, Could've, Should've\" \n\"You're Losing Me\" The Tortured Poets\nDepartment\n \n\n\"Fortnight\" \n\"The Tortured Poets Department \n\"My Boy Only Breaks His Favorite Toys\" \n\"Down Bad\" \n\"So Long, London\" \n\"But Daddy I Love Him\" \n\"Fresh Out the Slammer\" \n\"Florida!!!\" \n\"Guilty as Sin?\" \n\"Who's Afraid of Little Old Me?\" \n\"Loml\" \n\"I Can Do It with a Broken Heart\" \n\"The Smallest Man Who Ever Lived\" \n\"The Alchemy\" \n\"Clara Bow\" \n\"The Black Dog\" \n\"So High School\" \n\"Thank You Aimee\" Soundtrack songs\n \n\n\"Crazier\" \n\"Today Was a Fairytale\" \n\"Safe & Sound\" \n\"Eyes Open\" \n\"Sweeter than Fiction\" \n\"I Don't Wanna Live Forever\" \n\"Beautiful Ghosts\" \n\"Only the Young\" \n\"Carolina\" Featured songs\n \n\n\"Two Is Better Than One\" \n\"Half of My Heart\" \n\"Both of Us\" \n\"Babe\" \n\"Gasoline\" \n\"Renegade\" \n\"The Joker and the Queen\" \n\"The Alcott\" \n\"Us\" Other songs\n \n\n\"Best Days of Your Life\" \n\"Ronan\" \n\"Highway Don't Care\" \n\"Better Man\" \n\"Christmas Tree Farm\" \n\"All of the Girls You Loved Before\" \n \n\n Category  \n\n\nshow\n\n\nv\nt\ne\nAndrew Lloyd Webber\nAccoladesMusicals\n \n\nJesus Christ Superstar \nJoseph and the Amazing Technicolor Dreamcoat \nEvita \nTell Me on a Sunday \nCats \nSong and Dance \nStarlight Express \nCricket \nThe Phantom of the Opera \nAspects of Love \nSunset Boulevard \nBy Jeeves (aka Jeeves) \nWhistle Down the Wind \nThe Beautiful Game (aka The Boys in the Photograph) \nThe Woman in White \nThe Likes of Us \nLove Never Dies \nThe Wizard of Oz \nStephen Ward \nSchool of Rock \nBad Cinderella (aka Cinderella) Songs\n \n\n\"Amigos Para Siempre\" \n\"Any Dream Will Do\" \n\"Bad Cinderella\" \n\"Close Every Door\" \n\"Everything's Alright\" \n\"This Jesus Must Die\" \n\"I Don't Know How to Love Him\" \n\"Superstar\" \n\"Oh What a Circus\" \n\"Another Suitcase in Another Hall\" \n\"Don't Cry for Me Argentina\" \n\"You Must Love Me\" \n\"Take That Look Off Your Face\" \n\"Unexpected Song\" \n\"Memory\" \n\"Pie Jesu\" \n\"The Phantom of the Opera\" \n\"The Music of the Night\" \n\"All I Ask of You\" \n\"Anything But Lonely\" \n\"Tetris\" \n\"As If We Never Said Goodbye\" \n\"The Perfect Year\" \n\"No Matter What\" \n\"It's My Time\" \n\"'Til I Hear You Sing\" \n\"Beautiful Ghosts\" Albums\n \n\nJesus Christ Superstar \nEvita \nVariations \nCats \nThe Premiere Collection \nThe Premiere Collection Encore \nThe Collection \nNow & Forever Other compositions\n \n\nRequiem \nWatership Down Film adaptations\n \n\nJesus Christ Superstar \n\nSoundtrack \nEvita \nCats (1998) \nJoseph and the Amazing Technicolor Dreamcoat \nThe Phantom of the Opera \nThe Phantom of the Opera at the Royal Albert Hall \nJesus Christ Superstar Live in Concert \nCats (2019) Family\n \n\nWilliam Lloyd Webber (father) \nJulian Lloyd Webber (brother) \nImogen Lloyd Webber (daughter) \nNick Lloyd Webber (son) \nSarah Brightman (second wife) \nMadeleine Gurdon (third wife) \nJiaxin Cheng (sister-in-law) Company\n \n\nReally Useful Group  \n\n\nshow\n\n\nv\nt\ne\nAndrew Lloyd Webber's Cats\n \n\nBased on Old Possum's Book of Practical Cats by T. S. Eliot Characters\n \n\nBombalurina \nDemeter \nGrizabella \nGus \nJellylorum \nJemima \nJennyanydots \nMacavity \nMr. Mistoffelees \nMungojerrie \nMunkustrap \nOld Deuteronomy \nRum Tum Tugger \nRumpleteazer \nRumpus Cat \nSkimbleshanks \nVictoria the White Cat Music\nMusical\n \n\n\"Memory\" Musical cast recording\n \n\nComplete Original Broadway Cast Recording 2019 film\n \n\nHighlights from the Motion Picture Soundtrack \n\"Beautiful Ghosts\" \nFilm adaptations\n \n\n1998 \n2019 Other\n \n\nAccolades \n\"The Naming of Cats\" \n\"The Awefull Battle of the Pekes and the Pollicles\" \n \n\n Category  \n\nAuthority control databases \n\n\nMusicBrainz work\nMusicBrainz release group     \nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Beautiful_Ghosts&oldid=1259058735\" \n\nCategories: \n\n2019 songs\n2019 singles\n2010s ballads\nSongs written by Taylor Swift\nTaylor Swift songs\nSongs with music by Andrew Lloyd Webber\nSong recordings produced by Andrew Lloyd Webber\nSong recordings produced by Greg Wells\nPolydor Records singles\nSongs written for films\nCats (musical)\nPop ballads\nMusic based on works by T. S. Eliot\nHidden categories: \n\nArticles with short description\nShort description is different from Wikidata\nUse mdy dates from October 2021\nArticles with hAudio microformats\nSingle chart usages for Hungarysingle\nSingle chart called without artist\nSingle chart called without song\nSingle chart usages for Scotland\nSingle chart usages for UKdownload\nSingle chart usages for Billboarddigitalsongs    \n \n \n \n This page was last edited on 23 November 2024, at 03:38 (UTC). \nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.  \n \nPrivacy policy \nAbout Wikipedia \nDisclaimers \nContact Wikipedia \nCode of Conduct \nDevelopers \nStatistics \nCookie statement \nMobile view \nEdit preview settings \n \n \n      \n \n   \n\n", "title": "Beautiful Ghosts - Wikipedia", "description": "&quot;<strong>Beautiful</strong> <strong>Ghosts</strong>&quot; (also subtitled &quot;(From the Motion Picture Cats)&quot;) is a song by the American singer-songwriter <strong>Taylor</strong> <strong>Swift</strong> and the English composer <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> from the 2019 Cats film adaptation, in which <strong>Swift</strong> played Bombalurina. The song was written by <strong>Swift</strong> (lyrics and music) ...", "usage": {"tokens": 6694}, "retrieved_text": ["The lyric video has over 6 million views on YouTube. [6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats. [7] Composition[edit] The song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6 8 and 9 8 time signatures throughout most of the song, with the ending in compound septuple meter 21 8. [8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber. [2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics. [1][4] According to Lloyd Webber, the two \"wrote 90 percent of it pretty much over an afternoon.", "years. [4] The song was released on digital platforms and to streaming services on November 15, 2019, one month before the film's theatrical release. [5] A lyric video for the song was released on YouTube the same day. The lyric video has over 6 million views on YouTube. [6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats. [7] Composition[edit] The song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6\n8 and 9\n8 time signatures throughout most of the song, with the ending in compound septuple meter 21\n8. [8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber. [2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics.", "Jump to content \n   \n \n \n \n\n  \n \n \n  \n     \n  \n     \n   \n \n \n  \n \n \n \nArticle\nTalk    \n  English  \n \n \n \n        \n  \n \n \n \nRead\nEdit\nView history      \n  Tools  \n \n \n \n \nTools \nmove to sidebar \nhide  \n \n Actions  \n \n \nRead\nEdit\nView history    \n \n General  \n \n \nWhat links here\nRelated changes\nUpload file\nSpecial pages\nPermanent link\nPage information\nCite this page\nGet shortened URL\nDownload QR code \nEdit interlanguage links\nExpand all   \n \n Print/export  \n \n \nDownload as PDF\nPrintable version    \n \n In other projects  \n \n \nWikidata item            \n \n  \n    \n     \n \n \n  \nFrom Wikipedia, the free encyclopedia  \n\n \n\n\n2019 promotional single by Taylor Swift \n  \"Beautiful Ghosts\"Promotional single by Taylor Swiftfrom the album  Cats: Highlights from the Motion Picture Soundtrack ReleasedNovember 15, 2019 (2019-11-15)Recorded2019GenreOrchestralLength4:21LabelPolydorComposer(s)\n\n\nTaylor Swift\nAndrew Lloyd WebberLyricist(s)Taylor SwiftProducer(s)\n\n\nGreg Wells\nTom Hooper\nAndrew Lloyd WebberLyric video\"Beautiful Ghosts\" on YouTube  \n\"Beautiful Ghosts\" (also subtitled \"(From the Motion Picture Cats)\") is a song by the American singer-songwriter Taylor Swift and the English composer Andrew Lloyd Webber from the 2019 Cats film adaptation, in which Swift played Bombalurina. The song was written by Swift (lyrics and music) and Cats creator Andrew Lloyd Webber (music), and produced by Greg Wells, Lloyd Webber, and Cats director Tom Hooper. It was released on November 15, 2019."]}, {"text": "Home\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\nHome\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\n\n\n\n\n\n\nShare\n\n\n\nToday, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below:\n\n\nAs part of the release, an official lyric video has launched to accompany the song:\n\u200d\n\n\n\u200d\n\u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles.\nA behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here.\nOne of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201cBeautiful Ghosts,\u201d he says, is now, \u201can incredibly important and central part of the whole film.\u201d\nFor director Tom Hooper, the song exceeded even his high expectations. \u201cWhat I couldn\u2019t believe was the extraordinary beauty of these lyrics that Taylor had written,\u201d Hooper says. \u201cIt was as if someone had come into the film I was in the middle of making and \u2026 had reflected back to me a profound understanding of what we are trying to do with the movie. It was utterly thrilling.\u201d\n\n\n\n\nThis was a Box Five Club Exclusive\nMembers of The Box Five Club had access to this story before anybody else.\nBy signing up, you\u2019ll be among the first to receive exclusive content, special announcements, and behind-the-scenes updates, delivered straight to your inbox, as well as gaining access to the exclusive Box Five store. Join now and stay connected to all things Andrew Lloyd Webber!\nJoin Now for Free\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Phantom of the Opera | The London Cast Roundtable\nThe Phantom of the Opera | The London Cast Roundtable\nBox Five - Email Exclusive\nThe Phantom of the Opera | The London Cast Roundtable\nBox Five - Email Exclusive\nThe Phantom of the Opera | The London Cast Roundtable\n\n\nThe Royal Variety Performance 2024 - Starlight Express \nThe Royal Variety Performance 2024 - Starlight Express \nBox Five - Email Exclusive\nThe Royal Variety Performance 2024 - Starlight Express \nBox Five - Email Exclusive\nThe Royal Variety Performance 2024 - Starlight Express \n\n\n\u2019Tis sing-along season \n\u2019Tis sing-along season \nBox Five - Email Exclusive\n\u2019Tis sing-along season \nBox Five - Email Exclusive\n\u2019Tis sing-along season \n\n\nStocking Filler Gift Guide \nStocking Filler Gift Guide \nBox Five - Email Exclusive\nStocking Filler Gift Guide \nBox Five - Email Exclusive\nStocking Filler Gift Guide \n\n\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\nBox Five - Email Exclusive\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\nBox Five - Email Exclusive\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\n\n\nDonny Osmond\u2019s Pharaoh Story \nDonny Osmond\u2019s Pharaoh Story \nBox Five - Email Exclusive\nDonny Osmond\u2019s Pharaoh Story \nBox Five - Email Exclusive\nDonny Osmond\u2019s Pharaoh Story \n\n\n\n\n\n\nAll News\n\n\n\n\n\n\n\n\nTerms & Conditions\n\u25c6Privacy Policy & Cookies\n\n\u00a9 2025 Really Useful Group Ltd. \n\n           \n     ", "title": "Andrew & Taylor Swift's \"Beautiful Ghosts\" is out now! \u2022 Andrew Lloyd Webber Musicals", "description": "One of the longest-running shows ... <strong>Lloyd</strong> <strong>Webber</strong>, <strong>Swift</strong> and the filmmakers were excited at the idea of introducing a new song for the film adaptation.<strong>Andrew</strong> saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201c<strong>Beautiful</strong> <strong>Ghosts</strong>,\u201d he says, ...", "usage": {"tokens": 1042}, "retrieved_text": ["Home Composer Music Shows News Fan Club Store Home Composer Music Shows News Fan Club Store Share Today, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below: As part of the release, an official lyric video has launched to accompany the song: \u200d \u200d \u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here. One of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film.", "Home\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\nHome\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\n\n\n\n\n\n\nShare\n\n\n\nToday, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below:\n\n\nAs part of the release, an official lyric video has launched to accompany the song:\n\u200d\n\n\n\u200d\n\u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here.", "role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here. One of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201cBeautiful Ghosts,\u201d he says, is now, \u201can incredibly important and central part of the whole film.\u201d\nFor director Tom Hooper, the song exceeded even his high expectations. \u201cWhat I couldn\u2019t believe was the extraordinary beauty of these lyrics that Taylor had written,\u201d Hooper says."]}, {"text": " \n\n\n2024, In ReviewFeaturedChartsVideosPromote Your Music\n\nSign Up\n\n\n\n\nGenius Q&A\nWho wrote \u201cBeautiful Ghosts\u201d by Taylor Swift?\n\nCommunity Answer\n\n\n\u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift.\n\n\nBeautiful Ghosts\n\n\nTaylor Swift\n\n136.6K ViewsRead the Lyrics \n\n\nSourced by 30 Genius contributors\nSign up to add your knowledge\n\n\nRelated questions about \u201cBeautiful Ghosts\u201d\nWho produced \u201cBeautiful Ghosts\u201d by Taylor Swift?\nWhen did Taylor Swift release \u201cBeautiful Ghosts\u201d?\n\nRelated questions about Taylor Swift\nWhen did Taylor Swift's first album release?\nWhat is the most popular album by Taylor Swift?\nWhat is the most popular song by Taylor Swift?\nWhen did Taylor Swift start making music?\n\nSign Up and Drop Knowledge\nKnow something we don\u2019t about \u201cBeautiful Ghosts\u201d by \nTaylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love.\nSign Up\n\n\n\nGenius is the world\u2019s biggest collection of song lyrics and musical knowledge\n\nAbout GeniusContributor GuidelinesPressShopAdvertisePrivacy Policy\nLicensingJobsDevelopersCopyright PolicyContact UsSign InDo Not Sell My Personal Information\n\n\u00a9 2025 ML Genius Holdings, LLC\nTerms of Use\n\nVerified Artists\n\nAll Artists:abcdefghijklmnopqrstuvwxyz#\n\nHot Songs:\nDum, Dumb, and DumberF U 2XListen UpPeggyI Always Wanted A BrotherView All    \n           \n", "title": "Who wrote \u201cBeautiful Ghosts\u201d by Taylor Swift?", "description": "\u201c<strong>Beautiful</strong> <strong>Ghosts</strong>\u201d <strong>by</strong> <strong>Taylor</strong> <strong>Swift</strong> was written by <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> &amp; <strong>Taylor</strong> <strong>Swift</strong>.", "usage": {"tokens": 388}, "retrieved_text": ["2024, In ReviewFeaturedChartsVideosPromote Your Music Sign Up Genius Q&A Who wrote \u201cBeautiful Ghosts\u201d by Taylor Swift? Community Answer \u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift. Beautiful Ghosts Taylor Swift 136.6K ViewsRead the Lyrics Sourced by 30 Genius contributors Sign up to add your knowledge Related questions about \u201cBeautiful Ghosts\u201d Who produced \u201cBeautiful Ghosts\u201d by Taylor Swift? When did Taylor Swift release \u201cBeautiful Ghosts\u201d? Related questions about Taylor Swift When did Taylor Swift's first album release? What is the most popular album by Taylor Swift? What is the most popular song by Taylor Swift? When did Taylor Swift start making music? Sign Up and Drop Knowledge Know something we don\u2019t about \u201cBeautiful Ghosts\u201d by Taylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love. Sign Up\n\n\n\nGenius is the world\u2019s biggest collection of song lyrics and musical knowledge\n\nAbout GeniusContributor GuidelinesPressShopAdvertisePrivacy Policy\nLicensingJobsDevelopersCopyright PolicyContact UsSign InDo Not Sell My Personal Information\n\n\u00a9 2025 ML Genius Holdings, LLC\nTerms of Use\n\nVerified Artists\n\nAll Artists:abcdefghijklmnopqrstuvwxyz#\n\nHot Songs:\nDum, Dumb, and DumberF U 2XListen UpPeggyI Always Wanted A BrotherView All", " \n\n\n2024, In ReviewFeaturedChartsVideosPromote Your Music\n\nSign Up\n\n\n\n\nGenius Q&A\nWho wrote \u201cBeautiful Ghosts\u201d by Taylor Swift? Community Answer\n\n\n\u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift. Beautiful Ghosts\n\n\nTaylor Swift\n\n136.6K ViewsRead the Lyrics \n\n\nSourced by 30 Genius contributors\nSign up to add your knowledge\n\n\nRelated questions about \u201cBeautiful Ghosts\u201d\nWho produced \u201cBeautiful Ghosts\u201d by Taylor Swift? When did Taylor Swift release \u201cBeautiful Ghosts\u201d? Related questions about Taylor Swift\nWhen did Taylor Swift's first album release? What is the most popular album by Taylor Swift? What is the most popular song by Taylor Swift? When did Taylor Swift start making music? Sign Up and Drop Knowledge\nKnow something we don\u2019t about \u201cBeautiful Ghosts\u201d by \nTaylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love."]}, {"text": "             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Nov 15, 2019 6:40am PT    \n Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019  \n  \n \n \n \n \n \n \n \n By  Chris Willman  \n Plus Icon    \n  \n  Chris Willman   \nSenior Music Writer and Chief Music Critic \n  ChrisWillman  \n \nLatest \n  Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour  3 days ago   \n  Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86  3 days ago   \n  \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time  5 days ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nNot to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. \nHers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical. It\u2019ll also be heard as a reprise sung by Judi Dench. Swift\u2019s version is not, as rumored by some, a \u201cpop\u201d version; with its lush orchestration and dynamics, it\u2019s very much of a piece with the music that\u2019ll be heard in the body of the film. \n \n \n \n \n      \n \nAlthough it\u2019s hardly a country song, Swift is indulging in an old country music tradition, as well as a Broadway one: the answer song. And what it\u2019s answering is the tune that broke out of the stage \u201cCats\u201d and became a standard in the 1980s, \u201cMemory.\u201d In the film, as on stage, Grizabella (Jennifer Hudson) will sing that standard about her glamour days being over, Now, Victoria essentially tells Grizabella that at least memories are better than nothing, which is what she feels she has. \n \n \n Related Stories  \n \n \n \n  \n     \n   VIP+   \n  2024 Lookback: Who Needs Emmys When the Creator Economy Is Cooking?     \n \n  \n     \n \n  \u2018The Traitors\u2019 Season 3 Adds Two New Players in Shocking Twist \u2014 Plus, Who Were Chosen as the Traitors?        \n \nBut, as Lloyd Webber told Variety in a recent interview, \u201cas she goes through the song, she becomes empowered.\u201d So, after two downbeat verses and choruses and a bridge, the lyrics change to a more upbeat mode for the final chorus. As the kitten finds her community on the streets with some other scrappy cats, the early chorus line \u201cBorn into nothing / At least you have something\u201d turns into \u201c\u2026with them I have something,\u201d and \u201cVisions of dazzling rooms I\u2019ll never get let into\u201d becomes \u201cI never knew I\u2019d love this world they\u2019ve let me into.\u201d \n \n \n Popular on Variety  \n    \n \nBelying the fact that the song was written to be sung by the youngest member of the cast, Swift returns to a more youthful tonality in her voice in her reading of the song, too, with an interpretation that makes her sound more like the ingenue of her \u201cFearless\u201d days than the domineering figure of \u201cReputation.\u201d In essence, we\u2019re hearing the return of the ghost of the younger Swift. \nWe now return you to the regularly scheduled cattiness of the music industry.  \n  \nRead More About: \n \n  Andrew Lloyd-Webber,  \n  Cats,  \n  Taylor Swift     \n  Jump to Comments   \n \n \n  \n  \n \n \n \n \n \n \n  \n       \n \n  \u2018Hollywood Squares\u2019 Premiere Delayed a Week by CBS Due to Los Angeles Fires   \n  \n      \n \n \n \n \n \n  \n       \n \n  \u2018Moana\u2019 Was Key to Disney\u2019s Comeback Year From the Beginning   \n  \n      \n \n \n \n \n \n  \n       \n \n  Sarah Michelle Gellar Is Now Open to Return for a \u2018Buffy the Vampire Slayer\u2019 Reboot: \u2018We Need Those Heroes, I Think, More So Than Ever\u2019   \n  \n      \n \n \n \n \n \n  \n       \n \n  AI Entertainment Studios: How Gen AI Toolsets Are Transforming Production Workflows   \n  \n        \n \n \n \n \n \n       JavaScript is required to load the comments. \nLoading comments... \n   \n  \n \n Most Popular   \n \n \n \n \n \n \n  Matthew Perry Gifted Lisa Kudrow the \u2018Friends\u2019 Cookie Jar in 2004. She Found the Note He Left Inside It 20 Years Later and After His Death: \u2018Timing\u2026    \n \n  \n        \n \n \n \n  Jeff Bridges, Miles Teller, Paris Hilton and More Celebrities Lose Homes in the L.A. Fires    \n \n  \n        \n \n \n \n  Milo Ventimiglia Chokes Up After Seeing Malibu Home Burn Down on Security Cameras, Eerie \u2018This Is Us\u2019 Parallel: \u2018It\u2019s Not Lost on Me, Life Imitating\u2026    \n \n  \n        \n \n \n \n \n \n \n         \n \n \n \n  \u2018Twisted Love\u2019 TV Series, Based on Ana Huang\u2019s Hit BookTok Romance Novels, Set at Netflix in Seven-Figure Deal (EXCLUSIVE)    \n \n  \n        \n \n \n \n  \u2018Wicked\u2019 Director Jon M. Chu Defends \u2018For Good\u2019 Sequel Title: \u2018Who Wants a Movie Called \u201cWicked: Part Two&#8221\u2026    \n \n  \n        \n \n \n \n  In Alleged Leaked Audio, Justin Baldoni Claims He Got \u2018Sent to the Basement\u2019 by Blake Lively at \u2018It Ends With Us\u2019 Premiere: She \u2018Didn't Want Me\u2026    \n \n  \n        \n \n \n \n  James Mangold\u2019s \u2018Star Wars\u2019 Movie Is Set 25,000 Years Before \u2018Phantom Menace\u2019 So That It\u2019s Not \u2018Handcuffed by Lore\u2019: Then \u2018You Can\u2019t Please Anybody\u2026    \n \n  \n        \n \n \n \n  Late Night Hosts Mock Trump\u2019s Bizarre Response to L.A. Fires: \u2018In Trump\u2019s Defense, Words Are Hard\u2019    \n \n  \n        \n \n \n \n  \u2018Miraculous \u2013 Tales of Ladybug & Cat Noir\u2019 Bought by Spanish Broadcaster Atresmedia (EXCLUSIVE)    \n \n  \n        \n \n \n \n  James Woods Breaks Down in Tears on CNN After Losing Home in L.A. Fire: \u2018One Day You\u2019re Swimming in the Pool and the Next Day It\u2019s All Gone\u2026    \n \n  \n            \n \n \n \n       \n \n Must Read   \n \n \n \n \n   Film   \n  ACTORS ON ACTORS | Ariana Grande and Paul Mescal Go Full \u2018Glicked\u2019 as They Dish on Singing and That \u2018Insufferable\u2019 Press Tour    By Andrew Wallenstein  1 month   \n \n  \n         \n \n \n \n   Film   \n  The Best Movies of 2024    \n \n  \n        \n \n \n   TV   \n  The Best TV Shows of 2024    \n \n  \n        \n \n \n   Music   \n  The Best Albums of 2024    \n \n  \n        \n \n \n   Film   \n  How Did Blake Lively\u2019s Lawyers Obtain Bombshell Texts From Justin Baldoni\u2019s PR Team?    \n \n  \n            \n \n Sign Up for Variety Newsletters  \n \n \n Your Email   \n  Sign Up  Sign Up             \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n\n \n \n \n \n       \n \n \n Variety Confidential   \n   \n A Variety and iHeartRadio Podcast        \n \n \n \n More From Our Brands    \n \n \n  \n  Rolling Stone  \n Mel Gibson Reveals Plans to Shoot Sequel to \u2018The Passion Of The Christ\u2019   \n \n \n        \n \n  \n  Robb Report  \n The Datsun 240Z, Influenced by the Jaguar E-Type and Early Porsche 911, Is a Classic All Its Own   \n \n \n        \n \n  \n  Sportico  \n LiAngelo Ball\u2019s \u2018Tweaker\u2019 Song Brings Fresh Family Biz to Masses   \n \n \n        \n \n  \n  SPY  \n The Best Loofahs and Body Scrubbers, According to Dermatologists   \n \n \n        \n \n  \n  TVLine  \n Milo Ventimiglia Visits Site of Family\u2019s Burned-Down Malibu Home, Says This Is Us Parallel Is \u2018Not Lost on Me\u2019   \n \n \n            \n \n \n \n      \n \n \n \n     \n \n \n  \nVariety \n \n  About Us  \n  Newsletter  \n  Variety Events  \n  Luminate - Film & TV  \n  Advertise  \n  Media Kit  \n  Careers  \n  Customer Service     \nLegal \n \n  Terms of Use  \n  Privacy Policy  \n  California Privacy Rights  \n  AdChoices  \n  Your Privacy Choices  \n  Accessibility     \nVariety Magazine \n \n  Subscribe  \n  Variety Digital Login  \n  Group Subscriptions  \n  Variety Archives  \n  Back Issues  \n  Help     \nVIP+ \n \n  Subscribe  \n  Login  \n  Learn More  \n  FAQ     \nConnect \n \n  Instagram  \n  Twitter  \n  YouTube  \n  Facebook  \n  LinkedIn      \n   Variety  \nThe Business of Entertainment  Subscribe Today \n Have a News Tip? Let us know     \n   \n \n \n \n \n \n \n   Variety   \n \n   \n Close the menu    \n \n \n \nExpand the sub menu  Film  \n \n  News  \n  Reviews  \n  Podcasts  \n  Box Office  \n  Events  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  TV  \n \n  News  \n  Reviews  \n  Podcasts  \n  Ratings  \n  Pilot Scorecard  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  What To Watch  \n \n  Lists & Features    \n \n \nExpand the sub menu  Music  \n \n  News  \n  Album Reviews  \n  Concert Reviews  \n  Hitmakers  \n  Music For Screens  \n  Grammys    \n \n \nExpand the sub menu  Docs  \n \n  Lists & Features    \n \n \nExpand the sub menu  Tech  \n \n  Features  \n  Global  \n  VIP+ Tech    \n \n \nExpand the sub menu  Global  \n \n  News  \n  Asia  \n  Markets & Festivals  \n  Features    \n \n \nExpand the sub menu  Awards Circuit  \n \n  News  \n  Film Predictions  \n  TV Predictions  \n  Video  \n  Artisans  \n  Columns  \n  Awards Circuit Podcast    \n \n \nExpand the sub menu  Video  \n \n  Popular  \n  Cover Shoots - Behind the Scenes  \n  Actors on Actors  \n  Power of Women  \n  Events  \n  Artisans    \n \n \nExpand the sub menu  What to Hear  \n \n  Lists & Features    \n \n \nExpand the sub menu  VIP+  \n \n  Daily Commentary  \n  Special Reports  \n  Video    \n \n \nExpand the sub menu  More Coverage  \n \n  Obituaries - submit obituaries to obits@variety.com  \n  Photos  \n  Podcasts  \n  Scene  \n  Voices  \n  Theater    \n \n \nExpand the sub menu  More Variety  \n \n  Live Media Summits  \n  Vscore  \n  Luminate - Film & TV  \n  Variety Archives  \n  Variety500  \n  Industry Executives      \n \n  Have a News Tip?  \n \n  \n  Newsletters    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal    \n \n  \n \nAlerts and Newsletters \n \n \n \n Your Email   \n  Sign Up  Sign Up            \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n \n  \nFollow Us \n \n      \n      \n      \n      \n        \n \n   \n  \n \n  Got a Tip?    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal     \n \n  \n \n  Advertise  \n  About  \n  Contact Us    Variety is a part of Penske Media Corporation. \u00a9 2025 Variety Media, LLC. All Rights Reserved.  \n  Icon Link Plus Icon          \nad    \n                                                                    \n                 ", "title": "Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019", "description": "<strong>Taylor</strong> <strong>Swift</strong> has released her end-titles version of &quot;<strong>Beautiful</strong> <strong>Ghosts</strong>,&quot; the song she <strong>co</strong>-<strong>wrote</strong> <strong>with</strong> <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> for the &quot;Cats&quot; film.", "usage": {"tokens": 2689}, "retrieved_text": ["Plus Icon Film Plus Icon TV Plus Icon What To Watch Plus Icon Music Plus Icon Docs Plus Icon Digital & Gaming Plus Icon Global Plus Icon Awards Circuit Plus Icon Video Plus Icon What To Hear Home Music News Nov 15, 2019 6:40am PT Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019 By Chris Willman Plus Icon Chris Willman Senior Music Writer and Chief Music Critic ChrisWillman Latest Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour 3 days ago Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86 3 days ago \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time 5 days ago See All Not to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. Hers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical.", "             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Nov 15, 2019 6:40am PT    \n Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019  \n  \n \n \n \n \n \n \n \n By  Chris Willman  \n Plus Icon    \n  \n  Chris Willman   \nSenior Music Writer and Chief Music Critic \n  ChrisWillman  \n \nLatest \n  Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour  3 days ago   \n  Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86  3 days ago   \n  \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time  5 days ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nNot to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20.", "Time 5 days ago See All Not to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. Hers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical. It\u2019ll also be heard as a reprise sung by Judi Dench. Swift\u2019s version is not, as rumored by some, a \u201cpop\u201d version; with its lush orchestration and dynamics, it\u2019s very much of a piece with the music that\u2019ll be heard in the body of the film. Although it\u2019s hardly a country song, Swift is indulging in an old country music tradition, as well as a Broadway one: the answer song."]}, {"text": "             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n  \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Oct 24, 2019 2:55pm PT    \n Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019  \n Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. \n \n \n \n \n \n \n \n By  Jon Burlingame  \n Plus Icon    \n  \n  Jon Burlingame   \n  @jonburlingame  \n \nLatest \n  Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist  3 weeks ago   \n  David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop  4 weeks ago   \n  \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE)  1 month ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nWhen Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \n\u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical. \nThey didn\u2019t actually meet up with the intention of writing a new song\u2026 at least, not ostensibly. Swift had gone over to Lloyd Webber\u2019s last December during filming to rehearse her performance of a song she sings in the film, \u201cMacavity the Mystery Cat,\u201d in her supporting role as Bombalurina. But at some point in the afternoon the fact that Lloyd Webber had written the instrumental basis of a new song came up. He played it on the piano, she began improvising lyrics and topline vocal elements, and the song now dubbed \u201cBeautiful Ghosts\u201d was nearly done. \n \n \n \n \n      \n \n Related Stories  \n \n \n \n  \n     \n   VIP+   \n  2024 Lookback: A Watershed Year for Generative AI in Hollywood     \n \n  \n     \n \n  \u2018The Traitors\u2019 Season 3 Adds Two New Players in Shocking Twist \u2014 Plus, Who Were Chosen as the Traitors?        \n \n\u201cTaylor got to the essence of what the piece is about. It\u2019s not an ordinary lyric,\u201d says Lloyd Webber, who adds that director Tom Hooper considered her attempt \u201cabsolutely extraordinary, because it\u2019s as if she really has understood the central message of the piece.\u201d \nIt\u2019s said to appear at least three times in the movie. \u201cBeautiful Ghosts\u201d is initially sung halfway through the film by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the reimagining of the 1981 stage musical. It reappears later in a brief reprise, sung by Judi Dench, as Old Deuteronomy. Finally, Swift herself will be heard performing it under the end titles. \nIs Swift\u2019s closing studio rendition a \u201cpop\u201d version? No \u2014 sources who\u2019ve heard it say her recording is very much in keeping with the spirit of the rest of the musical, not positioned as a hit single. \nNews of the collaboration satisfies ongoing curiosity on two fronts: how Swift, arguably the biggest pop star in the world, might play a greater musical role on a film project in which her character is not necessarily the lead; and whether any fresh material would be added to the nearly 40-year-old score that might make \u201cCats\u201d eligible for best song at the Oscars or Globes. (Lloyd Webber won an Oscar in 1997 for a new song he and Tim Rice wrote for Madonna to sing in \u201cEvita.\u201d) \n \n \n \n \n      \n \nIt doesn\u2019t hurt that the movie has some fresh musical material that will be awards-eligible. But there was a much more compelling reason than that to write a new song for the film: Without one, it would have been a movie musical in which the main character doesn\u2019t sing. The \u201cwhite cat\u201d exists primarily as a ballerina figure on stage. In the movie, boosted into a central role, Victoria certainly needed to express herself in song, or audiences would wonder why the cat had her tongue. \n\u201cWe had to have something for the white cat the sing, to have some kind of reaction to what\u2019s going on around her,\u201d Lloyd Webber says. \u201cWe felt there had to be a major statement for her, something that had the spirit of (T.S.) Eliot but also was specific to the story.\u201d \nIn a way, it\u2019s an answer song to the best known and loved number in \u201cCats,\u201d \u201cMemory.\u201d In the film, as on stage, Grizabella (Jennifer Hudson) will sing that standard about her glamour days being over, Now, the white cat \u2014 abandoned as a kitten \u2014 laments that her own life has barely begun. At least you have memories, Victoria tells Grizabella, in so many words, feeling that that\u2019s more than she has. \nVictoria\u2019s message, Lloyd Webber says, paraphrasing the lyrics, is that \u201cI haven\u2019t had a chance yet to dance with beautiful ghosts.\u201d He adds, \u201cAs she goes through the song, she becomes empowered, and she says, \u2018I will dance someday, and maybe they will be beautiful ghosts to me too.\u2019\u201d \n\u201cIt\u2019s a very sophisticated lyric,\u201d Lloyd Webber says, \u201cand it\u2019s entirely written for the dramatic situation. Tom Hooper said to me, \u2018It\u2019s as if she\u2019s been reading her T.S. Eliot without us knowing.'\u201d \n\u201cBeautiful Ghosts\u201d will be the only song in the score not drawn directly from Eliot\u2019s \u201cOld Possum\u2019s Book of Practical Cats,\u201d whose verse the composer set to music for the original 1981 show. In the case of \u201cGhosts,\u201d the composer and Swift share credit for both music and lyrics. \nLloyd Webber stands a chance at his first Oscar since winning for \u201cEvita\u2019s\u201d \u201cYou Must Love Me\u201d if \u201cGhosts\u201d comes to life with voters; it could well leap ahead in the pack in a year in which there have not yet been any clear \u201cShallow\u201d-style favorites. Swift has never had an Oscar nomination, but she has been a Golden Globes or Grammy nominee all three times she\u2019s written or co-written a song for a film. The star\u2019s Globe-nominated songs were \u201cSafe and Sound\u201d for 2013\u2019s \u201cThe Hunger Games\u201d and \u201cSweeter Than Fiction\u201d for 2014\u2019s \u201cOne Chance.\u201d At the 2018 Grammys, Swift was nominated for best song written for visual media for \u201cI Don\u2019t Wanna Live Forever,\u201d from \u201cFifty Shades Darker.\u201d \n \n \n \n \n      \n \nLloyd Webber couldn\u2019t help noticing one point of comparison with his new songwriting partner: \u201cShe\u2019s the same age as I was when I started writing \u2018Cats,'\u201d he points out \u2014 age 29. \nAsked if he and his fellow cat fancier spent any time talking about favorite felines, Lloyd Webber says, \u201cYes, we did talk about cats. The only thing is, at the advanced age of 71, I have actually got a puppy. So whether Taylor\u2019s ever going to speak to me again, I don\u2019t know.\u201d \nThe song\u2019s commercial release date has yet to be determined, although brief snippets can be heard in EPK footage released by the studio (watch it above).  \n  \nRead More About: \n \n  Andrew Lloyd-Webber,  \n  Cats,  \n  Taylor Swift     \n  Jump to Comments   \n \n \n  \n  \n \n \n \n \n \n \n  \n       \n \n  \u2018Hollywood Squares\u2019 Premiere Delayed a Week by CBS Due to Los Angeles Fires   \n  \n      \n \n \n \n \n \n  \n       \n \n  2024 Lookback: Who Needs Emmys When the Creator Economy Is Cooking?   \n  \n      \n \n \n \n \n \n  \n       \n \n  Sarah Michelle Gellar Is Now Open to Return for a \u2018Buffy the Vampire Slayer\u2019 Reboot: \u2018We Need Those Heroes, I Think, More So Than Ever\u2019   \n  \n      \n \n \n \n \n \n  \n       \n \n  Taylor Swift Leads New \u2018Era\u2019 for Stadium Tours: Less Rock, More Pop   \n  \n        \n \n \n \n \n \n       JavaScript is required to load the comments. \nLoading comments... \n   \n  \n \n Most Popular   \n \n \n \n \n \n \n  Matthew Perry Gifted Lisa Kudrow the \u2018Friends\u2019 Cookie Jar in 2004. She Found the Note He Left Inside It 20 Years Later and After His Death: \u2018Timing\u2026    \n \n  \n        \n \n \n \n  Jeff Bridges, Miles Teller, Paris Hilton and More Celebrities Lose Homes in the L.A. Fires    \n \n  \n        \n \n \n \n  Milo Ventimiglia Chokes Up After Seeing Malibu Home Burn Down on Security Cameras, Eerie \u2018This Is Us\u2019 Parallel: \u2018It\u2019s Not Lost on Me, Life Imitating\u2026    \n \n  \n        \n \n \n \n \n \n \n         \n \n \n \n  \u2018Twisted Love\u2019 TV Series, Based on Ana Huang\u2019s Hit BookTok Romance Novels, Set at Netflix in Seven-Figure Deal (EXCLUSIVE)    \n \n  \n        \n \n \n \n  \u2018Wicked\u2019 Director Jon M. Chu Defends \u2018For Good\u2019 Sequel Title: \u2018Who Wants a Movie Called \u201cWicked: Part Two&#8221\u2026    \n \n  \n        \n \n \n \n  In Alleged Leaked Audio, Justin Baldoni Claims He Got \u2018Sent to the Basement\u2019 by Blake Lively at \u2018It Ends With Us\u2019 Premiere: She \u2018Didn't Want Me\u2026    \n \n  \n        \n \n \n \n  James Mangold\u2019s \u2018Star Wars\u2019 Movie Is Set 25,000 Years Before \u2018Phantom Menace\u2019 So That It\u2019s Not \u2018Handcuffed by Lore\u2019: Then \u2018You Can\u2019t Please Anybody\u2026    \n \n  \n        \n \n \n \n  Late Night Hosts Mock Trump\u2019s Bizarre Response to L.A. Fires: \u2018In Trump\u2019s Defense, Words Are Hard\u2019    \n \n  \n        \n \n \n \n  \u2018Miraculous \u2013 Tales of Ladybug & Cat Noir\u2019 Bought by Spanish Broadcaster Atresmedia (EXCLUSIVE)    \n \n  \n        \n \n \n \n  James Woods Breaks Down in Tears on CNN After Losing Home in L.A. Fire: \u2018One Day You\u2019re Swimming in the Pool and the Next Day It\u2019s All Gone\u2026    \n \n  \n            \n \n \n \n       \n \n Must Read   \n \n \n \n \n   Film   \n  ACTORS ON ACTORS | Ariana Grande and Paul Mescal Go Full \u2018Glicked\u2019 as They Dish on Singing and That \u2018Insufferable\u2019 Press Tour    By Andrew Wallenstein  1 month   \n \n  \n         \n \n \n \n   Film   \n  The Best Movies of 2024    \n \n  \n        \n \n \n   TV   \n  The Best TV Shows of 2024    \n \n  \n        \n \n \n   Music   \n  The Best Albums of 2024    \n \n  \n        \n \n \n   Film   \n  How Did Blake Lively\u2019s Lawyers Obtain Bombshell Texts From Justin Baldoni\u2019s PR Team?    \n \n  \n            \n \n Sign Up for Variety Newsletters  \n \n \n Your Email   \n  Sign Up  Sign Up             \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n\n \n \n \n \n       \n \n \n Variety Confidential   \n   \n A Variety and iHeartRadio Podcast        \n \n \n \n More From Our Brands    \n \n \n  \n  Rolling Stone  \n Mel Gibson Reveals Plans to Shoot Sequel to \u2018The Passion Of The Christ\u2019   \n \n \n        \n \n  \n  Robb Report  \n The Datsun 240Z, Influenced by the Jaguar E-Type and Early Porsche 911, Is a Classic All Its Own   \n \n \n        \n \n  \n  Sportico  \n LiAngelo Ball\u2019s \u2018Tweaker\u2019 Song Brings Fresh Family Biz to Masses   \n \n \n        \n \n  \n  SPY  \n The Best Loofahs and Body Scrubbers, According to Dermatologists   \n \n \n        \n \n  \n  TVLine  \n Milo Ventimiglia Visits Site of Family\u2019s Burned-Down Malibu Home, Says This Is Us Parallel Is \u2018Not Lost on Me\u2019   \n \n \n            \n \n \n \n      \n \n \n \n     \n \n \n  \nVariety \n \n  About Us  \n  Newsletter  \n  Variety Events  \n  Luminate - Film & TV  \n  Advertise  \n  Media Kit  \n  Careers  \n  Customer Service     \nLegal \n \n  Terms of Use  \n  Privacy Policy  \n  California Privacy Rights  \n  AdChoices  \n  Your Privacy Choices  \n  Accessibility     \nVariety Magazine \n \n  Subscribe  \n  Variety Digital Login  \n  Group Subscriptions  \n  Variety Archives  \n  Back Issues  \n  Help     \nVIP+ \n \n  Subscribe  \n  Login  \n  Learn More  \n  FAQ     \nConnect \n \n  Instagram  \n  Twitter  \n  YouTube  \n  Facebook  \n  LinkedIn      \n   Variety  \nThe Business of Entertainment  Subscribe Today \n Have a News Tip? Let us know     \n   \n \n \n \n \n \n \n   Variety   \n \n   \n Close the menu    \n \n \n \nExpand the sub menu  Film  \n \n  News  \n  Reviews  \n  Podcasts  \n  Box Office  \n  Events  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  TV  \n \n  News  \n  Reviews  \n  Podcasts  \n  Ratings  \n  Pilot Scorecard  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  What To Watch  \n \n  Lists & Features    \n \n \nExpand the sub menu  Music  \n \n  News  \n  Album Reviews  \n  Concert Reviews  \n  Hitmakers  \n  Music For Screens  \n  Grammys    \n \n \nExpand the sub menu  Docs  \n \n  Lists & Features    \n \n \nExpand the sub menu  Tech  \n \n  Features  \n  Global  \n  VIP+ Tech    \n \n \nExpand the sub menu  Global  \n \n  News  \n  Asia  \n  Markets & Festivals  \n  Features    \n \n \nExpand the sub menu  Awards Circuit  \n \n  News  \n  Film Predictions  \n  TV Predictions  \n  Video  \n  Artisans  \n  Columns  \n  Awards Circuit Podcast    \n \n \nExpand the sub menu  Video  \n \n  Popular  \n  Cover Shoots - Behind the Scenes  \n  Actors on Actors  \n  Power of Women  \n  Events  \n  Artisans    \n \n \nExpand the sub menu  What to Hear  \n \n  Lists & Features    \n \n \nExpand the sub menu  VIP+  \n \n  Daily Commentary  \n  Special Reports  \n  Video    \n \n \nExpand the sub menu  More Coverage  \n \n  Obituaries - submit obituaries to obits@variety.com  \n  Photos  \n  Podcasts  \n  Scene  \n  Voices  \n  Theater    \n \n \nExpand the sub menu  More Variety  \n \n  Live Media Summits  \n  Vscore  \n  Luminate - Film & TV  \n  Variety Archives  \n  Variety500  \n  Industry Executives      \n \n  Have a News Tip?  \n \n  \n  Newsletters    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal    \n \n  \n \nAlerts and Newsletters \n \n \n \n Your Email   \n  Sign Up  Sign Up            \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n \n  \nFollow Us \n \n      \n      \n      \n      \n        \n \n   \n  \n \n  Got a Tip?    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal     \n \n  \n \n  Advertise  \n  About  \n  Contact Us    Variety is a part of Penske Media Corporation. \u00a9 2025 Variety Media, LLC. All Rights Reserved.  \n  Icon Link Plus Icon          \nad    \n                                                                    \n                 ", "title": "Taylor Swift, Andrew Lloyd Webber Have Co-Written a New 'Cats' Song", "description": "<strong>Taylor</strong> <strong>Swift</strong> <strong>and</strong> <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> have revealed they collaborated on writing &quot;<strong>Beautiful</strong> <strong>Ghosts</strong>,&quot; a new song for the film version of &quot;Cats.&quot;", "usage": {"tokens": 3445}, "retrieved_text": ["             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n  \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Oct 24, 2019 2:55pm PT    \n Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019  \n Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. By  Jon Burlingame  \n Plus Icon    \n  \n  Jon Burlingame   \n  @jonburlingame  \n \nLatest \n  Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist  3 weeks ago   \n  David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop  4 weeks ago   \n  \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE)  1 month ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nWhen Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it.", "Plus Icon Film Plus Icon TV Plus Icon What To Watch Plus Icon Music Plus Icon Docs Plus Icon Digital & Gaming Plus Icon Global Plus Icon Awards Circuit Plus Icon Video Plus Icon What To Hear Home Music News Oct 24, 2019 2:55pm PT Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019 Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. By Jon Burlingame Plus Icon Jon Burlingame @jonburlingame Latest Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist 3 weeks ago David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop 4 weeks ago \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE) 1 month ago See All When Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical.", "Betty Boop 4 weeks ago \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE) 1 month ago See All When Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical. They didn\u2019t actually meet up with the intention of writing a new song\u2026 at least, not ostensibly. Swift had gone over to Lloyd Webber\u2019s last December during filming to rehearse her performance of a song she sings in the film, \u201cMacavity the Mystery Cat,\u201d in her supporting role as Bombalurina. But at some point in the afternoon the fact that Lloyd Webber had written the instrumental basis of a new song came up. He played it on the piano, she began improvising lyrics and topline vocal elements, and the song now dubbed \u201cBeautiful Ghosts\u201d was nearly done."]}]}, "retrieved_search_results": {"Taylor Swift co-wrote \"Beautiful Ghosts\" with Andrew Lloyd Webber.": [{"text": "Jump to content \n   \n \n \n \n\n  \n \n \n  \n     \n  \n     \n   \n \n \n  \n \n \n \nArticle\nTalk    \n  English  \n \n \n \n        \n  \n \n \n \nRead\nEdit\nView history      \n  Tools  \n \n \n \n \nTools \nmove to sidebar \nhide  \n \n Actions  \n \n \nRead\nEdit\nView history    \n \n General  \n \n \nWhat links here\nRelated changes\nUpload file\nSpecial pages\nPermanent link\nPage information\nCite this page\nGet shortened URL\nDownload QR code \nEdit interlanguage links\nExpand all   \n \n Print/export  \n \n \nDownload as PDF\nPrintable version    \n \n In other projects  \n \n \nWikidata item            \n \n  \n    \n     \n \n \n  \nFrom Wikipedia, the free encyclopedia  \n\n \n\n\n2019 promotional single by Taylor Swift \n  \"Beautiful Ghosts\"Promotional single by Taylor Swiftfrom the album  Cats: Highlights from the Motion Picture Soundtrack ReleasedNovember 15, 2019 (2019-11-15)Recorded2019GenreOrchestralLength4:21LabelPolydorComposer(s)\n\n\nTaylor Swift\nAndrew Lloyd WebberLyricist(s)Taylor SwiftProducer(s)\n\n\nGreg Wells\nTom Hooper\nAndrew Lloyd WebberLyric video\"Beautiful Ghosts\" on YouTube  \n\"Beautiful Ghosts\" (also subtitled \"(From the Motion Picture Cats)\") is a song by the American singer-songwriter Taylor Swift and the English composer Andrew Lloyd Webber from the 2019 Cats film adaptation, in which Swift played Bombalurina. The song was written by Swift (lyrics and music) and Cats creator Andrew Lloyd Webber (music), and produced by Greg Wells, Lloyd Webber, and Cats director Tom Hooper. It was released on November 15, 2019. Considered as its signature song, it is performed in the film by the principal character Victoria, portrayed by Francesca Hayward as its signature song. A 30-second reprise is also sung by Judi Dench as Old Deuteronomy.[1] Swift performs the official single version that is played over the ending credits.[2] \nUpon release, the song received positive reviews from music critics. It received nominations for Best Original Song at the 77th Golden Globe Awards and Best Song Written for Visual Media at the 63rd Annual Grammy Awards\u2014Swift's third nomination for the latter, following \"Safe & Sound\" (2011) and \"I Don't Wanna Live Forever\" (2016).   \n\nBackground and release[edit] \nAs a concept, the then-untitled song was first reported in early 2018, when the film adaptation was in early stages of production.[3] The song was designed to provide a way for Victoria to introduce her character to the audience, as she communicates solely through gestures and dance in the original stage show.[2] Swift described the song as such:  \n\n\"Beautiful Ghosts\" is sung from a young voice who is wondering if she will ever have glory days. Longing for the sense of belonging she sees everyone finding. Reaching for it, desperately afraid of never having beautiful ghosts of days gone by to cling to in her older years.[4]\nThe song was released on digital platforms and to streaming services on November 15, 2019, one month before the film's theatrical release.[5] A lyric video for the song was released on YouTube the same day. The lyric video has over 6 million views on YouTube.[6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats.[7] \n\nComposition[edit] \nThe song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6\n8 and 9\n8 time signatures throughout most of the song, with the ending in compound septuple meter 21\n8.[8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber.[2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics.[1][4] According to Lloyd Webber, the two \"wrote 90 percent of it pretty much over an afternoon.\"[1] He described the collaboration with Swift as \"a joy\", the collaboration as \"one of the finest of [his] 50-year career\",[9] Swift's lyrics as \"brilliant\", and her performance of the song as \"emotional\" and \"among the very best [he's] ever had\". [10] Tom Hooper, director of the film and a producer of the song, complimented the \"extraordinary beauty\" of Swift's lyrics and praised Swift for her \"profound understanding of what we're trying to do with the movie.\"[11]  \n\nCritical reception[edit] \nIn contrast to the negative reception Cats received, the song received positive reviews from music critics, who praised Swift's vocal performance. Darlene Aderoju and Joelle Goldstein of People described the song as \"chillingly-beautiful\" and a \"haunting melody\" that \"speaks to a feeling of longing to be wanted and reminiscing on better memories.\" They also praised Swift's singing, describing it as \"impressive pipes beyond her typical country-pop style\", especially the final belted note.[11] Writing for HuffPost, Ron Dicker complimented Swift's vocals, referring to the song as \"hauntingly gorgeous\" and an \"absolute showstopper\".[12] Elite Daily's Jessica Bola\u00f1os opined that the song is \"nothing short of perfection\" and stated that the lyrics are \"dripping with passion\".[13][14] Chris Willman of Variety wrote that the song is a return to the \"youthful tonality\" in Swift's voice, typical of \"her Fearless days\".[15] Brittany Spanos and Ryan Reeds of Rolling Stone described the song as \"cinematic\" and \"wistful\". They also opined that the song \"builds to a rousing climax with the singer's voice in a full roar\".[16] MTV's Madeline Roth praised Swift's vocal performance and labeled the final high note of the song as \"more powerful than anything we've heard from T. Swift in recent memory\".[17] Carolyn Droke of Uproxx wrote that the song is about \"the melancholy feeling that comes with reminiscing on old memories\" and appreciated Swift for showing off her vocal range.[18] \nOthers gave the song less favourable reviews. Adam Feldman wrote for Time Out that \"Beautiful Ghosts\" is \"terrible\", saying that \"Swift's lyrics clank with banality at nearly every turn of phrase\".[19] Johnny Oleksinski of the New York Post agreed, calling the song \"a boring 'Memory' copycat\".[20] Leah Marilla Thomas of Cosmopolitan praised the final chorus, but described the song as \"a little bonkers\" and said that listeners need to lower their expectations before listening.[21] Various Vulture staff also criticized the song, describing the lyrics as nonsensical and corny; Swift's vocal performance was criticised, with Rebecca Alter summarizing the song as Swift \"leaping for that note and falling off a cliff\".[22] In June 2022, Insider ranked \"Beautiful Ghosts\" as Swift's worst soundtrack song.[23]  \n\nAccolades[edit] \nThe song was nominated for Best Original Song at the 77th Golden Globe Awards, becoming Swift's third nomination in the category, following \"Safe & Sound\" (2013) and \"Sweeter than Fiction\" (2014).[24] At the 63rd Annual Grammy Awards, \"Beautiful Ghosts\" is nominated for Best Song Written for Visual Media, marking Swift's third nomination in the category, following \"Safe & Sound\" (2011) and \"I Don't Wanna Live Forever\" (2016).    Year Organization Award Result Ref.   \n2020  Golden Globe Awards  Best Original Song  Nominated  [25]   Huading Awards  Best Global Film Theme Song  Nominated  [26]   \n2021  Grammy Awards  Best Song Written for Visual Media  Nominated  [27]   \n\nCredits and personnel[edit] \nCredits adapted from Tidal.[28]  \n \n\nTaylor Swift \u2013 vocals, songwriter \nAndrew Lloyd Webber \u2013 producer, songwriter \nGreg Wells \u2013 producer \nTom Hooper \u2013 producer \nGus Pirelli \u2013 engineer, studio personnel \nRyan Smith \u2013 mastering engineer, studio personnel \nSerban Ghenea \u2013 mixer, studio personnel \nJohn Ashton Thomas \u2013 conductor \nEverton Nelson \u2013 orchestra leader \nSusie Gillis \u2013 orchestra contractor \nEliza Marshall, Helen Keen \u2013 alto flute, flute \nAnthony Pike \u2013 bass clarinet \nJodi Milliner \u2013 bass guitar \nAndy Wood \u2013 bass trombone \nAdrian Bradbury, Caroline Dearnley, Chris Worsey, Frank Schaefer, Ian Burdge, Paul Kegg \u2013 cello \nJohn Carnac \u2013 clarinet \nJane Marshall \u2013 cor anglais \nAllen Walley, Mary Scully, Richard Pryce, Steve Mair \u2013 double bass \nAlexei Watkins, Martin Owen, Richard Watkins, Simon Rayner \u2013 French horn \nSkaila Kanga \u2013 harp \nDavid Thomas \u2013 oboe \nChris Baron, Frank Ricotti \u2013 percussion \nJohn Ashton Thomas, Peter Murray \u2013 piano \nBill Lockhart \u2013 timpani \nEd Tarrant, Mark Nightingale \u2013 trombone \nDan Newell, Kate Moore, Pat White \u2013 trumpet \nOwen Slade \u2013 tuba \nAndy Parker, Gillianne Haddow, Gustav Clarkson, Jake Walker, Martin Humbey, Peter Lale, Polly Wiltshire, Sue Dench \u2013 viola \nDai Emanuel, Dave Williams, Debbie Preece, Debbie Widdup, Emil Chakalov, Everton Nelson, Ian Humphries, Jonathan Evans-Jones, Julian Leaper, Kate Robinson, Laura Melhuish, Lorraine McAslan, Mark Berrow, Martyn Jackson, Miranda Dale, Natalia Bonner, Odile Ollagnon, Patrick Kiernan, Paul Willey, Ralph de Souza, Richard George, Rick Koster \u2013 violin  \n\nCharts[edit]  \nChart performance for \"Beautiful Ghosts\"   Chart (2019)  Peak\nposition   Australia Digital Tracks (ARIA)[29]  22   Canada Digital Song Sales (Billboard)[30]  31   Greece Digital Songs (Billboard)[31]  5   Hungary (Single Top 40)[32]  26   New Zealand Hot Singles (RMNZ)[33]  36   Scotland (OCC)[34]  44   UK Singles Downloads (OCC)[35]  32   US Digital Song Sales (Billboard)[36]  14  \n\nRelease history[edit]  \nRelease history and formats for \"Beautiful Ghosts\"   Country  Date  Format  Label  Ref.   Various  November 15, 2019  \n\n\nDigital download\nstreaming  Polydor  [37]  \n\nReferences[edit] \n \n\n \n^ Jump up to: a b c Burlingame, Jon (October 24, 2019). \"Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for 'Cats'\". Variety. Archived from the original on October 25, 2019. Retrieved October 25, 2019.  \n^ Jump up to: a b c Vlessing, Etan (October 24, 2019). \"Andrew Lloyd Webber Talks Working With Taylor Swift on New 'Cats' Song\". The Hollywood Reporter. Archived from the original on October 25, 2019. Retrieved October 24, 2019.  \n^ Gans, Andrew (January 5, 2018). \"Andrew Lloyd Webber Pens New Song for Possible Cats Film\". Playbill. Archived from the original on October 25, 2019. Retrieved October 24, 2019.  \n^ Jump up to: a b Swift, Taylor (November 15, 2019). \"The Importance of 'Cats,' in Taylor Swift's Own Words\". Billboard. Archived from the original on November 16, 2019. Retrieved November 16, 2019.  \n^ Aniftos, Rania (November 15, 2019). \"Taylor Swift Releases 'Beautiful Ghosts,' Co-Written With Andrew Lloyd Webber for 'Cats' Film\". Billboard. Archived from the original on November 19, 2019. Retrieved November 15, 2019.  \n^ Swift, Taylor (November 15, 2019), \"Taylor Swift - Beautiful Ghosts (From The Motion Picture \"Cats\" / Lyric Video)\", YouTube, archived from the original on November 29, 2019, retrieved November 30, 2019  \n^ Rubin, Rebecca (October 12, 2021). \"Andrew Lloyd Webber Says Writing a Song With Taylor Swift Was the Only Enjoyable Part of 'Cats' Movie\". Variety. Archived from the original on October 13, 2021. Retrieved October 14, 2021.  \n^ \"Taylor Swift \"Beautiful Ghosts\"\". www.musicnotes.com. November 15, 2019. Archived from the original on April 20, 2021. Retrieved November 18, 2019.  \n^ Lloyd Webber, Andrew [@OfficialALW] (November 18, 2019). \"Thrilled at the reaction to \"Beautiful Ghosts\". This collaboration with @taylorswift13 has been a joy, one of the finest of my 50-year career\" (Tweet). Retrieved November 23, 2019 \u2013 via Twitter.  \n^ Lloyd Webber, Andrew [@OfficialALW] (December 1, 2019). \"Taylor's lyric for Beautiful Ghosts is brilliant and her emotional performance of our song must be among the very best I've ever had\" (Tweet). Retrieved December 2, 2019 \u2013 via Twitter.  \n^ Jump up to: a b Aderoju, Darlene; Goldstein, Joelle (November 15, 2019). \"Taylor Swift Shows Off Her Impressive Vocals in New Song 'Beautiful Ghosts' from Cats Musical Film\". People. Archived from the original on November 15, 2019. Retrieved November 15, 2019.  \n^ Dicker, Ron (November 15, 2019). \"Taylor Swift's New Song For 'Cats' Is An Absolute Showstopper\". HuffPost. Archived from the original on November 16, 2019. Retrieved November 15, 2019.  \n^ Bola\u00f1os, Jessica (November 15, 2019). \"Taylor Swift's \"Beautiful Ghosts\" Lyrics Will Make You So Emotional\". Elite Daily. Archived from the original on November 15, 2019. Retrieved November 15, 2019.  \n^ Bola\u00f1os, Jessica (November 15, 2019). \"Taylor Swift's \"Beautiful Ghosts\" From 'Cats' Shows Off Her English Accent\". Elite Daily. Archived from the original on November 15, 2019. Retrieved November 15, 2019.  \n^ Willman, Chris (November 15, 2019). \"Taylor Swift Releases 'Beautiful Ghosts,' Her Andrew Lloyd Webber Collaboration for 'Cats'\". Variety. Archived from the original on November 15, 2019. Retrieved November 16, 2019.  \n^ Spanos, Brittany; Reeds, Ryan (November 15, 2019). \"Hear Taylor Swift's New 'Cats' Soundtrack Song 'Beautiful Ghosts'\". Rolling Stone. Archived from the original on November 15, 2019. Retrieved November 16, 2019.  \n^ Roth, Madeline (November 15, 2019). \"Taylor Swift Drops Her Pitch-Purrfect Cats Ballad 'Beautiful Ghosts'\". MTV. Archived from the original on November 16, 2019. Retrieved November 16, 2019.  \n^ Droke, Carolyn (November 19, 2019). \"All The Best New Pop Music From This Week\". Uproxx. Archived from the original on November 20, 2019. Retrieved November 20, 2019.  \n^ Feldman, Adam (November 15, 2019). \"The Taylor Swift song from Cats has dropped and it's terrible\". Time Out New York. Archived from the original on December 9, 2019. Retrieved December 10, 2019.  \n^ Oleksinski, Johnny (November 15, 2019). \"Taylor Swift's 'Beautiful Ghosts' is a boring 'Memory' copycat\". New York Post. Archived from the original on December 9, 2019. Retrieved December 10, 2019.  \n^ Thomas, Leah Marilla (November 18, 2019). \"Taylor Swift's 'Cats' Song Is Here and There's Sooo Much to Unpack\". Cosmopolitan. Archived from the original on December 9, 2019. Retrieved December 10, 2019.  \n^ \"Let Us Gather to Discuss Taylor Swift and Andrew Lloyd Webber's Cats Song\". Vulture. December 9, 2019. Archived from the original on December 12, 2019. Retrieved December 11, 2019.  \n^ Ahlgrim, Callie; Larocca, Courteney (June 26, 2022). \"Taylor Swift's 8 soundtrack songs, ranked from worst to best\". Insider. Archived from the original on February 7, 2023. Retrieved June 26, 2022.  \n^ Hipes, Patrick (December 9, 2019). \"Golden Globes Nominations\". Deadline. Archived from the original on December 11, 2019. Retrieved December 10, 2019.  \n^ Babiana, Ana Maria (December 9, 2019). \"Nominations for the 77th Golden Globes Have Been Announced\". Hollywood Foreign Press Association. Archived from the original on December 11, 2019. Retrieved December 10, 2019.  \n^ \"Nominations Announced for the 28th Huading Awards -- \"The Global Film Satisfaction Survey Release Ceremony\"\". Tyler Morning Telegraph. October 23, 2020. Archived from the original on November 15, 2020. Retrieved November 14, 2020.  \n^ \"2021 GRAMMYs: Complete Nominees List\". GRAMMY.com. November 24, 2020. Archived from the original on November 28, 2020. Retrieved November 24, 2020.  \n^ \"\"Beautiful Ghosts\" (From The Motion Picture Cats) by Taylor Swift\". Tidal. Archived from the original on November 16, 2019. Retrieved November 16, 2019.  \n^ \"ARIA Australian Top 40 Digital Tracks\" (PDF). Australian Recording Industry Association. November 25, 2019. Archived (PDF) from the original on November 23, 2019. Retrieved November 23, 2019.  \n^ \"Taylor Swift Chart History (Hot Canadian Digital Song Sales)\". Billboard. Archived from the original on December 13, 2020. Retrieved December 11, 2019.  \n^ \"Greece Digital Song Sales\". Billboard. Archived from the original on December 6, 2019. Retrieved December 6, 2019.  \n^ \"Arch\u00edvum \u2013 Sl\u00e1gerlist\u00e1k \u2013 MAHASZ\" (in Hungarian). Single (track) Top 40 lista. Magyar Hanglemezkiad\u00f3k Sz\u00f6vets\u00e9ge. Retrieved November 29, 2019.  \n^ \"NZ Hot Singles Chart\". Recorded Music NZ. November 25, 2019. Archived from the original on November 22, 2019. Retrieved November 22, 2019.  \n^ \"Official Scottish Singles Sales Chart Top 100\". Official Charts Company. Retrieved November 23, 2019.  \n^ \"Official Singles Downloads Chart Top 100\". Official Charts Company. Retrieved December 11, 2019.  \n^ \"Taylor Swift Chart History (Digital Song Sales)\". Billboard. Retrieved December 5, 2019.  \n^ \"Beautiful Ghosts (From the Motion Picture \"Cats\") - Single by Taylor Swift on Apple Music\". Apple Music. Archived from the original on November 15, 2019. Retrieved November 15, 2019.   \n\n\nshow\n\n\nv\nt\ne\nTaylor Swift songs\n \n\nSingles discography Taylor Swift\n \n\n\"Tim McGraw\" \n\"Picture to Burn\" \n\"Teardrops on My Guitar\" \n\"Should've Said No\" \n\"Our Song\" Fearless\n \n\n\"Fearless\" \n\"Fifteen\" \n\"Love Story\" \n\"Hey Stephen\" \n\"White Horse\" \n\"You Belong with Me\" \n\"Breathe\" \n\"You're Not Sorry\" \n\"The Way I Loved You\" \n\"Forever & Always\" \n\"The Best Day\" \n\"Change\" Taylor's Version\n \n\n\"You All Over Me\" \n\"Mr. Perfectly Fine\" \n\"That's When\" \n Speak Now\n \n\n\"Mine\" \n\"Sparks Fly\" \n\"Back to December\" \n\"Speak Now\" \n\"Dear John\" \n\"Mean\" \n\"The Story of Us\" \n\"Never Grow Up\" \n\"Enchanted\" \n\"Better than Revenge\" \n\"Innocent\" \n\"Haunted\" \n\"Long Live\" \n\"Ours\" Taylor's Version\n \n\n\"Electric Touch\" \n\"When Emma Falls in Love\" \n\"I Can See You\" \n\"Castles Crumbling\" \n Red\n \n\n\"State of Grace\" \n\"Red\" \n\"Treacherous\" \n\"I Knew You Were Trouble\" \n\"All Too Well\" \n\"22\" \n\"We Are Never Ever Getting Back Together\" \n\"The Last Time\" \n\"Holy Ground\" \n\"Everything Has Changed\" \n\"Begin Again\" Taylor's Version\n \n\n\"Nothing New\" \n\"Message in a Bottle\" \n\"I Bet You Think About Me\" \n\"Forever Winter\" \n 1989\n \n\n\"Welcome to New York\" \n\"Blank Space\" \n\"Style\" \n\"Out of the Woods\" \n\"All You Had to Do Was Stay\" \n\"Shake It Off\" \n\"I Wish You Would\" \n\"Bad Blood\" \n\"Wildest Dreams\" \n\"How You Get the Girl\" \n\"This Love\" \n\"I Know Places\" \n\"Clean\" \n\"You Are in Love\" \n\"New Romantics\" Taylor's Version\n \n\n\"'Slut!'\" \n\"Say Don't Go\" \n\"Now That We Don't Talk\" \n\"Suburban Legends\" \n\"Is It Over Now?\" \n Reputation\n \n\n\"...Ready for It?\" \n\"End Game\" \n\"I Did Something Bad\" \n\"Don't Blame Me\" \n\"Delicate\" \n\"Look What You Made Me Do\" \n\"So It Goes...\" \n\"Gorgeous\" \n\"Getaway Car\" \n\"Dress\" \n\"Call It What You Want\" \n\"New Year's Day\" Lover\n \n\n\"I Forgot That You Existed\" \n\"Cruel Summer\" \n\"Lover\" \n\"The Man\" \n\"The Archer\" \n\"Miss Americana & the Heartbreak Prince\" \n\"Paper Rings\" \n\"Cornelia Street\" \n\"Death by a Thousand Cuts\" \n\"London Boy\" \n\"Soon You'll Get Better\" \n\"False God\" \n\"You Need to Calm Down\" \n\"Me!\" Folklore\n \n\n\"The 1\" \n\"Cardigan\" \n\"The Last Great American Dynasty\" \n\"Exile\" \n\"My Tears Ricochet\" \n\"Mirrorball\" \n\"Seven\" \n\"August\" \n\"This Is Me Trying\" \n\"Illicit Affairs\" \n\"Invisible String\" \n\"Mad Woman\" \n\"Epiphany\" \n\"Betty\" \n\"Peace\" \n\"Hoax\" \n\"The Lakes\" Evermore\n \n\n\"Willow\" \n\"Champagne Problems\" \n\"'Tis the Damn Season\" \n\"Tolerate It\" \n\"No Body, No Crime\" \n\"Happiness\" \n\"Dorothea\" \n\"Coney Island\" \n\"Long Story Short\" \n\"Marjorie\" Midnights\n \n\n\"Lavender Haze\" \n\"Maroon\" \n\"Anti-Hero\" \n\"Snow on the Beach\" \n\"You're on Your Own, Kid\" \n\"Midnight Rain\" \n\"Question...?\" \n\"Vigilante Shit\" \n\"Bejeweled\" \n\"Labyrinth\" \n\"Karma\" \n\"Sweet Nothing\" \n\"Mastermind\" \n\"Hits Different\" \n\"Bigger Than the Whole Sky\" \n\"Would've, Could've, Should've\" \n\"You're Losing Me\" The Tortured Poets\nDepartment\n \n\n\"Fortnight\" \n\"The Tortured Poets Department \n\"My Boy Only Breaks His Favorite Toys\" \n\"Down Bad\" \n\"So Long, London\" \n\"But Daddy I Love Him\" \n\"Fresh Out the Slammer\" \n\"Florida!!!\" \n\"Guilty as Sin?\" \n\"Who's Afraid of Little Old Me?\" \n\"Loml\" \n\"I Can Do It with a Broken Heart\" \n\"The Smallest Man Who Ever Lived\" \n\"The Alchemy\" \n\"Clara Bow\" \n\"The Black Dog\" \n\"So High School\" \n\"Thank You Aimee\" Soundtrack songs\n \n\n\"Crazier\" \n\"Today Was a Fairytale\" \n\"Safe & Sound\" \n\"Eyes Open\" \n\"Sweeter than Fiction\" \n\"I Don't Wanna Live Forever\" \n\"Beautiful Ghosts\" \n\"Only the Young\" \n\"Carolina\" Featured songs\n \n\n\"Two Is Better Than One\" \n\"Half of My Heart\" \n\"Both of Us\" \n\"Babe\" \n\"Gasoline\" \n\"Renegade\" \n\"The Joker and the Queen\" \n\"The Alcott\" \n\"Us\" Other songs\n \n\n\"Best Days of Your Life\" \n\"Ronan\" \n\"Highway Don't Care\" \n\"Better Man\" \n\"Christmas Tree Farm\" \n\"All of the Girls You Loved Before\" \n \n\n Category  \n\n\nshow\n\n\nv\nt\ne\nAndrew Lloyd Webber\nAccoladesMusicals\n \n\nJesus Christ Superstar \nJoseph and the Amazing Technicolor Dreamcoat \nEvita \nTell Me on a Sunday \nCats \nSong and Dance \nStarlight Express \nCricket \nThe Phantom of the Opera \nAspects of Love \nSunset Boulevard \nBy Jeeves (aka Jeeves) \nWhistle Down the Wind \nThe Beautiful Game (aka The Boys in the Photograph) \nThe Woman in White \nThe Likes of Us \nLove Never Dies \nThe Wizard of Oz \nStephen Ward \nSchool of Rock \nBad Cinderella (aka Cinderella) Songs\n \n\n\"Amigos Para Siempre\" \n\"Any Dream Will Do\" \n\"Bad Cinderella\" \n\"Close Every Door\" \n\"Everything's Alright\" \n\"This Jesus Must Die\" \n\"I Don't Know How to Love Him\" \n\"Superstar\" \n\"Oh What a Circus\" \n\"Another Suitcase in Another Hall\" \n\"Don't Cry for Me Argentina\" \n\"You Must Love Me\" \n\"Take That Look Off Your Face\" \n\"Unexpected Song\" \n\"Memory\" \n\"Pie Jesu\" \n\"The Phantom of the Opera\" \n\"The Music of the Night\" \n\"All I Ask of You\" \n\"Anything But Lonely\" \n\"Tetris\" \n\"As If We Never Said Goodbye\" \n\"The Perfect Year\" \n\"No Matter What\" \n\"It's My Time\" \n\"'Til I Hear You Sing\" \n\"Beautiful Ghosts\" Albums\n \n\nJesus Christ Superstar \nEvita \nVariations \nCats \nThe Premiere Collection \nThe Premiere Collection Encore \nThe Collection \nNow & Forever Other compositions\n \n\nRequiem \nWatership Down Film adaptations\n \n\nJesus Christ Superstar \n\nSoundtrack \nEvita \nCats (1998) \nJoseph and the Amazing Technicolor Dreamcoat \nThe Phantom of the Opera \nThe Phantom of the Opera at the Royal Albert Hall \nJesus Christ Superstar Live in Concert \nCats (2019) Family\n \n\nWilliam Lloyd Webber (father) \nJulian Lloyd Webber (brother) \nImogen Lloyd Webber (daughter) \nNick Lloyd Webber (son) \nSarah Brightman (second wife) \nMadeleine Gurdon (third wife) \nJiaxin Cheng (sister-in-law) Company\n \n\nReally Useful Group  \n\n\nshow\n\n\nv\nt\ne\nAndrew Lloyd Webber's Cats\n \n\nBased on Old Possum's Book of Practical Cats by T. S. Eliot Characters\n \n\nBombalurina \nDemeter \nGrizabella \nGus \nJellylorum \nJemima \nJennyanydots \nMacavity \nMr. Mistoffelees \nMungojerrie \nMunkustrap \nOld Deuteronomy \nRum Tum Tugger \nRumpleteazer \nRumpus Cat \nSkimbleshanks \nVictoria the White Cat Music\nMusical\n \n\n\"Memory\" Musical cast recording\n \n\nComplete Original Broadway Cast Recording 2019 film\n \n\nHighlights from the Motion Picture Soundtrack \n\"Beautiful Ghosts\" \nFilm adaptations\n \n\n1998 \n2019 Other\n \n\nAccolades \n\"The Naming of Cats\" \n\"The Awefull Battle of the Pekes and the Pollicles\" \n \n\n Category  \n\nAuthority control databases \n\n\nMusicBrainz work\nMusicBrainz release group     \nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Beautiful_Ghosts&oldid=1259058735\" \n\nCategories: \n\n2019 songs\n2019 singles\n2010s ballads\nSongs written by Taylor Swift\nTaylor Swift songs\nSongs with music by Andrew Lloyd Webber\nSong recordings produced by Andrew Lloyd Webber\nSong recordings produced by Greg Wells\nPolydor Records singles\nSongs written for films\nCats (musical)\nPop ballads\nMusic based on works by T. S. Eliot\nHidden categories: \n\nArticles with short description\nShort description is different from Wikidata\nUse mdy dates from October 2021\nArticles with hAudio microformats\nSingle chart usages for Hungarysingle\nSingle chart called without artist\nSingle chart called without song\nSingle chart usages for Scotland\nSingle chart usages for UKdownload\nSingle chart usages for Billboarddigitalsongs    \n \n \n \n This page was last edited on 23 November 2024, at 03:38 (UTC). \nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.  \n \nPrivacy policy \nAbout Wikipedia \nDisclaimers \nContact Wikipedia \nCode of Conduct \nDevelopers \nStatistics \nCookie statement \nMobile view \nEdit preview settings \n \n \n      \n \n   \n\n", "title": "Beautiful Ghosts - Wikipedia", "description": "&quot;<strong>Beautiful</strong> <strong>Ghosts</strong>&quot; (also subtitled &quot;(From the Motion Picture Cats)&quot;) is a song by the American singer-songwriter <strong>Taylor</strong> <strong>Swift</strong> and the English composer <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> from the 2019 Cats film adaptation, in which <strong>Swift</strong> played Bombalurina. The song was written by <strong>Swift</strong> (lyrics and music) ...", "usage": {"tokens": 6694}, "retrieved_text": ["The lyric video has over 6 million views on YouTube. [6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats. [7] Composition[edit] The song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6 8 and 9 8 time signatures throughout most of the song, with the ending in compound septuple meter 21 8. [8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber. [2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics. [1][4] According to Lloyd Webber, the two \"wrote 90 percent of it pretty much over an afternoon.", "years. [4] The song was released on digital platforms and to streaming services on November 15, 2019, one month before the film's theatrical release. [5] A lyric video for the song was released on YouTube the same day. The lyric video has over 6 million views on YouTube. [6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats. [7] Composition[edit] The song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6\n8 and 9\n8 time signatures throughout most of the song, with the ending in compound septuple meter 21\n8. [8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber. [2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics.", "Jump to content \n   \n \n \n \n\n  \n \n \n  \n     \n  \n     \n   \n \n \n  \n \n \n \nArticle\nTalk    \n  English  \n \n \n \n        \n  \n \n \n \nRead\nEdit\nView history      \n  Tools  \n \n \n \n \nTools \nmove to sidebar \nhide  \n \n Actions  \n \n \nRead\nEdit\nView history    \n \n General  \n \n \nWhat links here\nRelated changes\nUpload file\nSpecial pages\nPermanent link\nPage information\nCite this page\nGet shortened URL\nDownload QR code \nEdit interlanguage links\nExpand all   \n \n Print/export  \n \n \nDownload as PDF\nPrintable version    \n \n In other projects  \n \n \nWikidata item            \n \n  \n    \n     \n \n \n  \nFrom Wikipedia, the free encyclopedia  \n\n \n\n\n2019 promotional single by Taylor Swift \n  \"Beautiful Ghosts\"Promotional single by Taylor Swiftfrom the album  Cats: Highlights from the Motion Picture Soundtrack ReleasedNovember 15, 2019 (2019-11-15)Recorded2019GenreOrchestralLength4:21LabelPolydorComposer(s)\n\n\nTaylor Swift\nAndrew Lloyd WebberLyricist(s)Taylor SwiftProducer(s)\n\n\nGreg Wells\nTom Hooper\nAndrew Lloyd WebberLyric video\"Beautiful Ghosts\" on YouTube  \n\"Beautiful Ghosts\" (also subtitled \"(From the Motion Picture Cats)\") is a song by the American singer-songwriter Taylor Swift and the English composer Andrew Lloyd Webber from the 2019 Cats film adaptation, in which Swift played Bombalurina. The song was written by Swift (lyrics and music) and Cats creator Andrew Lloyd Webber (music), and produced by Greg Wells, Lloyd Webber, and Cats director Tom Hooper. It was released on November 15, 2019."]}, {"text": "Home\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\nHome\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\n\n\n\n\n\n\nShare\n\n\n\nToday, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below:\n\n\nAs part of the release, an official lyric video has launched to accompany the song:\n\u200d\n\n\n\u200d\n\u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles.\nA behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here.\nOne of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201cBeautiful Ghosts,\u201d he says, is now, \u201can incredibly important and central part of the whole film.\u201d\nFor director Tom Hooper, the song exceeded even his high expectations. \u201cWhat I couldn\u2019t believe was the extraordinary beauty of these lyrics that Taylor had written,\u201d Hooper says. \u201cIt was as if someone had come into the film I was in the middle of making and \u2026 had reflected back to me a profound understanding of what we are trying to do with the movie. It was utterly thrilling.\u201d\n\n\n\n\nThis was a Box Five Club Exclusive\nMembers of The Box Five Club had access to this story before anybody else.\nBy signing up, you\u2019ll be among the first to receive exclusive content, special announcements, and behind-the-scenes updates, delivered straight to your inbox, as well as gaining access to the exclusive Box Five store. Join now and stay connected to all things Andrew Lloyd Webber!\nJoin Now for Free\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Phantom of the Opera | The London Cast Roundtable\nThe Phantom of the Opera | The London Cast Roundtable\nBox Five - Email Exclusive\nThe Phantom of the Opera | The London Cast Roundtable\nBox Five - Email Exclusive\nThe Phantom of the Opera | The London Cast Roundtable\n\n\nThe Royal Variety Performance 2024 - Starlight Express \nThe Royal Variety Performance 2024 - Starlight Express \nBox Five - Email Exclusive\nThe Royal Variety Performance 2024 - Starlight Express \nBox Five - Email Exclusive\nThe Royal Variety Performance 2024 - Starlight Express \n\n\n\u2019Tis sing-along season \n\u2019Tis sing-along season \nBox Five - Email Exclusive\n\u2019Tis sing-along season \nBox Five - Email Exclusive\n\u2019Tis sing-along season \n\n\nStocking Filler Gift Guide \nStocking Filler Gift Guide \nBox Five - Email Exclusive\nStocking Filler Gift Guide \nBox Five - Email Exclusive\nStocking Filler Gift Guide \n\n\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\nBox Five - Email Exclusive\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\nBox Five - Email Exclusive\nTim Rice and Andrew Lloyd Webber's Evita At The London Palladium Summer 2025\n\n\nDonny Osmond\u2019s Pharaoh Story \nDonny Osmond\u2019s Pharaoh Story \nBox Five - Email Exclusive\nDonny Osmond\u2019s Pharaoh Story \nBox Five - Email Exclusive\nDonny Osmond\u2019s Pharaoh Story \n\n\n\n\n\n\nAll News\n\n\n\n\n\n\n\n\nTerms & Conditions\n\u25c6Privacy Policy & Cookies\n\n\u00a9 2025 Really Useful Group Ltd. \n\n           \n     ", "title": "Andrew & Taylor Swift's \"Beautiful Ghosts\" is out now! \u2022 Andrew Lloyd Webber Musicals", "description": "One of the longest-running shows ... <strong>Lloyd</strong> <strong>Webber</strong>, <strong>Swift</strong> and the filmmakers were excited at the idea of introducing a new song for the film adaptation.<strong>Andrew</strong> saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201c<strong>Beautiful</strong> <strong>Ghosts</strong>,\u201d he says, ...", "usage": {"tokens": 1042}, "retrieved_text": ["Home Composer Music Shows News Fan Club Store Home Composer Music Shows News Fan Club Store Share Today, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below: As part of the release, an official lyric video has launched to accompany the song: \u200d \u200d \u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here. One of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film.", "Home\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\nHome\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\n\n\n\n\n\n\nShare\n\n\n\nToday, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below:\n\n\nAs part of the release, an official lyric video has launched to accompany the song:\n\u200d\n\n\n\u200d\n\u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here.", "role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here. One of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201cBeautiful Ghosts,\u201d he says, is now, \u201can incredibly important and central part of the whole film.\u201d\nFor director Tom Hooper, the song exceeded even his high expectations. \u201cWhat I couldn\u2019t believe was the extraordinary beauty of these lyrics that Taylor had written,\u201d Hooper says."]}, {"text": " \n\n\n2024, In ReviewFeaturedChartsVideosPromote Your Music\n\nSign Up\n\n\n\n\nGenius Q&A\nWho wrote \u201cBeautiful Ghosts\u201d by Taylor Swift?\n\nCommunity Answer\n\n\n\u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift.\n\n\nBeautiful Ghosts\n\n\nTaylor Swift\n\n136.6K ViewsRead the Lyrics \n\n\nSourced by 30 Genius contributors\nSign up to add your knowledge\n\n\nRelated questions about \u201cBeautiful Ghosts\u201d\nWho produced \u201cBeautiful Ghosts\u201d by Taylor Swift?\nWhen did Taylor Swift release \u201cBeautiful Ghosts\u201d?\n\nRelated questions about Taylor Swift\nWhen did Taylor Swift's first album release?\nWhat is the most popular album by Taylor Swift?\nWhat is the most popular song by Taylor Swift?\nWhen did Taylor Swift start making music?\n\nSign Up and Drop Knowledge\nKnow something we don\u2019t about \u201cBeautiful Ghosts\u201d by \nTaylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love.\nSign Up\n\n\n\nGenius is the world\u2019s biggest collection of song lyrics and musical knowledge\n\nAbout GeniusContributor GuidelinesPressShopAdvertisePrivacy Policy\nLicensingJobsDevelopersCopyright PolicyContact UsSign InDo Not Sell My Personal Information\n\n\u00a9 2025 ML Genius Holdings, LLC\nTerms of Use\n\nVerified Artists\n\nAll Artists:abcdefghijklmnopqrstuvwxyz#\n\nHot Songs:\nDum, Dumb, and DumberF U 2XListen UpPeggyI Always Wanted A BrotherView All    \n           \n", "title": "Who wrote \u201cBeautiful Ghosts\u201d by Taylor Swift?", "description": "\u201c<strong>Beautiful</strong> <strong>Ghosts</strong>\u201d <strong>by</strong> <strong>Taylor</strong> <strong>Swift</strong> was written by <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> &amp; <strong>Taylor</strong> <strong>Swift</strong>.", "usage": {"tokens": 388}, "retrieved_text": ["2024, In ReviewFeaturedChartsVideosPromote Your Music Sign Up Genius Q&A Who wrote \u201cBeautiful Ghosts\u201d by Taylor Swift? Community Answer \u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift. Beautiful Ghosts Taylor Swift 136.6K ViewsRead the Lyrics Sourced by 30 Genius contributors Sign up to add your knowledge Related questions about \u201cBeautiful Ghosts\u201d Who produced \u201cBeautiful Ghosts\u201d by Taylor Swift? When did Taylor Swift release \u201cBeautiful Ghosts\u201d? Related questions about Taylor Swift When did Taylor Swift's first album release? What is the most popular album by Taylor Swift? What is the most popular song by Taylor Swift? When did Taylor Swift start making music? Sign Up and Drop Knowledge Know something we don\u2019t about \u201cBeautiful Ghosts\u201d by Taylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love. Sign Up\n\n\n\nGenius is the world\u2019s biggest collection of song lyrics and musical knowledge\n\nAbout GeniusContributor GuidelinesPressShopAdvertisePrivacy Policy\nLicensingJobsDevelopersCopyright PolicyContact UsSign InDo Not Sell My Personal Information\n\n\u00a9 2025 ML Genius Holdings, LLC\nTerms of Use\n\nVerified Artists\n\nAll Artists:abcdefghijklmnopqrstuvwxyz#\n\nHot Songs:\nDum, Dumb, and DumberF U 2XListen UpPeggyI Always Wanted A BrotherView All", " \n\n\n2024, In ReviewFeaturedChartsVideosPromote Your Music\n\nSign Up\n\n\n\n\nGenius Q&A\nWho wrote \u201cBeautiful Ghosts\u201d by Taylor Swift? Community Answer\n\n\n\u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift. Beautiful Ghosts\n\n\nTaylor Swift\n\n136.6K ViewsRead the Lyrics \n\n\nSourced by 30 Genius contributors\nSign up to add your knowledge\n\n\nRelated questions about \u201cBeautiful Ghosts\u201d\nWho produced \u201cBeautiful Ghosts\u201d by Taylor Swift? When did Taylor Swift release \u201cBeautiful Ghosts\u201d? Related questions about Taylor Swift\nWhen did Taylor Swift's first album release? What is the most popular album by Taylor Swift? What is the most popular song by Taylor Swift? When did Taylor Swift start making music? Sign Up and Drop Knowledge\nKnow something we don\u2019t about \u201cBeautiful Ghosts\u201d by \nTaylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love."]}, {"text": "             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Nov 15, 2019 6:40am PT    \n Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019  \n  \n \n \n \n \n \n \n \n By  Chris Willman  \n Plus Icon    \n  \n  Chris Willman   \nSenior Music Writer and Chief Music Critic \n  ChrisWillman  \n \nLatest \n  Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour  3 days ago   \n  Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86  3 days ago   \n  \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time  5 days ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nNot to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. \nHers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical. It\u2019ll also be heard as a reprise sung by Judi Dench. Swift\u2019s version is not, as rumored by some, a \u201cpop\u201d version; with its lush orchestration and dynamics, it\u2019s very much of a piece with the music that\u2019ll be heard in the body of the film. \n \n \n \n \n      \n \nAlthough it\u2019s hardly a country song, Swift is indulging in an old country music tradition, as well as a Broadway one: the answer song. And what it\u2019s answering is the tune that broke out of the stage \u201cCats\u201d and became a standard in the 1980s, \u201cMemory.\u201d In the film, as on stage, Grizabella (Jennifer Hudson) will sing that standard about her glamour days being over, Now, Victoria essentially tells Grizabella that at least memories are better than nothing, which is what she feels she has. \n \n \n Related Stories  \n \n \n \n  \n     \n   VIP+   \n  2024 Lookback: Who Needs Emmys When the Creator Economy Is Cooking?     \n \n  \n     \n \n  \u2018The Traitors\u2019 Season 3 Adds Two New Players in Shocking Twist \u2014 Plus, Who Were Chosen as the Traitors?        \n \nBut, as Lloyd Webber told Variety in a recent interview, \u201cas she goes through the song, she becomes empowered.\u201d So, after two downbeat verses and choruses and a bridge, the lyrics change to a more upbeat mode for the final chorus. As the kitten finds her community on the streets with some other scrappy cats, the early chorus line \u201cBorn into nothing / At least you have something\u201d turns into \u201c\u2026with them I have something,\u201d and \u201cVisions of dazzling rooms I\u2019ll never get let into\u201d becomes \u201cI never knew I\u2019d love this world they\u2019ve let me into.\u201d \n \n \n Popular on Variety  \n    \n \nBelying the fact that the song was written to be sung by the youngest member of the cast, Swift returns to a more youthful tonality in her voice in her reading of the song, too, with an interpretation that makes her sound more like the ingenue of her \u201cFearless\u201d days than the domineering figure of \u201cReputation.\u201d In essence, we\u2019re hearing the return of the ghost of the younger Swift. \nWe now return you to the regularly scheduled cattiness of the music industry.  \n  \nRead More About: \n \n  Andrew Lloyd-Webber,  \n  Cats,  \n  Taylor Swift     \n  Jump to Comments   \n \n \n  \n  \n \n \n \n \n \n \n  \n       \n \n  \u2018Hollywood Squares\u2019 Premiere Delayed a Week by CBS Due to Los Angeles Fires   \n  \n      \n \n \n \n \n \n  \n       \n \n  \u2018Moana\u2019 Was Key to Disney\u2019s Comeback Year From the Beginning   \n  \n      \n \n \n \n \n \n  \n       \n \n  Sarah Michelle Gellar Is Now Open to Return for a \u2018Buffy the Vampire Slayer\u2019 Reboot: \u2018We Need Those Heroes, I Think, More So Than Ever\u2019   \n  \n      \n \n \n \n \n \n  \n       \n \n  AI Entertainment Studios: How Gen AI Toolsets Are Transforming Production Workflows   \n  \n        \n \n \n \n \n \n       JavaScript is required to load the comments. \nLoading comments... \n   \n  \n \n Most Popular   \n \n \n \n \n \n \n  Matthew Perry Gifted Lisa Kudrow the \u2018Friends\u2019 Cookie Jar in 2004. She Found the Note He Left Inside It 20 Years Later and After His Death: \u2018Timing\u2026    \n \n  \n        \n \n \n \n  Jeff Bridges, Miles Teller, Paris Hilton and More Celebrities Lose Homes in the L.A. Fires    \n \n  \n        \n \n \n \n  Milo Ventimiglia Chokes Up After Seeing Malibu Home Burn Down on Security Cameras, Eerie \u2018This Is Us\u2019 Parallel: \u2018It\u2019s Not Lost on Me, Life Imitating\u2026    \n \n  \n        \n \n \n \n \n \n \n         \n \n \n \n  \u2018Twisted Love\u2019 TV Series, Based on Ana Huang\u2019s Hit BookTok Romance Novels, Set at Netflix in Seven-Figure Deal (EXCLUSIVE)    \n \n  \n        \n \n \n \n  \u2018Wicked\u2019 Director Jon M. Chu Defends \u2018For Good\u2019 Sequel Title: \u2018Who Wants a Movie Called \u201cWicked: Part Two&#8221\u2026    \n \n  \n        \n \n \n \n  In Alleged Leaked Audio, Justin Baldoni Claims He Got \u2018Sent to the Basement\u2019 by Blake Lively at \u2018It Ends With Us\u2019 Premiere: She \u2018Didn't Want Me\u2026    \n \n  \n        \n \n \n \n  James Mangold\u2019s \u2018Star Wars\u2019 Movie Is Set 25,000 Years Before \u2018Phantom Menace\u2019 So That It\u2019s Not \u2018Handcuffed by Lore\u2019: Then \u2018You Can\u2019t Please Anybody\u2026    \n \n  \n        \n \n \n \n  Late Night Hosts Mock Trump\u2019s Bizarre Response to L.A. Fires: \u2018In Trump\u2019s Defense, Words Are Hard\u2019    \n \n  \n        \n \n \n \n  \u2018Miraculous \u2013 Tales of Ladybug & Cat Noir\u2019 Bought by Spanish Broadcaster Atresmedia (EXCLUSIVE)    \n \n  \n        \n \n \n \n  James Woods Breaks Down in Tears on CNN After Losing Home in L.A. Fire: \u2018One Day You\u2019re Swimming in the Pool and the Next Day It\u2019s All Gone\u2026    \n \n  \n            \n \n \n \n       \n \n Must Read   \n \n \n \n \n   Film   \n  ACTORS ON ACTORS | Ariana Grande and Paul Mescal Go Full \u2018Glicked\u2019 as They Dish on Singing and That \u2018Insufferable\u2019 Press Tour    By Andrew Wallenstein  1 month   \n \n  \n         \n \n \n \n   Film   \n  The Best Movies of 2024    \n \n  \n        \n \n \n   TV   \n  The Best TV Shows of 2024    \n \n  \n        \n \n \n   Music   \n  The Best Albums of 2024    \n \n  \n        \n \n \n   Film   \n  How Did Blake Lively\u2019s Lawyers Obtain Bombshell Texts From Justin Baldoni\u2019s PR Team?    \n \n  \n            \n \n Sign Up for Variety Newsletters  \n \n \n Your Email   \n  Sign Up  Sign Up             \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n\n \n \n \n \n       \n \n \n Variety Confidential   \n   \n A Variety and iHeartRadio Podcast        \n \n \n \n More From Our Brands    \n \n \n  \n  Rolling Stone  \n Mel Gibson Reveals Plans to Shoot Sequel to \u2018The Passion Of The Christ\u2019   \n \n \n        \n \n  \n  Robb Report  \n The Datsun 240Z, Influenced by the Jaguar E-Type and Early Porsche 911, Is a Classic All Its Own   \n \n \n        \n \n  \n  Sportico  \n LiAngelo Ball\u2019s \u2018Tweaker\u2019 Song Brings Fresh Family Biz to Masses   \n \n \n        \n \n  \n  SPY  \n The Best Loofahs and Body Scrubbers, According to Dermatologists   \n \n \n        \n \n  \n  TVLine  \n Milo Ventimiglia Visits Site of Family\u2019s Burned-Down Malibu Home, Says This Is Us Parallel Is \u2018Not Lost on Me\u2019   \n \n \n            \n \n \n \n      \n \n \n \n     \n \n \n  \nVariety \n \n  About Us  \n  Newsletter  \n  Variety Events  \n  Luminate - Film & TV  \n  Advertise  \n  Media Kit  \n  Careers  \n  Customer Service     \nLegal \n \n  Terms of Use  \n  Privacy Policy  \n  California Privacy Rights  \n  AdChoices  \n  Your Privacy Choices  \n  Accessibility     \nVariety Magazine \n \n  Subscribe  \n  Variety Digital Login  \n  Group Subscriptions  \n  Variety Archives  \n  Back Issues  \n  Help     \nVIP+ \n \n  Subscribe  \n  Login  \n  Learn More  \n  FAQ     \nConnect \n \n  Instagram  \n  Twitter  \n  YouTube  \n  Facebook  \n  LinkedIn      \n   Variety  \nThe Business of Entertainment  Subscribe Today \n Have a News Tip? Let us know     \n   \n \n \n \n \n \n \n   Variety   \n \n   \n Close the menu    \n \n \n \nExpand the sub menu  Film  \n \n  News  \n  Reviews  \n  Podcasts  \n  Box Office  \n  Events  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  TV  \n \n  News  \n  Reviews  \n  Podcasts  \n  Ratings  \n  Pilot Scorecard  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  What To Watch  \n \n  Lists & Features    \n \n \nExpand the sub menu  Music  \n \n  News  \n  Album Reviews  \n  Concert Reviews  \n  Hitmakers  \n  Music For Screens  \n  Grammys    \n \n \nExpand the sub menu  Docs  \n \n  Lists & Features    \n \n \nExpand the sub menu  Tech  \n \n  Features  \n  Global  \n  VIP+ Tech    \n \n \nExpand the sub menu  Global  \n \n  News  \n  Asia  \n  Markets & Festivals  \n  Features    \n \n \nExpand the sub menu  Awards Circuit  \n \n  News  \n  Film Predictions  \n  TV Predictions  \n  Video  \n  Artisans  \n  Columns  \n  Awards Circuit Podcast    \n \n \nExpand the sub menu  Video  \n \n  Popular  \n  Cover Shoots - Behind the Scenes  \n  Actors on Actors  \n  Power of Women  \n  Events  \n  Artisans    \n \n \nExpand the sub menu  What to Hear  \n \n  Lists & Features    \n \n \nExpand the sub menu  VIP+  \n \n  Daily Commentary  \n  Special Reports  \n  Video    \n \n \nExpand the sub menu  More Coverage  \n \n  Obituaries - submit obituaries to obits@variety.com  \n  Photos  \n  Podcasts  \n  Scene  \n  Voices  \n  Theater    \n \n \nExpand the sub menu  More Variety  \n \n  Live Media Summits  \n  Vscore  \n  Luminate - Film & TV  \n  Variety Archives  \n  Variety500  \n  Industry Executives      \n \n  Have a News Tip?  \n \n  \n  Newsletters    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal    \n \n  \n \nAlerts and Newsletters \n \n \n \n Your Email   \n  Sign Up  Sign Up            \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n \n  \nFollow Us \n \n      \n      \n      \n      \n        \n \n   \n  \n \n  Got a Tip?    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal     \n \n  \n \n  Advertise  \n  About  \n  Contact Us    Variety is a part of Penske Media Corporation. \u00a9 2025 Variety Media, LLC. All Rights Reserved.  \n  Icon Link Plus Icon          \nad    \n                                                                    \n                 ", "title": "Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019", "description": "<strong>Taylor</strong> <strong>Swift</strong> has released her end-titles version of &quot;<strong>Beautiful</strong> <strong>Ghosts</strong>,&quot; the song she <strong>co</strong>-<strong>wrote</strong> <strong>with</strong> <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> for the &quot;Cats&quot; film.", "usage": {"tokens": 2689}, "retrieved_text": ["Plus Icon Film Plus Icon TV Plus Icon What To Watch Plus Icon Music Plus Icon Docs Plus Icon Digital & Gaming Plus Icon Global Plus Icon Awards Circuit Plus Icon Video Plus Icon What To Hear Home Music News Nov 15, 2019 6:40am PT Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019 By Chris Willman Plus Icon Chris Willman Senior Music Writer and Chief Music Critic ChrisWillman Latest Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour 3 days ago Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86 3 days ago \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time 5 days ago See All Not to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. Hers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical.", "             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Nov 15, 2019 6:40am PT    \n Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019  \n  \n \n \n \n \n \n \n \n By  Chris Willman  \n Plus Icon    \n  \n  Chris Willman   \nSenior Music Writer and Chief Music Critic \n  ChrisWillman  \n \nLatest \n  Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour  3 days ago   \n  Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86  3 days ago   \n  \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time  5 days ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nNot to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20.", "Time 5 days ago See All Not to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. Hers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical. It\u2019ll also be heard as a reprise sung by Judi Dench. Swift\u2019s version is not, as rumored by some, a \u201cpop\u201d version; with its lush orchestration and dynamics, it\u2019s very much of a piece with the music that\u2019ll be heard in the body of the film. Although it\u2019s hardly a country song, Swift is indulging in an old country music tradition, as well as a Broadway one: the answer song."]}, {"text": "             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n  \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Oct 24, 2019 2:55pm PT    \n Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019  \n Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. \n \n \n \n \n \n \n \n By  Jon Burlingame  \n Plus Icon    \n  \n  Jon Burlingame   \n  @jonburlingame  \n \nLatest \n  Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist  3 weeks ago   \n  David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop  4 weeks ago   \n  \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE)  1 month ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nWhen Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \n\u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical. \nThey didn\u2019t actually meet up with the intention of writing a new song\u2026 at least, not ostensibly. Swift had gone over to Lloyd Webber\u2019s last December during filming to rehearse her performance of a song she sings in the film, \u201cMacavity the Mystery Cat,\u201d in her supporting role as Bombalurina. But at some point in the afternoon the fact that Lloyd Webber had written the instrumental basis of a new song came up. He played it on the piano, she began improvising lyrics and topline vocal elements, and the song now dubbed \u201cBeautiful Ghosts\u201d was nearly done. \n \n \n \n \n      \n \n Related Stories  \n \n \n \n  \n     \n   VIP+   \n  2024 Lookback: A Watershed Year for Generative AI in Hollywood     \n \n  \n     \n \n  \u2018The Traitors\u2019 Season 3 Adds Two New Players in Shocking Twist \u2014 Plus, Who Were Chosen as the Traitors?        \n \n\u201cTaylor got to the essence of what the piece is about. It\u2019s not an ordinary lyric,\u201d says Lloyd Webber, who adds that director Tom Hooper considered her attempt \u201cabsolutely extraordinary, because it\u2019s as if she really has understood the central message of the piece.\u201d \nIt\u2019s said to appear at least three times in the movie. \u201cBeautiful Ghosts\u201d is initially sung halfway through the film by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the reimagining of the 1981 stage musical. It reappears later in a brief reprise, sung by Judi Dench, as Old Deuteronomy. Finally, Swift herself will be heard performing it under the end titles. \nIs Swift\u2019s closing studio rendition a \u201cpop\u201d version? No \u2014 sources who\u2019ve heard it say her recording is very much in keeping with the spirit of the rest of the musical, not positioned as a hit single. \nNews of the collaboration satisfies ongoing curiosity on two fronts: how Swift, arguably the biggest pop star in the world, might play a greater musical role on a film project in which her character is not necessarily the lead; and whether any fresh material would be added to the nearly 40-year-old score that might make \u201cCats\u201d eligible for best song at the Oscars or Globes. (Lloyd Webber won an Oscar in 1997 for a new song he and Tim Rice wrote for Madonna to sing in \u201cEvita.\u201d) \n \n \n \n \n      \n \nIt doesn\u2019t hurt that the movie has some fresh musical material that will be awards-eligible. But there was a much more compelling reason than that to write a new song for the film: Without one, it would have been a movie musical in which the main character doesn\u2019t sing. The \u201cwhite cat\u201d exists primarily as a ballerina figure on stage. In the movie, boosted into a central role, Victoria certainly needed to express herself in song, or audiences would wonder why the cat had her tongue. \n\u201cWe had to have something for the white cat the sing, to have some kind of reaction to what\u2019s going on around her,\u201d Lloyd Webber says. \u201cWe felt there had to be a major statement for her, something that had the spirit of (T.S.) Eliot but also was specific to the story.\u201d \nIn a way, it\u2019s an answer song to the best known and loved number in \u201cCats,\u201d \u201cMemory.\u201d In the film, as on stage, Grizabella (Jennifer Hudson) will sing that standard about her glamour days being over, Now, the white cat \u2014 abandoned as a kitten \u2014 laments that her own life has barely begun. At least you have memories, Victoria tells Grizabella, in so many words, feeling that that\u2019s more than she has. \nVictoria\u2019s message, Lloyd Webber says, paraphrasing the lyrics, is that \u201cI haven\u2019t had a chance yet to dance with beautiful ghosts.\u201d He adds, \u201cAs she goes through the song, she becomes empowered, and she says, \u2018I will dance someday, and maybe they will be beautiful ghosts to me too.\u2019\u201d \n\u201cIt\u2019s a very sophisticated lyric,\u201d Lloyd Webber says, \u201cand it\u2019s entirely written for the dramatic situation. Tom Hooper said to me, \u2018It\u2019s as if she\u2019s been reading her T.S. Eliot without us knowing.'\u201d \n\u201cBeautiful Ghosts\u201d will be the only song in the score not drawn directly from Eliot\u2019s \u201cOld Possum\u2019s Book of Practical Cats,\u201d whose verse the composer set to music for the original 1981 show. In the case of \u201cGhosts,\u201d the composer and Swift share credit for both music and lyrics. \nLloyd Webber stands a chance at his first Oscar since winning for \u201cEvita\u2019s\u201d \u201cYou Must Love Me\u201d if \u201cGhosts\u201d comes to life with voters; it could well leap ahead in the pack in a year in which there have not yet been any clear \u201cShallow\u201d-style favorites. Swift has never had an Oscar nomination, but she has been a Golden Globes or Grammy nominee all three times she\u2019s written or co-written a song for a film. The star\u2019s Globe-nominated songs were \u201cSafe and Sound\u201d for 2013\u2019s \u201cThe Hunger Games\u201d and \u201cSweeter Than Fiction\u201d for 2014\u2019s \u201cOne Chance.\u201d At the 2018 Grammys, Swift was nominated for best song written for visual media for \u201cI Don\u2019t Wanna Live Forever,\u201d from \u201cFifty Shades Darker.\u201d \n \n \n \n \n      \n \nLloyd Webber couldn\u2019t help noticing one point of comparison with his new songwriting partner: \u201cShe\u2019s the same age as I was when I started writing \u2018Cats,'\u201d he points out \u2014 age 29. \nAsked if he and his fellow cat fancier spent any time talking about favorite felines, Lloyd Webber says, \u201cYes, we did talk about cats. The only thing is, at the advanced age of 71, I have actually got a puppy. So whether Taylor\u2019s ever going to speak to me again, I don\u2019t know.\u201d \nThe song\u2019s commercial release date has yet to be determined, although brief snippets can be heard in EPK footage released by the studio (watch it above).  \n  \nRead More About: \n \n  Andrew Lloyd-Webber,  \n  Cats,  \n  Taylor Swift     \n  Jump to Comments   \n \n \n  \n  \n \n \n \n \n \n \n  \n       \n \n  \u2018Hollywood Squares\u2019 Premiere Delayed a Week by CBS Due to Los Angeles Fires   \n  \n      \n \n \n \n \n \n  \n       \n \n  2024 Lookback: Who Needs Emmys When the Creator Economy Is Cooking?   \n  \n      \n \n \n \n \n \n  \n       \n \n  Sarah Michelle Gellar Is Now Open to Return for a \u2018Buffy the Vampire Slayer\u2019 Reboot: \u2018We Need Those Heroes, I Think, More So Than Ever\u2019   \n  \n      \n \n \n \n \n \n  \n       \n \n  Taylor Swift Leads New \u2018Era\u2019 for Stadium Tours: Less Rock, More Pop   \n  \n        \n \n \n \n \n \n       JavaScript is required to load the comments. \nLoading comments... \n   \n  \n \n Most Popular   \n \n \n \n \n \n \n  Matthew Perry Gifted Lisa Kudrow the \u2018Friends\u2019 Cookie Jar in 2004. She Found the Note He Left Inside It 20 Years Later and After His Death: \u2018Timing\u2026    \n \n  \n        \n \n \n \n  Jeff Bridges, Miles Teller, Paris Hilton and More Celebrities Lose Homes in the L.A. Fires    \n \n  \n        \n \n \n \n  Milo Ventimiglia Chokes Up After Seeing Malibu Home Burn Down on Security Cameras, Eerie \u2018This Is Us\u2019 Parallel: \u2018It\u2019s Not Lost on Me, Life Imitating\u2026    \n \n  \n        \n \n \n \n \n \n \n         \n \n \n \n  \u2018Twisted Love\u2019 TV Series, Based on Ana Huang\u2019s Hit BookTok Romance Novels, Set at Netflix in Seven-Figure Deal (EXCLUSIVE)    \n \n  \n        \n \n \n \n  \u2018Wicked\u2019 Director Jon M. Chu Defends \u2018For Good\u2019 Sequel Title: \u2018Who Wants a Movie Called \u201cWicked: Part Two&#8221\u2026    \n \n  \n        \n \n \n \n  In Alleged Leaked Audio, Justin Baldoni Claims He Got \u2018Sent to the Basement\u2019 by Blake Lively at \u2018It Ends With Us\u2019 Premiere: She \u2018Didn't Want Me\u2026    \n \n  \n        \n \n \n \n  James Mangold\u2019s \u2018Star Wars\u2019 Movie Is Set 25,000 Years Before \u2018Phantom Menace\u2019 So That It\u2019s Not \u2018Handcuffed by Lore\u2019: Then \u2018You Can\u2019t Please Anybody\u2026    \n \n  \n        \n \n \n \n  Late Night Hosts Mock Trump\u2019s Bizarre Response to L.A. Fires: \u2018In Trump\u2019s Defense, Words Are Hard\u2019    \n \n  \n        \n \n \n \n  \u2018Miraculous \u2013 Tales of Ladybug & Cat Noir\u2019 Bought by Spanish Broadcaster Atresmedia (EXCLUSIVE)    \n \n  \n        \n \n \n \n  James Woods Breaks Down in Tears on CNN After Losing Home in L.A. Fire: \u2018One Day You\u2019re Swimming in the Pool and the Next Day It\u2019s All Gone\u2026    \n \n  \n            \n \n \n \n       \n \n Must Read   \n \n \n \n \n   Film   \n  ACTORS ON ACTORS | Ariana Grande and Paul Mescal Go Full \u2018Glicked\u2019 as They Dish on Singing and That \u2018Insufferable\u2019 Press Tour    By Andrew Wallenstein  1 month   \n \n  \n         \n \n \n \n   Film   \n  The Best Movies of 2024    \n \n  \n        \n \n \n   TV   \n  The Best TV Shows of 2024    \n \n  \n        \n \n \n   Music   \n  The Best Albums of 2024    \n \n  \n        \n \n \n   Film   \n  How Did Blake Lively\u2019s Lawyers Obtain Bombshell Texts From Justin Baldoni\u2019s PR Team?    \n \n  \n            \n \n Sign Up for Variety Newsletters  \n \n \n Your Email   \n  Sign Up  Sign Up             \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n\n \n \n \n \n       \n \n \n Variety Confidential   \n   \n A Variety and iHeartRadio Podcast        \n \n \n \n More From Our Brands    \n \n \n  \n  Rolling Stone  \n Mel Gibson Reveals Plans to Shoot Sequel to \u2018The Passion Of The Christ\u2019   \n \n \n        \n \n  \n  Robb Report  \n The Datsun 240Z, Influenced by the Jaguar E-Type and Early Porsche 911, Is a Classic All Its Own   \n \n \n        \n \n  \n  Sportico  \n LiAngelo Ball\u2019s \u2018Tweaker\u2019 Song Brings Fresh Family Biz to Masses   \n \n \n        \n \n  \n  SPY  \n The Best Loofahs and Body Scrubbers, According to Dermatologists   \n \n \n        \n \n  \n  TVLine  \n Milo Ventimiglia Visits Site of Family\u2019s Burned-Down Malibu Home, Says This Is Us Parallel Is \u2018Not Lost on Me\u2019   \n \n \n            \n \n \n \n      \n \n \n \n     \n \n \n  \nVariety \n \n  About Us  \n  Newsletter  \n  Variety Events  \n  Luminate - Film & TV  \n  Advertise  \n  Media Kit  \n  Careers  \n  Customer Service     \nLegal \n \n  Terms of Use  \n  Privacy Policy  \n  California Privacy Rights  \n  AdChoices  \n  Your Privacy Choices  \n  Accessibility     \nVariety Magazine \n \n  Subscribe  \n  Variety Digital Login  \n  Group Subscriptions  \n  Variety Archives  \n  Back Issues  \n  Help     \nVIP+ \n \n  Subscribe  \n  Login  \n  Learn More  \n  FAQ     \nConnect \n \n  Instagram  \n  Twitter  \n  YouTube  \n  Facebook  \n  LinkedIn      \n   Variety  \nThe Business of Entertainment  Subscribe Today \n Have a News Tip? Let us know     \n   \n \n \n \n \n \n \n   Variety   \n \n   \n Close the menu    \n \n \n \nExpand the sub menu  Film  \n \n  News  \n  Reviews  \n  Podcasts  \n  Box Office  \n  Events  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  TV  \n \n  News  \n  Reviews  \n  Podcasts  \n  Ratings  \n  Pilot Scorecard  \n  Columns  \n  Trailers    \n \n \nExpand the sub menu  What To Watch  \n \n  Lists & Features    \n \n \nExpand the sub menu  Music  \n \n  News  \n  Album Reviews  \n  Concert Reviews  \n  Hitmakers  \n  Music For Screens  \n  Grammys    \n \n \nExpand the sub menu  Docs  \n \n  Lists & Features    \n \n \nExpand the sub menu  Tech  \n \n  Features  \n  Global  \n  VIP+ Tech    \n \n \nExpand the sub menu  Global  \n \n  News  \n  Asia  \n  Markets & Festivals  \n  Features    \n \n \nExpand the sub menu  Awards Circuit  \n \n  News  \n  Film Predictions  \n  TV Predictions  \n  Video  \n  Artisans  \n  Columns  \n  Awards Circuit Podcast    \n \n \nExpand the sub menu  Video  \n \n  Popular  \n  Cover Shoots - Behind the Scenes  \n  Actors on Actors  \n  Power of Women  \n  Events  \n  Artisans    \n \n \nExpand the sub menu  What to Hear  \n \n  Lists & Features    \n \n \nExpand the sub menu  VIP+  \n \n  Daily Commentary  \n  Special Reports  \n  Video    \n \n \nExpand the sub menu  More Coverage  \n \n  Obituaries - submit obituaries to obits@variety.com  \n  Photos  \n  Podcasts  \n  Scene  \n  Voices  \n  Theater    \n \n \nExpand the sub menu  More Variety  \n \n  Live Media Summits  \n  Vscore  \n  Luminate - Film & TV  \n  Variety Archives  \n  Variety500  \n  Industry Executives      \n \n  Have a News Tip?  \n \n  \n  Newsletters    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal    \n \n  \n \nAlerts and Newsletters \n \n \n \n Your Email   \n  Sign Up  Sign Up            \n By providing your information, you agree to our Terms of Use and our Privacy Policy. We use vendors that may also process your information to help provide our services. // This site is protected by reCAPTCHA Enterprise and the Google Privacy Policy and Terms of Service apply.   \n \n  \nFollow Us \n \n      \n      \n      \n      \n        \n \n   \n  \n \n  Got a Tip?    \n  Switch edition between U.S. Edition Asia Edition Global Edition  \n \nU.S. \nAsia \nGlobal     \n \n  \n \n  Advertise  \n  About  \n  Contact Us    Variety is a part of Penske Media Corporation. \u00a9 2025 Variety Media, LLC. All Rights Reserved.  \n  Icon Link Plus Icon          \nad    \n                                                                    \n                 ", "title": "Taylor Swift, Andrew Lloyd Webber Have Co-Written a New 'Cats' Song", "description": "<strong>Taylor</strong> <strong>Swift</strong> <strong>and</strong> <strong>Andrew</strong> <strong>Lloyd</strong> <strong>Webber</strong> have revealed they collaborated on writing &quot;<strong>Beautiful</strong> <strong>Ghosts</strong>,&quot; a new song for the film version of &quot;Cats.&quot;", "usage": {"tokens": 3445}, "retrieved_text": ["             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n  \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Oct 24, 2019 2:55pm PT    \n Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019  \n Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. By  Jon Burlingame  \n Plus Icon    \n  \n  Jon Burlingame   \n  @jonburlingame  \n \nLatest \n  Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist  3 weeks ago   \n  David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop  4 weeks ago   \n  \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE)  1 month ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nWhen Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it.", "Plus Icon Film Plus Icon TV Plus Icon What To Watch Plus Icon Music Plus Icon Docs Plus Icon Digital & Gaming Plus Icon Global Plus Icon Awards Circuit Plus Icon Video Plus Icon What To Hear Home Music News Oct 24, 2019 2:55pm PT Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019 Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. By Jon Burlingame Plus Icon Jon Burlingame @jonburlingame Latest Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist 3 weeks ago David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop 4 weeks ago \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE) 1 month ago See All When Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical.", "Betty Boop 4 weeks ago \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE) 1 month ago See All When Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical. They didn\u2019t actually meet up with the intention of writing a new song\u2026 at least, not ostensibly. Swift had gone over to Lloyd Webber\u2019s last December during filming to rehearse her performance of a song she sings in the film, \u201cMacavity the Mystery Cat,\u201d in her supporting role as Bombalurina. But at some point in the afternoon the fact that Lloyd Webber had written the instrumental basis of a new song came up. He played it on the piano, she began improvising lyrics and topline vocal elements, and the song now dubbed \u201cBeautiful Ghosts\u201d was nearly done."]}]}, "claim_verification_result": [{"claim": "Taylor Swift co-wrote \"Beautiful Ghosts\" with Andrew Lloyd Webber.", "search_results": "Evidence 1\nTitle: Beautiful Ghosts - Wikipedia\nContent: The lyric video has over 6 million views on YouTube. [6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats. [7] Composition[edit] The song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6 8 and 9 8 time signatures throughout most of the song, with the ending in compound septuple meter 21 8. [8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber. [2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics. [1][4] According to Lloyd Webber, the two \"wrote 90 percent of it pretty much over an afternoon.\nyears. [4] The song was released on digital platforms and to streaming services on November 15, 2019, one month before the film's theatrical release. [5] A lyric video for the song was released on YouTube the same day. The lyric video has over 6 million views on YouTube. [6] In a 2021 interview, Webber said writing \"Beautiful Ghosts\" with Swift was his \"only enjoyable part\" of working in Cats. [7] Composition[edit] The song has a running time of four minutes and twenty-one seconds. It is in the key of E, modulating from minor key in the verses to a major key in the chorus with a vocal range spanning from G#3 to C#5. It has a tempo of 60 beats per minute. The song is in a compound meter that changes between 6\n8 and 9\n8 time signatures throughout most of the song, with the ending in compound septuple meter 21\n8. [8] The orchestral ballad started as an acoustic melody composed by Lloyd Webber. [2] During rehearsals in December 2018 at his London studio, he played Swift the melody on the piano, and Swift immediately started improvising lyrics.\nJump to content \n   \n \n \n \n\n  \n \n \n  \n     \n  \n     \n   \n \n \n  \n \n \n \nArticle\nTalk    \n  English  \n \n \n \n        \n  \n \n \n \nRead\nEdit\nView history      \n  Tools  \n \n \n \n \nTools \nmove to sidebar \nhide  \n \n Actions  \n \n \nRead\nEdit\nView history    \n \n General  \n \n \nWhat links here\nRelated changes\nUpload file\nSpecial pages\nPermanent link\nPage information\nCite this page\nGet shortened URL\nDownload QR code \nEdit interlanguage links\nExpand all   \n \n Print/export  \n \n \nDownload as PDF\nPrintable version    \n \n In other projects  \n \n \nWikidata item            \n \n  \n    \n     \n \n \n  \nFrom Wikipedia, the free encyclopedia  \n\n \n\n\n2019 promotional single by Taylor Swift \n  \"Beautiful Ghosts\"Promotional single by Taylor Swiftfrom the album  Cats: Highlights from the Motion Picture Soundtrack ReleasedNovember 15, 2019 (2019-11-15)Recorded2019GenreOrchestralLength4:21LabelPolydorComposer(s)\n\n\nTaylor Swift\nAndrew Lloyd WebberLyricist(s)Taylor SwiftProducer(s)\n\n\nGreg Wells\nTom Hooper\nAndrew Lloyd WebberLyric video\"Beautiful Ghosts\" on YouTube  \n\"Beautiful Ghosts\" (also subtitled \"(From the Motion Picture Cats)\") is a song by the American singer-songwriter Taylor Swift and the English composer Andrew Lloyd Webber from the 2019 Cats film adaptation, in which Swift played Bombalurina. The song was written by Swift (lyrics and music) and Cats creator Andrew Lloyd Webber (music), and produced by Greg Wells, Lloyd Webber, and Cats director Tom Hooper. It was released on November 15, 2019.\n\nEvidence 2\nTitle: Andrew & Taylor Swift's \"Beautiful Ghosts\" is out now! \u2022 Andrew Lloyd Webber Musicals\nContent: Home Composer Music Shows News Fan Club Store Home Composer Music Shows News Fan Club Store Share Today, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below: As part of the release, an official lyric video has launched to accompany the song: \u200d \u200d \u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here. One of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film.\nHome\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\nHome\n\nComposer\n\nMusic\n\nShows\n\nNews\n\nFan Club\nStore\n\n\n\n\n\n\n\n\n\n\n\nShare\n\n\n\nToday, in collaboration with Universal Pictures, Polydor Records announced the worldwide release of \u201cBeautiful Ghosts,\u201d the new original song from Andrew and global music icon Taylor Swift for Universal Pictures\u2019 film Cats. The song is available to stream and download below:\n\n\nAs part of the release, an official lyric video has launched to accompany the song:\n\u200d\n\n\n\u200d\n\u200dCats, in theaters December 20, is directed by Oscar\u00ae-winning filmmaker Tom Hooper (Les Mis\u00e9rables, The King\u2019s Speech), and stars Swift, James Corden, Judi Dench, Jason Derulo, Idris Elba, Jennifer Hudson, Ian McKellen, Rebel Wilson and Royal Ballet principal dancer Francesca Hayward.In the film, the song is performed by Hayward, who stars in the role of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here.\nrole of Victoria. The version of the song performed by Swift, who plays Bombalurina in the film, will play over the end titles. A behind-the-scenes video documenting the creation and recording of \u201cBeautiful Ghosts,\u201d including an interview with Swift, Lloyd Webber and Tom Hooper, can also be viewed here. One of the longest-running shows in West End and Broadway history, the stage musical \u201cCats\u201d includes some of Lloyd Webber\u2019s most indelible songs, including \u201cMemory.\u201d Lloyd Webber, Swift and the filmmakers were excited at the idea of introducing a new song for the film adaptation.Andrew saw an opportunity to create a new signature song for Victoria, whose character has been expanded for the film. \u201cWhen I first read the screenplay, the first thing I said was, \u2018We have to have a song for Victoria.\u2019 \u201cBeautiful Ghosts,\u201d he says, is now, \u201can incredibly important and central part of the whole film.\u201d\nFor director Tom Hooper, the song exceeded even his high expectations. \u201cWhat I couldn\u2019t believe was the extraordinary beauty of these lyrics that Taylor had written,\u201d Hooper says.\n\nEvidence 3\nTitle: Who wrote \u201cBeautiful Ghosts\u201d by Taylor Swift?\nContent: 2024, In ReviewFeaturedChartsVideosPromote Your Music Sign Up Genius Q&A Who wrote \u201cBeautiful Ghosts\u201d by Taylor Swift? Community Answer \u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift. Beautiful Ghosts Taylor Swift 136.6K ViewsRead the Lyrics Sourced by 30 Genius contributors Sign up to add your knowledge Related questions about \u201cBeautiful Ghosts\u201d Who produced \u201cBeautiful Ghosts\u201d by Taylor Swift? When did Taylor Swift release \u201cBeautiful Ghosts\u201d? Related questions about Taylor Swift When did Taylor Swift's first album release? What is the most popular album by Taylor Swift? What is the most popular song by Taylor Swift? When did Taylor Swift start making music? Sign Up and Drop Knowledge Know something we don\u2019t about \u201cBeautiful Ghosts\u201d by Taylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love. Sign Up\n\n\n\nGenius is the world\u2019s biggest collection of song lyrics and musical knowledge\n\nAbout GeniusContributor GuidelinesPressShopAdvertisePrivacy Policy\nLicensingJobsDevelopersCopyright PolicyContact UsSign InDo Not Sell My Personal Information\n\n\u00a9 2025 ML Genius Holdings, LLC\nTerms of Use\n\nVerified Artists\n\nAll Artists:abcdefghijklmnopqrstuvwxyz#\n\nHot Songs:\nDum, Dumb, and DumberF U 2XListen UpPeggyI Always Wanted A BrotherView All\n \n\n\n2024, In ReviewFeaturedChartsVideosPromote Your Music\n\nSign Up\n\n\n\n\nGenius Q&A\nWho wrote \u201cBeautiful Ghosts\u201d by Taylor Swift? Community Answer\n\n\n\u201cBeautiful Ghosts\u201d by Taylor Swift was written by Andrew Lloyd Webber & Taylor Swift. Beautiful Ghosts\n\n\nTaylor Swift\n\n136.6K ViewsRead the Lyrics \n\n\nSourced by 30 Genius contributors\nSign up to add your knowledge\n\n\nRelated questions about \u201cBeautiful Ghosts\u201d\nWho produced \u201cBeautiful Ghosts\u201d by Taylor Swift? When did Taylor Swift release \u201cBeautiful Ghosts\u201d? Related questions about Taylor Swift\nWhen did Taylor Swift's first album release? What is the most popular album by Taylor Swift? What is the most popular song by Taylor Swift? When did Taylor Swift start making music? Sign Up and Drop Knowledge\nKnow something we don\u2019t about \u201cBeautiful Ghosts\u201d by \nTaylor Swift? Genius is the ultimate source of music knowledge, created by scholars like you who share facts and insight about the songs and artists they love.\n\nEvidence 4\nTitle: Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019\nContent: Plus Icon Film Plus Icon TV Plus Icon What To Watch Plus Icon Music Plus Icon Docs Plus Icon Digital & Gaming Plus Icon Global Plus Icon Awards Circuit Plus Icon Video Plus Icon What To Hear Home Music News Nov 15, 2019 6:40am PT Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019 By Chris Willman Plus Icon Chris Willman Senior Music Writer and Chief Music Critic ChrisWillman Latest Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour 3 days ago Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86 3 days ago \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time 5 days ago See All Not to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. Hers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical.\n             \n \n \n \n \n \n \n \n         \n  \n \n \n \n \n       \n  \n \n  Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Nov 15, 2019 6:40am PT    \n Taylor Swift Releases \u2018Beautiful Ghosts,\u2019 Her Andrew Lloyd Webber Collaboration for \u2018Cats\u2019  \n  \n \n \n \n \n \n \n \n By  Chris Willman  \n Plus Icon    \n  \n  Chris Willman   \nSenior Music Writer and Chief Music Critic \n  ChrisWillman  \n \nLatest \n  Taylor Swift\u2019s Rarest Album, \u2018Lover (Live From Paris),\u2019 Finally Gets a Wider Vinyl Release \u2014 but Still Sells Out in an Hour  3 days ago   \n  Peter Yarrow, Singer With Folk Legends Peter, Paul & Mary and Co-Writer of \u2018Puff the Magic Dragon,\u2019 Dies at 86  3 days ago   \n  \u2018Wicked\u2019 Music Producer Greg Wells on Going Big With the Soundtrack \u2014 and Hearing Ariana Grande and Cynthia Erivo\u2019s Voices Merge Into One in Real Time  5 days ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nNot to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20.\nTime 5 days ago See All Not to be buried among the week\u2019s other Taylor Swift news, the singer has released her recording of the song she wrote with Andrew Lloyd Webber \u201cBeautiful Ghosts,\u201d which will be heard over the end credits of \u201cCats\u201d upon its release Dec. 20. Hers is one of three versions of \u201cGhosts\u201d that will be heard in the film, as Swift and Lloyd Webber wrote it primarily to be sung by Francesca Hayward, who plays Victoria, the white cat that has been made a central figure in the film adaptation of the 1981 stage musical. It\u2019ll also be heard as a reprise sung by Judi Dench. Swift\u2019s version is not, as rumored by some, a \u201cpop\u201d version; with its lush orchestration and dynamics, it\u2019s very much of a piece with the music that\u2019ll be heard in the body of the film. Although it\u2019s hardly a country song, Swift is indulging in an old country music tradition, as well as a Broadway one: the answer song.\n\nEvidence 5\nTitle: Taylor Swift, Andrew Lloyd Webber Have Co-Written a New 'Cats' Song\nContent: Plus Icon   Film    \n  Plus Icon   TV    \n  Plus Icon   What To Watch    \n  Plus Icon   Music    \n  Plus Icon   Docs    \n  Plus Icon   Digital & Gaming    \n  Plus Icon   Global    \n  Plus Icon   Awards Circuit    \n  Plus Icon   Video    \n  Plus Icon   What To Hear    \n           \n   \n \n \n  \n \n \n \n \n  \n \n  Home  \n  Music  \n  News      \n  Oct 24, 2019 2:55pm PT    \n Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019  \n Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. By  Jon Burlingame  \n Plus Icon    \n  \n  Jon Burlingame   \n  @jonburlingame  \n \nLatest \n  Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist  3 weeks ago   \n  David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop  4 weeks ago   \n  \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE)  1 month ago    \n  See All         \n \n    \n \n \n      \n      \n      \n      \n        \n      \n      \n      \n      \n      \n      \n           \n \n \n \n \n \n   \n \n      \n      \n \n  \n\nWhen Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it.\nPlus Icon Film Plus Icon TV Plus Icon What To Watch Plus Icon Music Plus Icon Docs Plus Icon Digital & Gaming Plus Icon Global Plus Icon Awards Circuit Plus Icon Video Plus Icon What To Hear Home Music News Oct 24, 2019 2:55pm PT Taylor Swift and Andrew Lloyd Webber Have Co-Written a New Song for \u2018Cats\u2019 Francesca Hayward sings \"Beautiful Ghosts\" during the film, and Swift's studio version will be heard over the end titles. By Jon Burlingame Plus Icon Jon Burlingame @jonburlingame Latest Robbie Williams\u2019 \u2018Better Man\u2019 Original Song Disqualified From Oscars After Making Shortlist 3 weeks ago David Foster Is Heading to Broadway With the Musical-Comedy Adaptation of the 1930s Cartoon Character Betty Boop 4 weeks ago \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE) 1 month ago See All When Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical.\nBetty Boop 4 weeks ago \u2018Wicked,\u2019 \u2018Moana 2\u2019 and 143 More Film Scores Eligible for the Oscar as Shortlist Voting Begins (EXCLUSIVE) 1 month ago See All When Andrew Lloyd Webber and Taylor Swift had the opportunity to write a song together for \u201cCats,\u201d they, well, pounced on it. \u201cWe wrote 90 percent of it pretty much over an afternoon,\u201d Lloyd Webber tells Variety about his and Swift\u2019s fast-combusting collaboration on \u201cBeautiful Ghosts,\u201d the one brand new song written for the forthcoming movie adaptation of the 1981 stage musical. They didn\u2019t actually meet up with the intention of writing a new song\u2026 at least, not ostensibly. Swift had gone over to Lloyd Webber\u2019s last December during filming to rehearse her performance of a song she sings in the film, \u201cMacavity the Mystery Cat,\u201d in her supporting role as Bombalurina. But at some point in the afternoon the fact that Lloyd Webber had written the instrumental basis of a new song came up. He played it on the piano, she began improvising lyrics and topline vocal elements, and the song now dubbed \u201cBeautiful Ghosts\u201d was nearly done.\n\n", "verification_result": "supported"}], "response_stats": {"sentences": 1, "verified_claims": 1, "verified_supported_claims": 1, "pre_supported_claims": 1, "pre_verified_claims": 3, "all_claims": 4, "P": 0.5}}
{"question": "Show me some papers Graham Neubig has published.", "prompt_source": "felm", "response": "\"Graham Neubig is a computer science researcher who has made significant contributions in the field of natural language processing. He has published numerous research papers, conference proceedings, and book chapters. Some of his notable publications are:\\n\\n1. Neural Machine Translation of Rare Words with Subword Units: This paper discusses the use of subword units for neural machine translation, which improves translation quality for words that are rare or out-of-vocabulary.\\n\\n2. Learning to Translate in Real-time with Neural Machine Translation: This paper describes a method for real-time neural machine translation that can handle streaming inputs without waiting for the entire input sequence.\\n\\n3. Unsupervised Cross-lingual Representation Learning at Scale: This paper discusses an unsupervised approach to learning cross-lingual representations that can be used for tasks such as transfer learning and zero-shot learning.\\n\\n4. Robust Multilingual Part-of-Speech Tagging via Adversarial Training: This paper presents a method for improving part-of-speech tagging across multiple languages by using adversarial training to generate more robust models.\\n\\n5. Unsupervised Natural Language Generation with Denoising Autoencoders: This paper proposes a method for unsupervised natural language generation using denoising autoencoders, which can be used for tasks such as text summarization and paraphrasing.\\n\\nThese are just a few examples of the many research papers that Graham Neubig has published. Interested readers can find more of his work on his personal webpage or on popular academic databases such as Google Scholar or arXiv.\"", "prompt_tok_cnt": 2334, "response_tok_cnt": 291, "model": "ChatGPT", "abstained": false, "predefined_stats_lst": [[5, 2, 0]], "pre_supported_claims_lst": ["Graham Neubig is a computer science researcher who has made significant contributions in the field of natural language processing.", "Graham Neubig has published numerous research papers, conference proceedings, and book chapters."], "pre_unsupported_claims_lst": [], "claim_to_verify_list": [["The paper titled \"Neural Machine Translation of Rare Words with Subword Units\" discusses the use of subword units for neural machine translation, which improves translation quality for words that are rare or out-of-vocabulary.", "The paper titled \"Learning to Translate in Real-time with Neural Machine Translation\" describes a method for real-time neural machine translation that can handle streaming inputs without waiting for the entire input sequence.", "The paper titled \"Unsupervised Cross-lingual Representation Learning at Scale\" discusses an unsupervised approach to learning cross-lingual representations that can be used for tasks such as transfer learning and zero-shot learning.", "The paper titled \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\" presents a method for improving part-of-speech tagging across multiple languages by using adversarial training to generate more robust models.", "The paper titled \"Unsupervised Natural Language Generation with Denoising Autoencoders\" proposes a method for unsupervised natural language generation using denoising autoencoders, which can be used for tasks such as text summarization and paraphrasing."]], "all_claims_to_verify": ["The paper titled \"Neural Machine Translation of Rare Words with Subword Units\" discusses the use of subword units for neural machine translation, which improves translation quality for words that are rare or out-of-vocabulary.", "The paper titled \"Learning to Translate in Real-time with Neural Machine Translation\" describes a method for real-time neural machine translation that can handle streaming inputs without waiting for the entire input sequence.", "The paper titled \"Unsupervised Cross-lingual Representation Learning at Scale\" discusses an unsupervised approach to learning cross-lingual representations that can be used for tasks such as transfer learning and zero-shot learning.", "The paper titled \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\" presents a method for improving part-of-speech tagging across multiple languages by using adversarial training to generate more robust models.", "The paper titled \"Unsupervised Natural Language Generation with Denoising Autoencoders\" proposes a method for unsupervised natural language generation using denoising autoencoders, which can be used for tasks such as text summarization and paraphrasing."], "claim_search_results": {"The paper titled \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\" presents a method for improving part-of-speech tagging across multiple languages by using adversarial training to generate more robust models.": [{"text": " \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1711.04903 (cs)   \n \n \n [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] \nTitle:Robust Multilingual Part-of-Speech Tagging via Adversarial Training \nAuthors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev \nView a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF \n Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.   \n   Comments: NAACL 2018   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1711.04903 [cs.CL]     (or  arXiv:1711.04903v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1711.04903\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Michihiro Yasunaga [view email] \n [v1] Tue, 14 Nov 2017 01:50:30 UTC (359 KB)\n [v2] Fri, 20 Apr 2018 15:49:22 UTC (187 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2017-11 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Michihiro Yasunaga\nJungo Kasai\nDragomir R. Radev  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1711.04903] Robust Multilingual Part-of-Speech Tagging via Adversarial Training", "description": "<strong>Adversarial</strong> <strong>training</strong> (AT) is a powerful regularization <strong>method</strong> <strong>for</strong> neural networks, aiming to achieve <strong>robustness</strong> <strong>to</strong> input perturbations. Yet, the specific effects of <strong>the</strong> <strong>robustness</strong> obtained from AT are still unclear in the context of natural <strong>language</strong> processing.", "usage": {"tokens": 1315}, "retrieved_text": [" \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1711.04903 (cs)   \n \n \n [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] \nTitle:Robust Multilingual Part-of-Speech Tagging via Adversarial Training \nAuthors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev \nView a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF \n Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations.", "Computer Science > Computation and Language arXiv:1711.04903 (cs) [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] Title:Robust Multilingual Part-of-Speech Tagging via Adversarial Training Authors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks.", "for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. Comments: NAACL 2018   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1711.04903 [cs.CL]     (or  arXiv:1711.04903v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1711.04903\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Michihiro Yasunaga [view email] \n [v1] Tue, 14 Nov 2017 01:50:30 UTC (359 KB)\n [v2] Fri, 20 Apr 2018 15:49:22 UTC (187 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2017-11 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Michihiro Yasunaga\nJungo Kasai\nDragomir R. Radev  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training\nMichihiro Yasunaga, Jungo Kasai, Dragomir Radev\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractAdversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.\n\nAnthology ID:\nN18-1089\nVolume:\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\nMonth:\nJune\nYear:\n2018\nAddress:\nNew Orleans, Louisiana\nEditors:\nMarilyn Walker, Heng Ji, Amanda Stent\nVenue:\nNAACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n976\u2013986\nLanguage:\n\nURL:\nhttps://aclanthology.org/N18-1089/\nDOI:\n10.18653/v1/N18-1089\nBibkey:\n\nyasunaga-etal-2018-robust\nCite (ACL):\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics.\n\nCite (Informal):\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/N18-1089.pdf\nCode\n michiyasunaga/pos_adv\nData\nCoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"yasunaga-etal-2018-robust\"> <titleInfo> <title>Robust Multilingual Part-of-Speech Tagging via Adversarial Training</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Michihiro</namePart> <namePart type=\"family\">Yasunaga</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Jungo</namePart> <namePart type=\"family\">Kasai</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Dragomir</namePart> <namePart type=\"family\">Radev</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2018-06</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Marilyn</namePart> <namePart type=\"family\">Walker</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Heng</namePart> <namePart type=\"family\">Ji</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Amanda</namePart> <namePart type=\"family\">Stent</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">New Orleans, Louisiana</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.</abstract> <identifier type=\"citekey\">yasunaga-etal-2018-robust</identifier> <identifier type=\"doi\">10.18653/v1/N18-1089</identifier> <location> <url>https://aclanthology.org/N18-1089/</url> </location> <part> <date>2018-06</date> <extent unit=\"page\"> <start>976</start> <end>986</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Robust Multilingual Part-of-Speech Tagging via Adversarial Training %A Yasunaga, Michihiro %A Kasai, Jungo %A Radev, Dragomir %Y Walker, Marilyn %Y Ji, Heng %Y Stent, Amanda %S Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) %D 2018 %8 June %I Association for Computational Linguistics %C New Orleans, Louisiana %F yasunaga-etal-2018-robust %X Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. %R 10.18653/v1/N18-1089 %U https://aclanthology.org/N18-1089/ %U https://doi.org/10.18653/v1/N18-1089 %P 976-986 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Robust Multilingual Part-of-Speech Tagging via Adversarial Training](https://aclanthology.org/N18-1089/) (Yasunaga et al., NAACL 2018)\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\nACL\n\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training - ACL Anthology", "description": "Michihiro Yasunaga, Jungo Kasai, Dragomir Radev. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human <strong>Language</strong> Technologies, Volume 1 (Long <strong>Papers</strong>). 2018.", "usage": {"tokens": 2785}, "retrieved_text": ["(ACL): Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics. Cite (Informal): Robust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/N18-1089.pdf Code michiyasunaga/pos_adv Data CoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies PDF Cite Search Code Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing.", "Year: 2018 Address: New Orleans, Louisiana Editors: Marilyn Walker, Heng Ji, Amanda Stent Venue: NAACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 976\u2013986 Language: URL: https://aclanthology.org/N18-1089/ DOI: 10.18653/v1/N18-1089 Bibkey: yasunaga-etal-2018-robust Cite (ACL): Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics. Cite (Informal):\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/N18-1089.pdf\nCode\n michiyasunaga/pos_adv\nData\nCoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations.", "%C New Orleans, Louisiana %F yasunaga-etal-2018-robust %X Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. %R 10.18653/v1/N18-1089 %U https://aclanthology.org/N18-1089/ %U https://doi.org/10.18653/v1/N18-1089 %P 976-986 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Robust Multilingual Part-of-Speech Tagging via Adversarial Training](https://aclanthology.org/N18-1089/) (Yasunaga et al., NAACL 2018)\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\nACL\n\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev."]}, {"text": " \n \n \n \nRobust Multilingual Part-of-Speech Tagging via Adversarial Training \n  Michihiro Yasunaga Jungo Kasai Dragomir Radev \nDepartment of Computer Science, Yale University \n  {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu    \n \nAbstract \nAdversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.   \n 1 Introduction \n \nFigure 1: Illustration of our architecture for adversarial POS tagging. Given a sentence, we input the normalized word embeddings (\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0,\ud835\udc98\ud835\udfd1subscript\ud835\udc981subscript\ud835\udc982subscript\ud835\udc983\\mbox{\\boldmath$\\uline{w}_{1}$},\\mbox{\\boldmath$\\uline{w}_{2}$},\\mbox{\\boldmath$\\uline{w}_{3}$}) and character embeddings (showing \ud835\udc84\ud835\udfcf,\ud835\udc84\ud835\udfd0,\ud835\udc84\ud835\udfd1subscript\ud835\udc841subscript\ud835\udc842subscript\ud835\udc843\\mbox{\\boldmath$\\uline{c}_{1}$},\\mbox{\\boldmath$\\uline{c}_{2}$},\\mbox{\\boldmath$\\uline{c}_{3}$} for w1subscript\ud835\udc641w_{1}). Each word is represented by concatenating its word embedding and its character-level BiLSTM output. They are fed into the main BiLSTM-CRF network for POS tagging. In adversarial training, we compute and add the worst-case perturbation \u03b7\ud835\udf02\\eta to all the input embeddings for regularization.   \n \nRecently, neural network-based approaches have become popular in many natural language processing (NLP) tasks including tagging, parsing, and translation Chen and Manning (2014); Bahdanau et al. (2015); Ma and Hovy (2016). However, it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models Szegedy et al. (2014). Such maliciously perturbed inputs are called adversarial examples. Adversarial training Goodfellow et al. (2015) aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples. Previous work Goodfellow et al. (2015); Shaham et al. (2015) on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial training and has provided theoretical explanations of the regularization effects.  \n \nDespite its potential as a powerful regularizer, adversarial training (AT) has yet to be explored extensively in natural language tasks. Recently, Miyato et al. (2017) applied AT on text classification, achieving state-of-the-art accuracy. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of NLP. For example, research studies have yet to answer questions such as 1) how can we interpret perturbations or robustness on natural language inputs? 2) how are they related to linguistic factors like vocabulary statistics? 3) are the effects of AT language-dependent? Answering such questions is crucial to understand and motivate the application of adversarial training on natural language tasks.  \n \nIn this paper, spotlighting a well-studied core problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training. With a BiLSTM-CRF model Huang et al. (2015); Ma and Hovy (2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word \u200b/\u200b character embeddings. In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset (27 languages), with thorough analyses of the following points: \n \n \u2022 \n \nEffects on different target languages   \n \u2022 \n \nVocabulary statistics and tagging accuracy   \n \u2022 \n \nInfluence on downstream tasks   \n \u2022 \n \nRepresentation learning of words     \n \nIn our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages. Furthermore, our analyses reveal the following insights into adversarial training in the context of NLP:  \n \n \n \u2022 \n \nThe regularization effects of adversarial training (AT) are general across different languages. AT can prevent overfitting especially well when training examples are scarce, providing an effective tool to process low resource languages.   \n \u2022 \n \nAT can boost the tagging performance for rare \u200b/\u200b unseen words and increase the sentence-level accuracy. This positively affects the performance of down-stream tasks such as dependency parsing, where low sentence-level POS accuracy can be a bottleneck Manning (2011).   \n \u2022 \n \nAT helps the network learn cleaner word embeddings, showing stronger correlations with their POS tags.    \nWe argue that the effects of AT can be interpreted from the perspective of natural language. Finally, we demonstrate that the proposed AT model is generally effective across different sequence labeling tasks. This work therefore provides a strong motivation and basis for utilizing adversarial training in NLP tasks.    \n 2 Related Work  \n 2.1 POS Tagging \n \nPart-of-speech (POS) tagging is a fundamental NLP task that facilitates downstream tasks such as syntactic parsing. While current state-of-the-art POS taggers Ling et al. (2015); Ma and Hovy (2016) yield accuracy over 97.5% on PTB-WSJ, there still remain issues. The per token accuracy metric is easy since taggers can easily assign correct POS tags to highly unambiguous tokens, such as punctuation Manning (2011). Sentence-level accuracy serves as a more realistic metric for POS taggers but it still remains low. Another problem with current POS taggers is that their accuracy deteriorates drastically on low resource languages and rare words Plank et al. (2016). In this work, we demonstrate that adversarial training (AT) can mitigate these issues.  \n \nIt is empirically shown that POS tagging performance can greatly affect downstream tasks such as dependency parsing Dozat et al. (2017). In this work, we also demonstrate that the improvements obtained from our AT POS tagger actually contribute to dependency parsing. Nonetheless, parsing with gold POS tags still yields better results, bolstering the view that POS tagging is an essential task in NLP that needs further development.    \n 2.2 Adversarial Training \n \nThe concept of adversarial training Szegedy et al. (2014); Goodfellow et al. (2015) was originally introduced in the context of image classification to improve the robustness of a model by training on input images with malicious perturbations. Previous work Goodfellow et al. (2015); Shaham et al. (2015); Wang et al. (2017) has provided a theoretical framework to understand adversarial examples and the regularization effects of adversarial training (AT) in image recognition.  \n \nRecently, Miyato et al. (2017) applied AT to a natural language task (text classification) by extending the concept of adversarial perturbations to word embeddings. Wu et al. (2017) further explored the possibility of AT in relation extraction. Both report improved performance on their tasks via AT, but the specific effects of AT have yet to be analyzed. In our work, we aim to address this issue by providing detailed analyses on the effects of AT from the perspective of NLP, such as different languages, vocabulary statistics, word embedding distribution, and aim to motivate future research that exploits AT in NLP tasks.  \n \nAT is related to other regularization methods that add noise to data such as dropout Srivastava et al. (2014) and its variant for NLP tasks, word dropout Iyyer et al. (2015). Xie et al. (2017) discuss various data noising techniques for language modeling. While these methods produce random noise, AT generates perturbations that the current model is particularly vulnerable to, and thus is claimed to be effective Goodfellow et al. (2015).  \n \nIt should be noted that while related in name, adversarial training (AT) differs from Generative Adversarial Networks (GANs) Goodfellow et al. (2014). GANs have already been applied to NLP tasks such as dialogue generation Li et al. (2017) and transfer learning Kim et al. (2017); Gui et al. (2017). Adversarial training also differs from adversarial evaluation, recently proposed for reading comprehension tasks Jia and Liang (2017).     \n 3 Method \n \nIn this section, we introduce our baseline POS tagging model and explain how we implement adversarial training on top.   \n 3.1 Baseline POS Tagging Model \n \nFollowing the recent top-performing models for sequence labeling tasks Plank et al. (2016); Lample et al. (2016); Ma and Hovy (2016), we employ a Bi-directional LSTM-CRF model as our baseline (see Figure 1 for an illustration).   \nCharacter-level BiLSTM. \n \nPrior work has shown that incorporating character-level representations of words can boost POS tagging accuracy by capturing morphological information present in each language. Major neural character-level models include the character-level CNN Ma and Hovy (2016) and (Bi)LSTM Dozat et al. (2017). A Bi-directional LSTM (BiLSTM) Hochreiter and Schmidhuber (1997); Schuster and Paliwal (1997) processes each sequence both forward and backward to capture sequential information, while preventing the vanishing \u200b/\u200b exploding gradient problem. We observed that the character-level BiLSTM outperformed the CNN by 0.1% on the PTB-WSJ development set, and hence in all of our experiments we use the character-level BiLSTM. Specifically, we generate a character-level representation for each word by feeding its character embeddings into the BiLSTM and obtaining the concatenated final states.    \nWord-level BiLSTM. \n \nEach word in a sentence is represented by concatenating its word embedding and its character-level representation. They are fed into another level of BiLSTM (word-level BiLSTM) to process the entire sentence.    \nCRF. \n \nIn sequence labeling tasks it is beneficial to consider the correlations between neighboring labels and jointly decode the best chain of labels for a given sentence. With this motivation, we apply a conditional random field (CRF) Lafferty et al. (2001) on top of the word-level BiLSTM to perform POS tag inference with global normalization, addressing the \u201clabel bias\u201d problem. Specifically, given an input sentence, we pass the output sequence of the word-level BiLSTM to a first-order chain CRF to compute the conditional probability of the target label sequence:    p\u200b(\ud835\udc9a|\ud835\udc94;\ud835\udf3d)\ud835\udc5dconditional\ud835\udc9a\ud835\udc94\ud835\udf3dp(\\mbox{\\boldmath$y$}\\,|\\,\\mbox{\\boldmath$s$};\\mbox{\\boldmath$\\theta$})    \nwhere \ud835\udf3d\ud835\udf3d\\theta represents all of the model parameters (in the BiLSTMs and CRF), \ud835\udc94\ud835\udc94s and \ud835\udc9a\ud835\udc9ay denote the input embeddings and the target POS tag sequence, respectively, for the given sentence.  \n \nFor training, we minimize the negative log-likelihood (loss function)    L\u200b(\ud835\udf3d;\ud835\udc94,\ud835\udc9a)=\u2212log\u2061p\u200b(\ud835\udc9a|\ud835\udc94;\ud835\udf3d)\ud835\udc3f\ud835\udf3d\ud835\udc94\ud835\udc9a\ud835\udc5dconditional\ud835\udc9a\ud835\udc94\ud835\udf3dL(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$})=-\\log p(\\mbox{\\boldmath$y$}\\,|\\,\\mbox{\\boldmath$s$};\\mbox{\\boldmath$\\theta$})  (1)   \nwith respect to the model parameters. Decoding searches for the POS tag sequence \ud835\udc9a\u2217superscript\ud835\udc9a\\mbox{\\boldmath$y$}^{*} with the highest conditional probability using the Viterbi algorithm. For more detail about the BiLSTM-CRF formulation, refer to Ma and Hovy (2016).     \n 3.2 Adversarial Training \n \nAdversarial training Goodfellow et al. (2015) is a powerful regularization method, primarily explored in image recognition to improve the robustness of classifiers to input perturbations. Given a classifier, we first generate input examples that are very close to original inputs (so should yield the same labels) yet are likely to be misclassified by the current model. Specifically, these adversarial examples are generated by adding small perturbations to the inputs in the direction that significantly increases the loss function of the classifier (worst-case perturbations). Then, the classifier is trained on the mixture of clean examples and adversarial examples to improve the stability to input perturbations. In this work, we incorporate adversarial training into our baseline POS tagger, aiming to achieve better regularization effects and to provide their interpretations in the context of NLP.   \nGenerating adversarial examples. \n \nAdversarial training (AT) considers continuous perturbations to inputs, so we define perturbations at the level of dense word / character embeddings rather than one-hot vector representations, similarly to Miyato et al. (2017). Specifically, given an input sentence, we consider the concatenation of all the word \u200b/\u200b character embeddings in the sentence: \ud835\udc94=[\ud835\udc981,\ud835\udc982,\u2026,\ud835\udc841,\ud835\udc842,\u2026]\ud835\udc94subscript\ud835\udc981subscript\ud835\udc982\u2026subscript\ud835\udc841subscript\ud835\udc842\u2026\\mbox{\\boldmath$s$}=[\\mbox{\\boldmath$w$}_{1},\\mbox{\\boldmath$w$}_{2},\\dots,\\mbox{\\boldmath$c$}_{1},\\mbox{\\boldmath$c$}_{2},\\dots]. To prepare an adversarial example, we aim to generate the worst-case perturbation of a small bounded norm \u03f5italic-\u03f5\\epsilon that maximizes the loss function L\ud835\udc3fL of the current model:    \ud835\udf3c=arg\u200bmax\ud835\udf3c\u2032:\u2016\ud835\udf3c\u2032\u20162\u2264\u03f5\u2061L\u200b(\ud835\udf3d^;\ud835\udc94+\ud835\udf3c\u2032,\ud835\udc9a)\ud835\udf3csubscriptargmax:superscript\ud835\udf3c\u2032subscriptnormsuperscript\ud835\udf3c\u20322italic-\u03f5\ud835\udc3f^\ud835\udf3d\ud835\udc94superscript\ud835\udf3c\u2032\ud835\udc9a\\mbox{\\boldmath$\\eta$}=\\operatorname*{arg\\,max}_{\\scalebox{0.75}{$\\mbox{\\boldmath$\\eta$}^{\\prime}$}:\\,\\|\\scalebox{0.75}{$\\mbox{\\boldmath$\\eta$}^{\\prime}$}\\|_{2}\\,\\leq\\,\\epsilon}L(\\hat{\\mbox{\\boldmath$\\theta$}};\\mbox{\\boldmath$s$}\\!\\!~{}+\\!\\!~{}\\mbox{\\boldmath$\\eta$}^{\\prime},\\mbox{\\boldmath$y$})    \nwhere \ud835\udf3d^^\ud835\udf3d\\hat{\\mbox{\\boldmath$\\theta$}} is the current value of the model parameters, treated as a constant, and \ud835\udc9a\ud835\udc9ay denotes the target labels. Since the exact computation of such \ud835\udf3c\ud835\udf3c\\eta is intractable in complex neural networks, we employ the Fast Gradient Method Liu et al. (2017); Miyato et al. (2017) i.e. first order approximation to obtain an approximate worst-case perturbation of norm \u03f5italic-\u03f5\\epsilon, by a single gradient computation:    \ud835\udf3c=\u03f5\u200b\ud835\udc88/\u2016\ud835\udc88\u20162\u200b,  where \ud835\udc88=\u2207\ud835\udc94L\u200b(\ud835\udf3d^;\ud835\udc94,\ud835\udc9a)\ud835\udf3citalic-\u03f5\ud835\udc88subscriptnorm\ud835\udc882,  where \ud835\udc88subscript\u2207\ud835\udc94\ud835\udc3f^\ud835\udf3d\ud835\udc94\ud835\udc9a\\mbox{\\boldmath$\\eta$}=\\epsilon\\,\\mbox{\\boldmath$g$}/\\|\\mbox{\\boldmath$g$}\\|_{2}\\mbox{, ~{}where }\\mbox{\\boldmath$g$}=\\nabla_{\\scalebox{0.8}{$\\mbox{\\boldmath$s$}$}}L(\\hat{\\mbox{\\boldmath$\\theta$}};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$})  (2)   \n\u03f5italic-\u03f5\\epsilon is a hyperparameter to be determined in the development dataset. Note that the perturbation \ud835\udf3c\ud835\udf3c\\eta is generated in the direction that significantly increases the loss L\ud835\udc3fL. We find such \ud835\udf3c\ud835\udf3c\\eta against the current model parameterized by \ud835\udf3d^^\ud835\udf3d\\hat{\\mbox{\\boldmath$\\theta$}}, at each training step, and construct an adversarial example by    \ud835\udc94adv=\ud835\udc94+\ud835\udf3csubscript\ud835\udc94adv\ud835\udc94\ud835\udf3c\\mbox{\\boldmath$s$}_{\\mathrm{adv}}=\\mbox{\\boldmath$s$}+\\mbox{\\boldmath$\\eta$}\\vspace{-1.5mm}     \n \nHowever, if we do not restrict the norm of word \u200b/\u200b character embeddings, the model could trivially learn embeddings of large norms to make the perturbations insignificant. To prevent this issue, we normalize word \u200b/\u200b character embeddings so that they have mean 0 and variance 1 for every entry, as in Miyato et al. (2017). The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples. To ensure a fair comparison, we also normalize input embeddings in our baseline model.  \n \nWhile Miyato et al. (2017) set the norm of a perturbation \u03f5italic-\u03f5\\epsilon (Eq 2) to be a fixed value for all input sentences, to generate adversarial examples for an entire sentence of a variable length and to include character embeddings besides word embeddings, we make the perturbation size \u03f5italic-\u03f5\\epsilon adaptive to the dimension of the concatenated input embedding \ud835\udc94\u2208\u211dD\ud835\udc94superscript\u211d\ud835\udc37\\mbox{\\boldmath$s$}\\in\\mathbb{R}^{D}. We set \u03f5italic-\u03f5\\epsilon to be \u03b1\u200bD\ud835\udefc\ud835\udc37\\alpha\\mbox{\\scalebox{0.95}[0.95]{$\\sqrt{D}$}} (i.e., proportional to   D\ud835\udc37\\sqrt{D} ), as the expected squared norm of \ud835\udc94\ud835\udc94s after the embedding normalization is D\ud835\udc37D. The scaling factor \u03b1\ud835\udefc\\alpha is selected from {{\\{0.001, 0.005, 0.01, 0.05, 0.1}}\\} based on the development performance in each treebank. We used 0.01 for PTB-WSJ and UD-Spanish, and 0.05 for the rest. Note that \u03b1=0\ud835\udefc0\\alpha\\!=\\!0 would generate no noise (identical to the baseline); if \u03b1=1\ud835\udefc1\\alpha\\!=\\!1, the generated adversarial perturbation would have a norm comparable to the original embedding, which could change the semantics of the input sentence Wu et al. (2017). Hence, the optimal perturbation scale \u03b1\ud835\udefc\\alpha should lie in between and be small enough to preserve the semantics of the original input.    \nAdversarial training. \n \nAt each training step, we generate adversarial examples against the current model, and train on the mixture of clean examples and adversarial examples to achieve robustness to input perturbations. To this end, we define the loss function for adversarial training as:    L~=\u03b3\u200bL\u200b(\ud835\udf3d;\ud835\udc94,\ud835\udc9a)+(1\u2212\u03b3)\u200bL\u200b(\ud835\udf3d;\ud835\udc94adv,\ud835\udc9a)~\ud835\udc3f\ud835\udefe\ud835\udc3f\ud835\udf3d\ud835\udc94\ud835\udc9a1\ud835\udefe\ud835\udc3f\ud835\udf3dsubscript\ud835\udc94adv\ud835\udc9a\\tilde{L}=\\gamma L(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$})+(1-\\gamma)L(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$}_{\\mathrm{adv}},\\mbox{\\boldmath$y$})    \nwhere L\u200b(\ud835\udf3d;\ud835\udc94,\ud835\udc9a)\ud835\udc3f\ud835\udf3d\ud835\udc94\ud835\udc9aL(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$}), L\u200b(\ud835\udf3d;\ud835\udc94adv,\ud835\udc9a)\ud835\udc3f\ud835\udf3dsubscript\ud835\udc94adv\ud835\udc9aL(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$}_{\\mathrm{adv}},\\mbox{\\boldmath$y$}) represent the loss from a clean example and the loss from its adversarial example, respectively, and \u03b3\ud835\udefe\\gamma determines the weighting between them. We used \u03b3=0.5\ud835\udefe0.5\\gamma=0.5 in all our experiments. This objective function can be optimized with respect to the model parameters \ud835\udf3d\ud835\udf3d\\theta, in the same manner as the baseline model.      \n 4 Experiments \n \nTo fully analyze the effects of adversarial training, we train and evaluate our baseline \u200b/\u200b adversarial POS tagging models on both a standard English dataset and a multilingual dataset.   \n 4.1 Datasets \n \nAs a standard English dataset, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) Marcus et al. (1993), containing 45 different POS tags. We adopt the standard split: sections 0-18 for training, 19-21 for development and 22-24 for testing Collins (2002); Manning (2011).  \n \nFor multilingual POS tagging experiments, to compare with prior work, we use treebanks from Universal Dependencies (UD) v1.2 Nivre et al. (2015) (17 POS) with the given data splits. We experiment on languages for which pre-trained Polyglot word embeddings Al-Rfou et al. (2013) are available, resulting in 27 languages listed in Table 2. We regard languages with less than 60k tokens of training data as low-resource (Table 2, bottom), as in Plank et al. (2016).    \n 4.2 Training & Evaluation Details  \nModel settings. \n \nWe initialize word embeddings with 100-dimensional GloVe Pennington et al. (2014) for English, and with 64-dimensional Polyglot Al-Rfou et al. (2013) for other languages. We use 30-dimensional character embeddings, and set the state sizes of character \u200b/\u200b word-level BiLSTM to be 50, 200 for English, 50, 100 for low resource languages, and 50, 150 for other languages. The model parameters and character embeddings are randomly initialized, as in Ma and Hovy (2016). We apply dropout Srivastava et al. (2014) to input embeddings and BiLSTM outputs for both baseline and adversarial training, with dropout rate 0.5.    \nOptimization. \n \nWe train the model parameters and word \u200b/\u200b character embeddings by the mini-batch stochastic gradient descent (SGD) with batch size 10, momentum 0.9, initial learning rate 0.01 and decay rate 0.05. We also use a gradient clipping of 5.0 Pascanu et al. (2012). The models are trained with early stopping Caruana et al. (2001) based on the development performance.    \nEvaluation. \n \nWe evaluate per token tagging accuracy on test sets. We repeat the experiment three times and report the statistical significance.     \n 4.3 Results  \nPTB-WSJ dataset. \n \n   Model Accuracy   Toutanova et al. (2003) 97.27   Manning (2011) 97.28   Collobert et al. (2011) 97.29   S\u00f8gaard (2011) 97.50   Ling et al. (2015) 97.78   Ma and Hovy (2016) 97.55   Yang et al. (2017) 97.55    Hashimoto et al. (2017)  97.55   Ours \u2013 Baseline (BiLSTM-CRF) 97.54   Ours \u2013 Adversarial 97.58    \nTable 1: POS tagging accuracy on the PTB-WSJ test set, with other top-performing systems.   \n \nTable 1 shows the POS tagging results. As expected, our baseline (BiLSTM-CRF) model (accuracy 97.54%) performs on par with other state-of-the-art systems. Built upon this baseline, our adversarial training (AT) model reaches accuracy 97.58% thanks to its regularization power, outperforming recent POS taggers except Ling et al. (2015). The improvement over the baseline is statistically significant, with p\ud835\udc5dp-value  <<  0.05 on the t\ud835\udc61t-test. We provide additional analysis on this result in later sections.    \nMultilingual dataset (UD). \n \nFigure 2: Learning curves for three representative languages (Romanian is low-resource). We show the transition of loss (defined in Eq 1) on the development sets.   \n \nTable 2:  POS tagging accuracy (test) for 27 UD v1.2 treebanks, with other recent works, Plank et al. (2016), Berend (2017) and Nguyen et al. (2017). For Plank et al. (2016), we include the traditional baselines TNT and CRF, and their state-of-the-art model that employs a multi-task BiLSTM. Languages with \u2219 are morphologically rich, and those at the bottom (\u2018el\u2019 to \u2018ta\u2019) are low-resource, containing less than 60k tokens in their training sets.   \n \nExperimental results are summarized in Table 2. Our AT model shows clear advantages over the baseline in all of the 27 languages (average improvement \u223csimilar-to\\sim0.25%; see the two shaded columns). Considering that our baseline (BiLSTM-CRF) is already a top performing model for POS tagging, these improvements made by AT are substantial. The improvements are also statistically significant for all the languages, with p\ud835\udc5dp-value  <<  0.05 on the t\ud835\udc61t-test, suggesting that the regularization by AT is generally effective across different languages. Moreover, our AT model achieves state-of-the-art on nearly all of the languages, except the five where Plank et al. (2016)\u2019s multi-task BiLSTM yielded better results. Among the five, most languages are morphologically rich (\u2219).222We followed the criteria of morphological richness used in Nguyen et al. (2017). We suspect that their joint training of word rarity may be of particular help in processing morphologically complex words.  \n \n \n \n \n \nEnglish (WSJ)   \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 3240 7687 20908 97819 129654   Baseline 92.25 95.36 96.03 98.19 97.53    Adversarial 92.01 95.52 96.10 98.23 97.57              \n \n \nFrench (UD)   \n \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 356 839 1492  *4523*   *7210*    Baseline 87.64 94.05 94.03 98.43 96.48    Adversarial 87.92 94.88 94.03 98.50 96.63              \n  \nTable 3: POS tagging accuracy (test) on different subsets of words, categorized by their frequency of occurrence in training. The second row shows the number of tokens in the test set that are in each category. The third and fourth rows show the performance of our two models. Better scores are underlined. The biggest improvement is in bold.   \n \nAdditionally, we see that our AT model achieves notably large improvements over the baseline in resource-poor languages (the bottom of Table 2), with average improvement 0.35%, as compared to that for resource-rich languages, 0.20%. To further visualize the regularization effects, we present the learning curves for three representative languages, English (WSJ), French (UD-fr) and Romanian (UD-ro, low-resource), based on the development loss (see Figure 2). For all the three languages, we can observe that the AT model (red solid line) prevents overfitting better than the baseline (black dotted line), and this advantage is more significant in low resource languages. For example, in Romanian, the baseline model starts to increase development loss after 1,000 iterations even with dropout, whereas the AT model keeps improving until 2,500 iterations, achieving notably lower development loss (0.4 down). These results illustrate that AT can prevent overfitting especially well on small datasets and can augment the regularization power beyond dropout. AT can also be viewed as an effective means of data augmentation, where we generate and train with new examples the current model is particularly vulnerable to at every time step, enhancing the robustness of the model. AT can therefore be a promising tool to process low resource languages.      \n 5 Analysis \n \n \n \n \n \nEnglish (WSJ)   \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 6480 15374 41815 \u200b195637 259306   Baseline 97.76 97.71 97.80 97.45 97.53    Adversarial 98.06 97.71 97.89 97.47 97.57              \n \n \n \nFrench (UD)   \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 712  *1678*  2983  *9045*   *14418*    Baseline 95.08 97.08 97.58 96.11 96.48    Adversarial 95.37 97.26 97.79 96.23 96.63              \n  \nTable 4: POS tagging accuracy (test) on neighboring words. We cluster all words in the test set in the same way as Table 3 and consider the tagging performance on the neighbors (left and right) of these words in the test text.   \n \nIn the previous sections, we demonstrated the regularization power of adversarial training (AT) on different languages, based on the overall POS tagging performance and learning curves. In this section, we conduct further analyses on the robustness of AT from NLP specific aspects such as word statistics, sequence modeling, downstream tasks, and word representation learning.  \n \nWe find that AT can boost tagging accuracy on rare words and neighbors of unseen words (\u00a75.1). Furthermore, this robustness against rare \u200b/\u200b unseen words leads to better sentence-level accuracy and downstream dependency parsing (\u00a75.2). We illustrate these findings using two major languages, English (WSJ) and French (UD), which have substantially large training and testing data to discuss vocabulary statistics and sentence-level performance. Finally, we study the effects of AT on word representation learning (\u00a75.3), and the applicability of AT to different sequential tasks (\u00a75.4).   \n 5.1 Word-level Analysis \n \nPoor tagging accuracy on rare \u200b/\u200b unseen words is one of the bottlenecks in current POS taggers Manning (2011); Plank et al. (2016). Aiming to reveal the effects of AT on rare \u200b/\u200b unseen words, we analyze tagging performance at the word level, considering vocabulary statistics.   \nWord frequency. \n \nTo define rare \u200b/\u200b unseen words, we consider each word\u2019s frequency of occurrence in the training set. We categorize all words in the test set based on this frequency and study the test tagging accuracy for each group (see Table 3).333 To conduct the analysis, we picked the median result from the three repeated experiments. In both languages, the AT model achieves large improvements over the baseline on rare words (e.g., frequency 1-10 in training), as opposed to more frequent words. This result again corroborates the data augmentation power of AT under small training examples. On the other hand, we did not observe meaningful improvements on unseen words (frequency 0 in training). A possible explanation is that AT can facilitate the learning of words with at least a few occurrences in training (rare words), but is not particularly effective in inferring the POS tags of words for which no training examples are given (unseen words).    \nNeighboring words. \n \nOne important characteristic of natural language tasks is the sequential nature of inputs (i.e., sequence of words), where each word influences the function of its neighboring words. Since our model uses BiLSTM-CRF for that reason, we also study the tagging performance on the neighbors of rare \u200b/\u200b unseen words, and analyze the effects of AT with the sequence model in mind. In Table 4, we cluster all words in the test set based on their frequency in training again, and consider the tagging accuracy on the neighbors (left and right) of these words in the test text. We observe that AT tends to achieve large improvements over the baseline on the neighbors of unseen words (training frequency 0), while the improvements on the neighbors of more frequent words remain moderate. Our AT model thus exhibits strong stability to uncertain neighbors, as compared to the baseline. We suspect that because we generate adversarial examples against entire input sentences, training with adversarial examples makes the model more robust not only to perturbations in each word but also to perturbations in its neighboring words, leading to greater stability to uncertain neighbors.     \n 5.2 Sentence-level & Downstream Analysis \n \n \n \n \n \nEnglish (WSJ)   \n \n \n        Sentence-         \n \nStanford Parser          \n \nParsey McParseface        level Acc.\u200b  UAS LAS    UAS LAS      Baseline 59.08 91.53 89.30 91.68 87.92   Adversarial 59.61 91.57 89.35 91.73 87.97    \n \n(w/ gold tags)\u200b  \u2013 (92.07) (90.63) (91.98) (88.60)              \n \n \n \nFrench (UD)   \n \n \n        Sentence-         \n \nParsey Universal        level Acc.\u200b    UAS LAS      Baseline 52.35 84.85 80.36   Adversarial 53.36 85.01 80.55    \n \n(w/ gold tags)\u200b  \u2013 (85.05) (80.75)            \n  \nTable 5: Sentence-level accuracy and downstream dependency parsing performance by our baseline \u200b/\u200b adversarial POS taggers.   \n \nIn the word-level analysis, we showed that AT can boost tagging accuracy on rare words and the neighbors of unseen words, enhancing overall robustness on rare \u200b/\u200b unseen words. In this section, we discuss the benefit of our improved POS tagger in a major downstream task, dependency parsing.  \n \nMost of the recent state-of-the-art dependency parsers take predicted POS tags as input (e.g. Chen and Manning (2014); Andor et al. (2016); Dozat and Manning (2017)). Dozat et al. (2017) empirically show that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger, while POS tags predicted by the UDPipe tagger Straka et al. (2016) do not contribute to parsing performance as much. This observation illustrates that POS tagging performance has a great influence on dependency parsing, motivating the hypothesis that the POS tagging improvements gained from our adversarial training help dependency parsing.  \n \nTo test the hypothesis, we consider three settings in dependency parsing of English and French: using POS tags predicted by the baseline model, using POS tags predicted by the AT model, and using gold POS tags. For English (PTB-WSJ), we first convert the treebank into Stanford Dependencies (SD) using Stanford CoreNLP (ver 3.8.0) Manning et al. (2014), and then apply two well-known dependency parsers: Stanford Parser (ver 3.5.0) Chen and Manning (2014) and Parsey McParseface (SyntaxNet) Andor et al. (2016). For French (UD), we use Parsey Universal from SyntaxNet. The three parsers are all publicly available and pre-trained on corresponding treebanks.  \n \nTable 5 shows the results of the experiments. We can observe improvements in both languages by using the POS tags predicted by our AT POS tagger. As Manning (2011) points out, when predicted POS tags are used for downstream dependency parsing, a single bad mistake in a sentence can greatly damage the usefulness of the POS tagger. The robustness of our AT POS tagger against rare \u200b/\u200b unseen words helps to mitigate such an issue. This advantage can also be observed from the AT POS tagger\u2019s notably higher sentence-level accuracy than the baseline (see Table 5 left). Nonetheless, gold POS tags still yield better parsing results as compared to the baseline \u200b/\u200b AT POS taggers, supporting the claim that POS tagging needs further improvement for downstream tasks.    \n 5.3 Effects on Representation Learning \n \n \n \n \n \nEnglish (WSJ)   \n \n      POS Cluster NN VB JJ RB Avg.      1) Initial \n \n(GloVe)  0.243 0.426 0.220 0.549 0.359   2) Baseline 0.280 0.431 0.309 0.667 0.422   3) Adversarial 0.281 0.436 0.306 0.675 0.424              \n \n \nFrench (UD)   \n \n \n      POS Cluster NOUN VERB ADJ ADV Avg.      1) Initial \n \n(polyglot)   0.215 0.233 0.210 0.540 0.299   2) Baseline 0.258 0.271 0.262 0.701 0.373   3) Adversarial 0.263 0.272 0.263 0.720 0.379              \n  \nTable 6:  Cluster tightness evaluation for word embeddings, based on the cosine similarity measure. Higher scores indicate better clustering (cleaner word vector distribution). Each row corresponds to word vectors 1) at the beginning, 2) after baseline training, and 3) after adversarial training.   \n \n \n \n \nEnglish (WSJ)   \n \n \n      \n \nPerturbation scale \u03b1\ud835\udefc\\alpha  0 0.001 0.01 0.05 0.1 0.5      \n \nAvg. cluster tightness   0.422 0.423 0.424 0.429 0.436 0.429               \n  \nTable 7: Average cluster tightness for word embeddings trained with varied perturbation scale \u03b1\ud835\udefc\\alpha (0 indicates baseline training).   \n \nNext, we perform an analysis on representation learning of words (word embeddings) for the English (PTB-WSJ) and French (UD) experiments. We hypothesize that adversarial training (AT) helps to learn better word embeddings so that the POS tag prediction of a word cannot be influenced by a small perturbation in the input embedding.  \n \nTo verify this hypothesis, we cluster all words in the test set based on their correct POS tags444We excluded words with multiple tags in the test text. and evaluate the tightness of the word vector distribution within each cluster. We compare this clustering quality among the three settings: 1) beginning (initialized with GloVe or Polyglot), 2) after baseline training (50 epochs), and 3) after adversarial training (50 epochs), to study the effects of AT on word representation learning.  \n \nFor evaluating the tightness of word vector distribution, we employ the cosine similarity metric, which is widely used as a measure of the closeness between two word vectors (e.g., Mikolov et al. (2013); Pennington et al. (2014)). To measure the tightness of each cluster, we compute the cosine similarity for every pair of words within, and then take the average. We also report the average tightness across all the clusters.  \n \nThe evaluation results are summarized in Table 6. We report the tightness scores for the four major clusters: noun, verb, adjective, and adverb (from left to right). As can be seen from the table, for both languages, adversarial training (AT) results in cleaner word embedding distributions than the baseline, with a higher cosine similarity within each POS cluster, and with a clear advantage in the average tightness across all the clusters. In other words, the learned word vectors show stronger correlations with their POS tags. This result confirms that training with adversarial examples can help to learn cleaner word embeddings so that the meaning \u200b/\u200b grammatical function of a word cannot be altered by a small perturbation in its embedding. This analysis provides a means to interpret the robustness to input perturbations, from the perspective of NLP.   \nRelation with perturbation size \u03f5bold-italic-\u03f5\\boldsymbol{\\epsilon}.  \n \nWe also study how the size of added perturbations influences word representation learning in adversarial training. Recall that we set the norm of a perturbation \u03f5italic-\u03f5\\epsilon to be \u03b1\u200bD\ud835\udefc\ud835\udc37\\alpha\\mbox{\\scalebox{0.95}[0.95]{$\\sqrt{D}$}}, where D\ud835\udc37D is the dimension of the concatenated input embeddings (see \u00a73.2). For instance, \u03b1=0\ud835\udefc0\\alpha\\!=\\!0 would produce no noise; \u03b1=1\ud835\udefc1\\alpha\\!=\\!1 would generate a perturbation of a norm equivalent to the original word embeddings. We hypothesize that AT facilitates word representation learning when \u03b1\ud835\udefc\\alpha is small enough to preserve the semantics of input words, but can hinder the learning when \u03b1\ud835\udefc\\alpha is too large. To test the hypothesis, we repeat the clustering evaluation for word embeddings trained with varied perturbation scale \u03b1\ud835\udefc\\alpha: 0, 0.001, 0.01, 0.05, 0.1, 0.5 (see Table 7). We observe that the quality of learned word embedding distribution keeps improving as \u03b1\ud835\udefc\\alpha goes up from 0 to 0.1, but starts to drop around \u03b1=0.5\ud835\udefc0.5\\alpha\\!=\\!0.5. We also find that this optimal \u03b1\ud835\udefc\\alpha in word embedding learning (i.e., 0.1) is larger than the \u03b1\ud835\udefc\\alpha which yielded the best tagging performance on development sets (i.e., 0.01 or 0.05). A possible explanation is that while word embeddings can adapt to relatively large \u03b1\ud835\udefc\\alpha (e.g., 0.1) during training, as adversarial perturbations are generated at the embedding level, such \u03b1\ud835\udefc\\alpha could change the semantics of the input from the current tagging model\u2019s perspective and hinder the training of tagging.     \n 5.4 Other Sequence Labeling Tasks \n \nFinally, to further confirm the applicability of AT, we experiment with our BiLSTM-CRF AT model in different sequence labeling tasks: chunking and named entity recognition (NER).  \n \n   Model F1   Tsuruoka et al. (2011) 93.81   Collobert et al. (2011) 94.32   Yang et al. (2017) 94.66   Suzuki and Isozaki (2008) 95.15   S\u00f8gaard and Goldberg (2016) 95.56   Hashimoto et al. (2017) 95.77    Peters et al. (2017)  96.37   Ours \u2013 Baseline (BiLSTM-CRF) 95.18   Ours \u2013 Adversarial 95.25    \nTable 8: Chunking F1 scores on the CoNLL-2000 task, with other top performing models.  \n \nChunking can be performed as a sequence labeling task that assigns a chunking tag (B-NP, I-VP, etc.) to each word. We conduct experiments on the CoNLL 2000 shared task with the standard data split: PTB-WSJ Sections 15-18 for training and 20 for testing. We use Section 19 as the development set and employ the IOBES tagging scheme, following Hashimoto et al. (2017).  \n \n   Model F1   Collobert et al. (2011) 89.59   Huang et al. (2015) 90.10   Chiu and Nichols (2016) 90.91   Lample et al. (2016) 90.94   Luo et al. (2015) 91.20   Ma and Hovy (2016) 91.21    Peters et al. (2017)  91.93   Ours \u2013 Baseline (BiLSTM-CRF) 91.22   Ours \u2013 Adversarial 91.56    \nTable 9: NER F1 scores on the CoNLL-2003 (English) task, with other top performing models.  \n \nNER aims to assign an entity type to each word, such as person, location, organization, and misc. We conduct experiments on the CoNLL-2003 (English) shared task Tjong Kim Sang and De Meulder (2003), adopting the IOBES tagging scheme as in Lample et al. (2016); Ma and Hovy (2016).  \n \nThe results are summarized in Table 8 and 9. AT enhanced F1 score from the baseline BiLSTM-CRF model\u2019s 95.18 to 95.25 for chunking, and from 91.22 to 91.56 for NER, also significantly outperforming Ma and Hovy (2016). These improvements made by AT are bigger than that for English POS tagging, most likely due to the larger room for improvement in chunking and NER. The improvements are again statistically significant, with p\ud835\udc5dp-value  <<  0.05 on the t\ud835\udc61t-test. The experimental results suggest that the proposed adversarial training scheme is generally effective across different sequence labeling tasks.  \n \nOur BiLSTM-CRF AT model did not reach the performance by Hashimoto et al. (2017)\u2019s multi-task model and Peters et al. (2017)\u2019s state-of-the-art system that incorporates pretrained language models. It would be interesting future work to combine the strengths of these joint models (e.g., syntactic and semantic aids) and adversarial training (e.g., robustness).     \n 6 Conclusion \n \nWe proposed and carefully analyzed a POS tagging model that exploits adversarial training (AT). In our multilingual experiments, we find that AT achieves substantial improvements on all the languages tested, especially on low resource ones. AT also enhances the robustness to rare \u200b/\u200b unseen words and sentence-level accuracy, alleviating the major issues of current POS taggers, and contributing to the downstream task, dependency parsing. Furthermore, our analyses on different languages, word \u200b/\u200b neighbor statistics and word representation learning reveal the effects of AT from the perspective of NLP. The proposed AT model is applicable to general sequence labeling tasks. This work therefore provides a strong basis and motivation for utilizing AT in natural language tasks.     \nAcknowledgements \n \nWe would like to thank Rui Zhang, Jonathan Kummerfeld, Yutaro Yamada, as well as all the anonymous reviewers for their helpful feedback and suggestions on this work.  \n   \nReferences \n \n Al-Rfou et al. (2013)  Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.  Polyglot: Distributed word representations for multilingual nlp.  In CoNLL.   \n Andor et al. (2016)  Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016.  Globally normalized transition-based neural networks.  In ACL.   \n Bahdanau et al. (2015)  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.  Neural machine translation by jointly learning to align and translate.  In ICLR.   \n Berend (2017)  G\u00e1bor Berend. 2017.  Sparse coding of neural word embeddings for multilingual sequence labeling.  TACL .   \n Caruana et al. (2001)  Rich Caruana, Steve Lawrence, and C Lee Giles. 2001.  Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping.  In NIPS.   \n Chen and Manning (2014)  Danqi Chen and Christopher D Manning. 2014.  A fast and accurate dependency parser using neural networks.  In EMNLP.   \n Chiu and Nichols (2016)  Jason PC Chiu and Eric Nichols. 2016.  Named entity recognition with bidirectional lstm-cnns.  In TACL.   \n Collins (2002)  Michael Collins. 2002.  Discriminative training methods for hidden markov models.  In EMNLP.   \n Collobert et al. (2011)  Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.  Natural language processing (almost) from scratch.  The Journal of Machine Learning Research 12:2493\u20132537.   \n Dozat and Manning (2017)  Timothy Dozat and Christopher D. Manning. 2017.  Deep biaffine attention for neural dependency parsing.  In ICLR.   \n Dozat et al. (2017)  Timothy Dozat, Peng Qi, and Christopher D. Manning. 2017.  Stanford\u2019s graph-based neural dependency parser at the conll 2017 shared task.  In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. pages 20\u201330.   \n Goodfellow et al. (2014)  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.  Generative adversarial nets.  In NIPS.   \n Goodfellow et al. (2015)  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015.  Explaining and harnessing adversarial examples.  In ICLR.   \n Gui et al. (2017)  Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and Xuanjing Huang. 2017.  Part-of-speech tagging for twitter with adversarial neural networks.  In EMNLP.   \n Hashimoto et al. (2017)  Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017.  A joint many-task model: Growing a neural network for multiple NLP tasks.  In EMNLP.   \n Hochreiter and Schmidhuber (1997)  Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.  Long short-term memory.  Neural Computation 9(8):1735\u20131780.   \n Huang et al. (2015)  Zhiheng Huang, Wei Xu, and Kai Yu. 2015.  Bidirectional lstm-crf models for sequence tagging.  arXiv preprint arXiv:1508.01991 .   \n Iyyer et al. (2015)  Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum\u00e9 III. 2015.  Deep unordered composition rivals syntactic methods for text classification.  In ACL.   \n Jia and Liang (2017)  Robin Jia and Percy Liang. 2017.  Adversarial examples for evaluating reading comprehension systems.  In EMNLP.   \n Kim et al. (2017)  Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and Eric Fosler-Lussier. 2017.  Cross-lingual transfer learning for pos tagging without cross-lingual resources.  In EMNLP.   \n Lafferty et al. (2001)  John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001.  Conditional random fields: Probabilistic models for segmenting and labeling sequence data.  In ICML.   \n Lample et al. (2016)  Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016.  Neural architectures for named entity recognition.  In NAACL.   \n Li et al. (2017)  Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. 2017.  Adversarial learning for neural dialogue generation.  In EMNLP.   \n Ling et al. (2015)  Wang Ling, Tiago Lu\u00eds, Lu\u00eds Marujo, R\u00e1mon Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015.  Finding function in form: Compositional character models for open vocabulary word representation.  In EMNLP.   \n Liu et al. (2017)  Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017.  Delving into transferable adversarial examples and black-box attacks.  In ICLR.   \n Luo et al. (2015)  Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Zaiqing Nie. 2015.  Joint entity recognition and disambiguation.  In EMNLP.   \n Ma and Hovy (2016)  Xuezhe Ma and Eduard Hovy. 2016.  End-to-end sequence labeling via bi-directional lstm-cnns-crf.  In ACL.   \n Manning et al. (2014)  Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014.  The Stanford CoreNLP natural language processing toolkit.  In Association for Computational Linguistics (ACL) System Demonstrations.   \n Manning (2011)  Christopher D Manning. 2011.  Part-of-speech tagging from 97% to 100%: is it time for some linguistics?  Computational Linguistics and Intelligent Text Processing pages 171\u2013189.   \n Marcus et al. (1993)  Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.  Building a large annotated corpus of english: The penn treebank.  Computational Linguistics 19(2):313\u2013330.   \n Mikolov et al. (2013)  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.  Distributed representations of words and phrases and their compositionality.  In NIPS.   \n Miyato et al. (2017)  Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2017.  Adversarial training methods for semi-supervised text classification.  In ICLR.   \n Nguyen et al. (2017)  Dat Quoc Nguyen, Mark Dras, and Mark Johnson. 2017.  A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing.  In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.   \n Nivre et al. (2015)  Joakim Nivre, \u017deljko Agi\u0107, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina Bosco, Sam Bowman, Giuseppe G. A. Celano, Miriam Connor, Marie-Catherine de Marneffe, Arantza Diaz de Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Toma\u017e Erjavec, Rich\u00e1rd Farkas, Jennifer Foster, Daniel Galbraith, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Berta Gonzales, Bruno Guillaume, Jan Haji\u010d, Dag Haug, Radu Ion, Elena Irimia, Anders Johannsen, Hiroshi Kanayama, Jenna Kanerva, Simon Krek, Veronika Laippala, Alessandro Lenci, Nikola Ljube\u0161i\u0107, Teresa Lynn, Christopher Manning, C\u0103t\u0103lina M\u0103r\u0103nduc, David Mare\u010dek, H\u00e9ctor Mart\u00ednez Alonso, Jan Ma\u0161ek, Yuji Matsumoto, Ryan McDonald, Anna Missil\u00e4, Verginica Mititelu, Yusuke Miyao, Simonetta Montemagni, Shunsuke Mori, Hanna Nurmi, Petya Osenova, Lilja \u00d8vrelid, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen, Barbara Plank, Martin Popel, Prokopis Prokopidis, Sampo Pyysalo, Loganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simk\u00f3, Kiril Simov, Aaron Smith, Jan \u0160t\u011bp\u00e1nek, Alane Suhr, Zsolt Sz\u00e1nt\u00f3, Takaaki Tanaka, Reut Tsarfaty, Sumire Uematsu, Larraitz Uria, Viktor Varga, Veronika Vincze, Zden\u011bk \u017dabokrtsk\u00fd, Daniel Zeman, and Hanzhi Zhu. 2015.  Universal dependencies 1.2.  LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University.   \n Pascanu et al. (2012)  Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012.  On the difficulty of training recurrent neural networks.  arXiv preprint arXiv:1211.5063 .   \n Pennington et al. (2014)  Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.  Glove: Global vectors for word representation.  In EMNLP.   \n Peters et al. (2017)  Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017.  Semi-supervised sequence tagging with bidirectional language models.  In ACL.   \n Plank et al. (2016)  Barbara Plank, Anders S\u00f8gaard, and Yoav Goldberg. 2016.  Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss.  In ACL.   \n Schuster and Paliwal (1997)  M. Schuster and K.K. Paliwal. 1997.  Bidirectional recurrent neural networks.  Trans. Sig. Proc. 45(11):2673\u20132681.   \n Shaham et al. (2015)  Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2015.  Understanding adversarial training: Increasing local stability of neural nets through robust optimization.  arXiv preprint arXiv:1511.05432 .   \n S\u00f8gaard (2011)  Anders S\u00f8gaard. 2011.  Semi-supervised condensed nearest neighbor for part-of-speech tagging.  In ACL-HLT.   \n S\u00f8gaard and Goldberg (2016)  Anders S\u00f8gaard and Yoav Goldberg. 2016.  Deep multi-task learning with low level tasks supervised at lower layers.  In ACL.   \n Srivastava et al. (2014)  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.  Dropout: A simple way to prevent neural networks from overfitting.  Journal of Machine Learning Research 15:1929\u20131958.   \n Straka et al. (2016)  Milan Straka, Jan Hajic, and Jana Strakov\u00e1. 2016.  Udpipe: Trainable pipeline for processing conll-u files performing tokenization, morphological analysis, pos tagging and parsing.  In LREC.   \n Suzuki and Isozaki (2008)  Jun Suzuki and Hideki Isozaki. 2008.  Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.  ACL-HLT .   \n Szegedy et al. (2014)  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014.  Intriguing properties of neural networks.  In ICLR.   \n Tjong Kim Sang and De Meulder (2003)  Erik F Tjong Kim Sang and Fien De Meulder. 2003.  Introduction to the conll-2003 shared task: Language-independent named entity recognition.  In CoNLL.   \n Toutanova et al. (2003)  Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003.  Feature-rich part-of-speech tagging with a cyclic dependency network.  In HLT-NAACL.   \n Tsuruoka et al. (2011)  Yoshimasa Tsuruoka, Yusuke Miyao, and Jun\u2019ichi Kazama. 2011.  Learning with lookahead: Can history-based models rival globally optimized models?  In CoNLL.   \n Wang et al. (2017)  Beilun Wang, Ji Gao, and Yanjun Qi. 2017.  A theoretical framework for robustness of (deep) classifiers against adversarial samples.  In ICLR.   \n Wu et al. (2017)  Yi Wu, David Bamman, and Stuart Russell. 2017.  Adversarial training for relation extraction.  In EMNLP.   \n Xie et al. (2017)  Ziang Xie, Sida I Wang, Jiwei Li, Daniel Levy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. 2017.  Data noising as smoothing in neural network language models.  In ICLR.   \n Yang et al. (2017)  Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2017.  Transfer learning for sequence tagging with hierarchical recurrent networks.  In ICLR.       \n\u25c4  Feeling\nlucky?  Conversion\nreport Report\nan issue View original\non arXiv\u25ba \n  Copyright Privacy Policy \nGenerated on Sun Mar 3 20:57:27 2024 by LaTeXML     ", "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training \u2013 arXiv Vanity", "description": "<strong>Adversarial</strong> <strong>training</strong> (AT)111We distinguish this from <strong>the</strong> <strong>Generative</strong> <strong>Adversarial</strong> Networks (GANs). is a powerful regularization <strong>method</strong> <strong>for</strong> neural networks, aiming to achieve <strong>robustness</strong> <strong>to</strong> input perturbations. Yet, the specific effects of <strong>the</strong> <strong>robustness</strong> obtained by AT are still unclear in the ...", "usage": {"tokens": 14193}, "retrieved_text": [" \n \n \n \nRobust Multilingual Part-of-Speech Tagging via Adversarial Training \n  Michihiro Yasunaga Jungo Kasai Dragomir Radev \nDepartment of Computer Science, Yale University \n  {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu    \n \nAbstract \nAdversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. 1 Introduction \n \nFigure 1: Illustration of our architecture for adversarial POS tagging.", "Robust Multilingual Part-of-Speech Tagging via Adversarial Training Michihiro Yasunaga Jungo Kasai Dragomir Radev Department of Computer Science, Yale University {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu Abstract Adversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. 1 Introduction Figure 1: Illustration of our architecture for adversarial POS tagging. Given a sentence, we input the normalized word embeddings (\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0,\ud835\udc98\ud835\udfd1subscript\ud835\udc981subscript\ud835\udc982subscript\ud835\udc983\\mbox{\\boldmath$\\uline{w}_{1}$},\\mbox{\\boldmath$\\uline{w}_{2}$},\\mbox{\\boldmath$\\uline{w}_{3}$}) and character embeddings (showing \ud835\udc84\ud835\udfcf,\ud835\udc84\ud835\udfd0,\ud835\udc84\ud835\udfd1subscript\ud835\udc841subscript\ud835\udc842subscript\ud835\udc843\\mbox{\\boldmath$\\uline{c}_{1}$},\\mbox{\\boldmath$\\uline{c}_{2}$},\\mbox{\\boldmath$\\uline{c}_{3}$} for w1subscript\ud835\udc641w_{1}).", "core problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training. With a BiLSTM-CRF model Huang et al. (2015); Ma and Hovy (2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word \u200b/\u200b character embeddings. In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset (27 languages), with thorough analyses of the following points: \u2022 Effects on different target languages \u2022 Vocabulary statistics and tagging accuracy \u2022 Influence on downstream tasks \u2022 Representation learning of words In our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages. Furthermore, our analyses reveal the following insights into adversarial training in the context of NLP:  \n \n \n \u2022 \n \nThe regularization effects of adversarial training (AT) are general across different languages. AT can prevent overfitting especially well when training examples are scarce, providing an effective tool to process low resource languages."]}, {"text": " \n \n Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. \n  Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    26hzhang   /  DL-NLP-Readings  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 268   \n \n   Star  850      \n   \n \n   Code   \n   Issues 0  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Security    \n      Insights             \n   \n       \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time.      \n     \n    \n \n    \n            \n            \n \n ", "title": "DL-NLP-Readings/Robust Multilingual Part-of-Speech Tagging via ...", "description": "My Reading Lists of Deep Learning and Natural <strong>Language</strong> Processing - DL-NLP-Readings/<strong>Robust</strong> <strong>Multilingual</strong> <strong>Part</strong>-<strong>of</strong>-<strong>Speech</strong> <strong>Tagging</strong> <strong>via</strong> <strong>Adversarial</strong> <strong>Training</strong>.bib at master \u00b7 IsaacChanghau/DL-NLP-Readings", "usage": {"tokens": 339}, "retrieved_text": [" \n \n Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    26hzhang   /  DL-NLP-Readings  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 268   \n \n   Star  850      \n   \n \n   Code   \n   Issues 0  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Security    \n      Insights             \n   \n       \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time."]}, {"text": "   \n \n1 Introduction \n 2 Related Work \n \nRetrieval-Augmented Generation with Noisy Context \nAdversarial Training   \n 3 Methodology \n \n3.1 Problem Setup \n3.2 Diverse Retrieval Noises \n3.3 Retrieval-augmented Adaptive Adversarial Training \n3.4 Incorporating Noise Awareness   \n 4 Experiments \n \n4.1 Dataset Construction \n4.2 Evaluation Metrics \n 4.3 Baseline Methods \n \nZero-Shot Methods \nFine-tuning Methods   \n4.4 Implementation Details   \n 5 Experimental Results \n \n5.1 Main Results \n5.2 Ablation Study \n 5.3 Further Discussion \n \nWhat types of adversarial samples does RAAT employ during training?     \n6 Conclusion \n7 Limitations \nA Has the Model Truly Attained Noise Awareness?   \n \n \n \nEnhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training \n  Feiteng Fang1,2, Yuelin Bai211footnotemark: 1, Shiwen Ni2, Min Yang222footnotemark: 2, Xiaojun Chen3, Ruifeng Xu4 \n1University of Science and Technology of China \n2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences \n3Shenzhen University 4Harbin Institute of Technology (Shenzhen) \nfeitengfang@mail.ustc.edu.cn, {yl.bai, sw.ni, min.yang}@siat.ac.cn, \nxjchen@szu.edu.cn, xuruifeng@hit.edu.cn Equal contribution.Corresponding author.  \n \nAbstract \nLarge Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs\u2019 capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model\u2019s training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model\u2019s capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.  \n \n \nEnhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training \n \n   Feiteng Fang1,2\u2020\u2020thanks: Equal contribution., Yuelin Bai211footnotemark: 1, Shiwen Ni2\u2020\u2020thanks: Corresponding author., Min Yang222footnotemark: 2, Xiaojun Chen3, Ruifeng Xu4  1University of Science and Technology of China  2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences  3Shenzhen University 4Harbin Institute of Technology (Shenzhen)  feitengfang@mail.ustc.edu.cn, {yl.bai, sw.ni, min.yang}@siat.ac.cn,  xjchen@szu.edu.cn, xuruifeng@hit.edu.cn  \n    \n 1 Introduction \n \nFigure 1:  An illustrative example of the RAG process applied to question answering. The model predicts the correct answer with accurate retrieved text. However, it fails to produce the right answer when the retrieved text contains misleading or inaccurate information.   \n \nLarge language models (LLMs) have garnered substantial attention in both academic and industrial research within the domain of artificial intelligence due to their remarkable capabilities (Brown et al., 2020; Bubeck et al., 2023). Despite their immense power, LLMs face challenges such as hallucinations and outdated knowledge (Gao et al., 2023). Moreover, a lack of domain knowledge may hinder their performance on domain-specific tasks (Kandpal et al., 2023). To mitigate these challenges, recent studies improve LLMs by retrieving passages from external databases and pretending them in context, constituting a framework known as retrieval-augmented language models (RALMs) (Mao et al., 2020; Lewis et al., 2020).  \n \nHowever, RALMs also present significant limitations. Previous studies (Yoran et al., 2023; Yu et al., 2023; Shi et al., 2023) have empirically demonstrated that retrieved noisy passages are problematic for LLMs, resulting in performance degradation. We term this issue as the noise robustness problem of RALMs. As illustrated in Figure 1, the model can provide correct answers when the retrieving context is accurate and related to the query. However, when the retrieved context contains misleading or inaccurate information, the model may yield incorrect answers. As the retriever inherently cannot achieve complete accuracy, the presence of noise in the retrieved context is inevitable. Therefore, designing robust algorithms against retrieved noises is of great practical importance.  \n \nRecently, several studies (Yoran et al., 2023; Li et al., 2022) have attempted to enhance the noise robustness of RALMs through noisy training, which involves incorporating retrieved noisy contexts into fine-tuning data. While noisy training exhibits promise, its effectiveness heavily relies on the composition of the training dataset. Incorrectly introducing noises to the training data can lead to model overfitting, adversely affecting generalization. In practice, meticulous adjustment of the type and intensity of noises is essential to ensure the model\u2019s proficiency across various tasks and datasets. This demands significant experimentation and tuning, adding complexity to the development process. Moreover, the lack of clear classification for retrieval noises in current studies stands in contrast to the diverse range of noises encountered in real retrieval environments.  \n \nThis paper systematically explores three types of retrieval noises: (i) contexts that are superficially related to the query but lack the correct answer (Relevant retrieval noise), (ii) contexts that are irrelevant to the query (Irrelevant retrieval noise), and (iii) contexts that are topically related to the query but contain incorrect information (Counterfactual retrieval noise). Our empirical study indicates that LLMs exhibit varying robustness to these three types of noise. Compared to entirely irrelevant texts, texts that are superficially related to the query or those containing counterfactual details often lead to more misinformation.  \n \nIn response to diverse types of noises, we propose a novel approach named Retrieval-augmented Adaptive Adversarial Training (RAAT), which employs adaptive adversarial training to dynamically regulate the model\u2019s training process in response to retrieved noisy texts. Concretely, RAAT generates adversarial samples (noises) by considering the model\u2019s sensitivity to different types of noises during training, which aligns with the min-max paradigm of adversarial training (Morris et al., 2020; Ivgi and Berant, 2021). Moreover, RAAT utilizes multi-task learning (Ruder, 2017) to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance.  \n \nThe main contributions of this paper can be summarized as follows: \n \n \u2022 \n \nWe systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises.   \n \u2022 \n \nWe propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises.    \n \u2022 \n \nWe set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments.       \n 2 Related Work  \nRetrieval-Augmented Generation with Noisy Context \n \nRetrieval-Augmented Language Models (RALMs) have shown impressive performance in various NLP tasks (Gao et al., 2023; Zhu et al., 2023). However, limited by the capabilities of the retriever, retrieval-augmented systems inevitably introduce irrelevant or partially relevant knowledge to the models (Yin et al., 2023). Recent studies (Yu et al., 2023; Yoran et al., 2023; Chen et al., 2023) have increasingly focused on the impact of noisy information on retrieval-augmented generation. For example, Jia and Liang (2017); Creswell et al. (2022) observed that adding irrelevant noise to the context could detrimentally affect model performance. Chen et al. (2023) demonstrated that as the proportion of noise in the retrieval context increases, the performance of LLMs experiences a notable decline. Similar phenomena have been reported by  Yoran et al. (2023) and  Thakur et al. (2023).    \nAdversarial Training \n \nAdversarial training is recognized as a crucial method for enhancing model robustness, initially proposed by Goodfellow et al. (2014). Early studies widely investigated adversarial training in the computer vision domain (Kurakin et al., 2016; Madry et al., 2017). In the NLP domain,  Miyato et al. (2016) applied perturbations to word embeddings, making the model less prone to overfitting. Similarly, perturbations on different granularities have been extensively studied, encompassing various aspects of NLP tasks (Yasunaga et al., 2017; Wu et al., 2017; Zhu et al., 2019; Wang et al., 2020; Ni et al., 2023; Liang et al., 2023).  \n \nRecently, several studies have concentrated on generating adversarial examples designed to induce LLMs to generate harmful or non-factual content (Zou et al., 2023; Shen et al., 2023) instead of merely causing the model to make inaccurate predictions. Shen et al. (2023) employed decision-based perturbation at different levels to craft adversarial examples, revealing vulnerabilities in ChatGPT to both sentence-level and character-level adversarial attacks. Shi et al. (2023) added irrelevant context to an arithmetic reasoning dataset, finding that including irrelevant information distracted the model\u2019s predictions. Zou et al. (2023) proposed a method that could reliably generate adversarial attack suffixes, yielding adversarial prompts that exhibit high transferability.  \n \nIn this study, we investigate adversarial training concerning LLMs in response to various retrieval noises, aiming to efficiently obtain adversarial examples that enhance model robustness while reducing training overhead. We construct noisy adversarial examples by sampling or paraphrasing the original dataset. This approach ensures more dependable and precise outputs even when confronted with imperfect retrieved contexts.     \n 3 Methodology  \n 3.1 Problem Setup \n \nIn the standard RALM, given input query x\ud835\udc65xitalic_x, a retriever r\ud835\udc5fritalic_r is designed to retrieve relevant contexts C={c1,c2,\u2026}\ud835\udc36subscript\ud835\udc501subscript\ud835\udc502\u2026C=\\{c_{1},c_{2},\\ldots\\}italic_C = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 } from an external database. During inference, the content of retrieval context is concatenated with x\ud835\udc65xitalic_x to form d\ud835\udc51ditalic_d, which is then fed into the pre-trained language model M\ud835\udc40Mitalic_M, yielding a response y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG regarding x\ud835\udc65xitalic_x. If the retrieved context c\ud835\udc50citalic_c contains the correct answer y\ud835\udc66yitalic_y about x\ud835\udc65xitalic_x, we can denote c\ud835\udc50citalic_c as cg\u2062o\u2062l\u2062d\u2062e\u2062nsubscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bc_{golden}italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT, representing the golden retrieval context. However, if c\ud835\udc50citalic_c does not contain the correct answer y\ud835\udc66yitalic_y or contains partially irrelevant content, we can denote c\ud835\udc50citalic_c as cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT.  \n \nIn our study, we transform open-domain question answering (QA) into a reading comprehension task to meet the RAG settings. Formally, given the objective f\ud835\udc53fitalic_f of an open domain question answering task is f:{x}\u2192y:\ud835\udc53\u2192\ud835\udc65\ud835\udc66f:\\left\\{x\\right\\}\\rightarrow yitalic_f : { italic_x } \u2192 italic_y, we can formulate the objective of the reading comprehension task as f:{cg\u2062o\u2062l\u2062d\u2062e\u2062n,x}\u2192y:\ud835\udc53\u2192subscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc65\ud835\udc66f:\\left\\{c_{golden},x\\right\\}\\rightarrow yitalic_f : { italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT , italic_x } \u2192 italic_y. In examining the challenge of the retrieval noise robustness problem of RALM, we aim to obtain a fine-tuned model M\u2032superscript\ud835\udc40\u2032M^{\\prime}italic_M start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT that can not only fulfill the function f:{cg\u2062o\u2062l\u2062d\u2062e\u2062n,x}\u2192y:\ud835\udc53\u2192subscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc65\ud835\udc66f:\\left\\{c_{golden},x\\right\\}\\rightarrow yitalic_f : { italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT , italic_x } \u2192 italic_y but also produce accurate answers even in the presence of additional retrieval noise cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT and achieve function f:{cg\u2062o\u2062l\u2062d\u2062e\u2062n,cn\u2062o\u2062i\u2062s\u2062y,x}\u2192y:\ud835\udc53\u2192subscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bsubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66\ud835\udc65\ud835\udc66f:\\left\\{c_{golden},c_{noisy},x\\right\\}\\rightarrow yitalic_f : { italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT , italic_x } \u2192 italic_y.  \n \nFigure 2: Exact match (EM) scores of various models under different types of retrieval noises. \u201cGolden Context\u201d denotes instances where LLMs respond to questions with reference to the golden retrieval context. \u201cNo Noise\u201d indicates instances where LLMs answer questions without any retrieval. The experimental configurations of other models involve the introduction of different types of noises on the foundation of the \u201cGolden Context\u201d.    \n 3.2 Diverse Retrieval Noises \n \nWe systematically classify the retrieval noise present in cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT to closely mimic real-world conditions. Existing studies (Yoran et al., 2023; Yu et al., 2023) on retrieval noise robustness often dichotomize noise into relevant and irrelevant categories. However, we contend that such a classification may not fully align with the retrieval noise robustness of RALMs. In this work, we propose a more nuanced classification of retrieval noise, differentiating it into three distinct types: Relevant retrieval noise, Irrelevant retrieval noise, and Counterfactual retrieval noise. Specifically, Relevant retrieval noise (denoted as crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT) pertains to contexts that exhibit superficial relevance to the query x\ud835\udc65xitalic_x but lack the information necessary for the correct answer y\ud835\udc66yitalic_y. These contexts may appear relevant at first glance but ultimately mislead the model. Irrelevant retrieval noise (denoted as cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) encompasses contexts with low relevance to the query x\ud835\udc65xitalic_x, often arising from erroneous retrievals and generally being off-topic. Counterfactual retrieval noise (denoted as ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT) encompasses contexts that are topically related to x\ud835\udc65xitalic_x but contain incorrect and misleading information, often attributed to inaccuracies in the retriever\u2019s database.  \n \nTo examine the influence of three distinct types of retrieval noise on LLMs, we establish a benchmark for assessing retrieval noise robustness in LLMs by employing three open-domain question-answering datasets: Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and WebQ (Berant et al., 2013). Leveraging this benchmark, we evaluated the susceptibility of various open-source large language models to the effects of the three identified types of noise. The details of the construction of this benchmark can be found in Section 4.1. Leveraging this benchmark, we evaluate the sensitivity of various LLMs to the effects of the three types of noise. Specifically, we conduct experiments on six LLMs, including ChatGPT3.5, LLaMA27B (Touvron et al., 2023), LLaMA213B (Touvron et al., 2023), LLaMA270B (Touvron et al., 2023), Qwen7B (Bai et al., 2023), and Qwen14B (Bai et al., 2023). For each model, our experiments encompass two distinct settings: one with the exclusive presence of the golden retrieval context cg\u2062o\u2062l\u2062d\u2062e\u2062nsubscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bc_{golden}italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT and another incorporating the introduction of three different types of retrieval noise cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT. As shown in Figure 2, all LLMs experience varying degrees of impact from the three types of noise. The performance of LLMs exhibits a decline ranging from 0.2% to 13.43%. Through a comparative analysis of the effects of the three types of noise, we observe that irrelevant retrieval noise has a comparatively minor impact on LLMs with substantial capabilities.  \n \nFigure 3: The overview of our proposed RAAT method, which incorporates three distinct types of retrieval noises and the golden retrieval context during the training process.    \n 3.3 Retrieval-augmented Adaptive Adversarial Training \n \nRecently, several studies (Yoran et al., 2023; Li et al., 2022) attempted to enhance the noise robustness of LLMs through noisy training, which involves incorporating retrieved noisy contexts into fine-tuning data. The essence of noisy training involves the exploration of offline data augmentation, while in contrast, adversarial training leverages online data augmentation for a similar purpose (Ivgi and Berant, 2021). The core idea of adversarial training is to fortify the models against adversarial conditions by introducing adversarial perturbations (Jain et al., 2023). In the construction of adversarial samples, also known as noise samples, the min-max optimization strategy assumes a pivotal role, encompassing two fundamental steps. Initially, the maximization process involves adjusting the input data to intentionally mislead the model, inducing the maximum prediction error. Then, the minimization process entails fine-tuning the model\u2019s parameters to enhance its resistance against these meticulously crafted input perturbations (Bai et al., 2021). This strategy seeks to strike a balance, allowing the model to accurately identify normal data while robustly defending against potential attacks from adversarial examples.  \n \nIn this study, we aim to refine the objective of adversarial training while exploring the noise robustness challenges of RALMs. Considering a given query x\ud835\udc65xitalic_x, we assume the existence of four types of data augmentation, namely, golden retrieval context only (d\u2062ag\ud835\udc51subscript\ud835\udc4e\ud835\udc54da_{g}italic_d italic_a start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT), additional relevant retrieval noise (d\u2062ar\ud835\udc51subscript\ud835\udc4e\ud835\udc5fda_{r}italic_d italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT), additional irrelevant retrieval noise (d\u2062ai\ud835\udc51subscript\ud835\udc4e\ud835\udc56da_{i}italic_d italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), and additional counterfactual retrieval noise (d\u2062ac\ud835\udc51subscript\ud835\udc4e\ud835\udc50da_{c}italic_d italic_a start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT). The space of data augmentation is denoted as D\u2062A={d\u2062ag,d\u2062ar,d\u2062ai,d\u2062ac}\ud835\udc37\ud835\udc34\ud835\udc51subscript\ud835\udc4e\ud835\udc54\ud835\udc51subscript\ud835\udc4e\ud835\udc5f\ud835\udc51subscript\ud835\udc4e\ud835\udc56\ud835\udc51subscript\ud835\udc4e\ud835\udc50{DA}=\\{{da}_{g},{da}_{r},{da}_{i},{da}_{c}\\}italic_D italic_A = { italic_d italic_a start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , italic_d italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_d italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d italic_a start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT }. Then, the optimization problem can be formulated as follows:    min\u03b8\u2061\ud835\udd3c(x,y)\u223c\ud835\udc9f\u2062[maxd\u2062a\u2208D\u2062A\u2061\u2112\u2062(\u03b8,d\u2062a\u2062(x),y)]subscript\ud835\udf03subscript\ud835\udd3csimilar-to\ud835\udc65\ud835\udc66\ud835\udc9fdelimited-[]subscript\ud835\udc51\ud835\udc4e\ud835\udc37\ud835\udc34\u2112\ud835\udf03\ud835\udc51\ud835\udc4e\ud835\udc65\ud835\udc66\\min_{\\theta}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\left[\\max_{da\\in DA}\\mathcal{L}% (\\theta,da(x),y)\\right]roman_min start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_x , italic_y ) \u223c caligraphic_D end_POSTSUBSCRIPT [ roman_max start_POSTSUBSCRIPT italic_d italic_a \u2208 italic_D italic_A end_POSTSUBSCRIPT caligraphic_L ( italic_\u03b8 , italic_d italic_a ( italic_x ) , italic_y ) ]  (1)   \nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D denotes training data, \u2112\u2112\\mathcal{L}caligraphic_L is the loss function, \u03b8\ud835\udf03\\thetaitalic_\u03b8 denotes the parameters of LLMs, and d\u2062a\u2062(x)\ud835\udc51\ud835\udc4e\ud835\udc65da(x)italic_d italic_a ( italic_x ) represents the data augmentation of x\ud835\udc65xitalic_x.  \n \nBuilding upon the optimization problem outlined above, we introduce adaptive adversarial training as a tailored approach to enhance the robustness of RALMs against retrieval noise. Within adaptive adversarial training, the model refrains from updating parameters across all adversarial samples. Instead, it initiates the process by computing the generation loss for each adversarial sample, quantifying its adaptability to varying noise environments. Notably, a higher generation loss implies reduced adaptability of the model to the noisy environment. Given that each query involves one sample with a golden retrieval context and three adversarial samples, the model generates four distinct generation losses in each iteration. Following a min-max optimization strategy, the model prioritizes the selection of the largest loss to guide subsequent parameter update. Formally, we define the generation loss function \u2112\u2032superscript\u2112\u2032\\mathcal{L}^{\\prime}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT for the augmented input x\u2032superscript\ud835\udc65\u2032x^{\\prime}italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT as:    \u2112\u2032\u2062(\u03b8,x\u2032,y)=\u22121|y|\u2062\u2211t=1|y|log\u2061P\u03b8\u2062(yt\u2223x\u2032,y<t)superscript\u2112\u2032\ud835\udf03superscript\ud835\udc65\u2032\ud835\udc661\ud835\udc66superscriptsubscript\ud835\udc611\ud835\udc66subscript\ud835\udc43\ud835\udf03conditionalsubscript\ud835\udc66\ud835\udc61superscript\ud835\udc65\u2032subscript\ud835\udc66absent\ud835\udc61\\mathcal{L}^{\\prime}\\left(\\theta,x^{\\prime},y\\right)=-\\frac{1}{\\left|y\\right|}% \\sum_{t=1}^{\\left|y\\right|}\\log P_{\\theta}\\left(y_{t}\\mid x^{\\prime},y_{<t}\\right)caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_\u03b8 , italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_y ) = - divide start_ARG 1 end_ARG start_ARG | italic_y | end_ARG \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_y | end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2223 italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT )  (2)   \nwhere x\u2032=d\u2062a\u2062(x)superscript\ud835\udc65\u2032\ud835\udc51\ud835\udc4e\ud835\udc65x^{\\prime}=da(x)italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_d italic_a ( italic_x ) represents the augmented noise context of x\ud835\udc65xitalic_x.  \n \nTo effectively enhance performance across diverse noise environments, adaptive adversarial training incorporates a regularization term into its loss function. This incorporation of a regularization term is designed to mitigate the risk of the model overfitting to a particular type of noise. The regularization term acts as a stabilizing factor, promoting generalization and preventing the model from becoming overly specialized in its response to a specific noise profile. To achieve this goal, we introduce a regularization term specifically designed to mitigate the variance between these generation losses. This regularization term operates by identifying the largest \u2112m\u2062a\u2062x\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc4e\ud835\udc65\\mathcal{L}^{\\prime}_{max}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT and the smallest \u2112m\u2062i\u2062n\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc56\ud835\udc5b\\mathcal{L}^{\\prime}_{min}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT of the four generation losses at each training step. \u2112m\u2062a\u2062x\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc4e\ud835\udc65\\mathcal{L}^{\\prime}_{max}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT is the generation loss with the highest numerical value among four losses being considered. Conversely, \u2112m\u2062i\u2062n\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc56\ud835\udc5b\\mathcal{L}^{\\prime}_{min}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT is the loss function with the lowest numerical value. Here, an increased loss value indicates a greater magnitude of error or disparity in the aspect of the model\u2019s performance being assessed. This suggests that the model exhibits heightened sensitivity to adversarial examples reflecting retrieval noise. These adversarial examples are designed to probe and exploit weaknesses in the model\u2019s processing capabilities, especially in how it deals with noisy information in its input data. The regularization term, calculated as the square of the difference between \u2112m\u2062a\u2062x\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc4e\ud835\udc65\\mathcal{L}^{\\prime}_{max}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT and \u2112m\u2062i\u2062n\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc56\ud835\udc5b\\mathcal{L}^{\\prime}_{min}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT, aims to reduce the model\u2019s sensitivity to retrieval noise by encouraging a more balanced optimization. Formally, we design the regularization term \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT as:    \u2112reg=\u2016\u2112max\u2032\u2212\u2112min\u2032\u201622subscript\u2112regsuperscriptsubscriptnormsubscriptsuperscript\u2112\u2032maxsubscriptsuperscript\u2112\u2032min22\\mathcal{L}_{\\text{reg}}=\\|\\mathcal{L}^{\\prime}_{\\text{max}}-\\mathcal{L}^{% \\prime}_{\\text{min}}\\|_{2}^{2}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT = \u2225 caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT min end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  (3)    \n \nSubsequently, we define the adaptive adversarial training loss function \u2112adasubscript\u2112ada\\mathcal{L}_{\\text{ada}}caligraphic_L start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT as follows:    \u2112ada=\u2112max\u2032+wreg\u22c5\u2112regsubscript\u2112adasubscriptsuperscript\u2112\u2032max\u22c5subscript\ud835\udc64regsubscript\u2112reg\\mathcal{L}_{\\text{ada}}=\\mathcal{L}^{\\prime}_{\\text{max}}+w_{\\text{\\text{reg}% }}\\cdot\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT = caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT max end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT \u22c5 caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT  (4)   \nwhere wregsubscript\ud835\udc64regw_{\\text{reg}}italic_w start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT is a pre-defined hyperparameter to control the weight of \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT.    \n 3.4 Incorporating Noise Awareness \n \nAccurately identifying retrieval noise plays a pivotal role in fortifying the robustness of RALMs against the retrieval noise. Models endowed with the ability to discern different types of noise can more effectively choose and utilize training data, leading to an improvement in the overall quality of their generated outputs. This capacity to distinguish between various noise types contributes significantly to the model\u2019s adaptive learning process, enabling it to optimize performance in the presence of diverse noise scenarios. Inspired by the above motivation, we propose an auxiliary task designed to autonomously recognize the types of noisy retrieval texts, aiming to significantly bolster the retrieval robustness of RALMs. This auxiliary task serves as a valuable augmentation, contributing to the overall adaptability and effectiveness of the model in scenarios involving retrieval noise.   \n \nSpecifically, we attempt to enable the model to generate tokens that are sensitive to noise, thereby improving the model\u2019s capacity to discern various types of retrieval noise internally. Specifically, we first incorporate a linear layer beneath LLMs. Subsequently, a classification loss \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT is computed for each of the golden retrieval context and the three adversarial samples corresponding to each input x\ud835\udc65xitalic_x. One-hot encoding is employed in classification tasks, assigning values from 1 to 4 as labels to train the classifiers, where each classifier is tailored to a different retrieval noise type. The loss function \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT is computed using cross-entropy.  \n \nFinally, we formulate the final RAAT loss \u2112RAATsubscript\u2112RAAT\\mathcal{L}_{\\text{RAAT}}caligraphic_L start_POSTSUBSCRIPT RAAT end_POSTSUBSCRIPT by combining the adaptive adversarial training loss and the classification loss in the context of multi-task learning:    \u2112RAAT=wada\u22c5\u2112ada+wcls\u22c5\u2112clssubscript\u2112RAAT\u22c5subscript\ud835\udc64adasubscript\u2112ada\u22c5subscript\ud835\udc64clssubscript\u2112cls\\mathcal{L}_{\\text{RAAT}}=w_{\\text{ada}}\\cdot\\mathcal{L}_{\\text{ada}}+w_{\\text% {cls}}\\cdot\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT RAAT end_POSTSUBSCRIPT = italic_w start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT \u22c5 caligraphic_L start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT \u22c5 caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT  (5)   \nwhere wadasubscript\ud835\udc64adaw_{\\text{ada}}italic_w start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT and wclssubscript\ud835\udc64clsw_{\\text{cls}}italic_w start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT represent pre-defined hyperparameters used to balance the importance of these two different tasks.     \n 4 Experiments  \n 4.1 Dataset Construction \n \nWe have formulated a benchmark named RAG-Bench that is specifically designed to evaluate the retrieval noise robustness of LLMs. RAG-Bench is established upon three widely available datasets that center around open-domain question answering (QA): Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and WebQ (Berant et al., 2013). For each dataset, we employ the retrieval model DPR (Karpukhin et al., 2020) as our retriever, which retrieves ten passages from Wikipedia for each query. Then, we apply filtering to the queries, ensuring that each query in the filtered subset contains at least two golden retrieval contexts, indicating the presence of correct answers. Detailed statistics for both the full set and the filtered subset can be found in Table 1.  \n \nEach sample in our dataset contains a golden retrieval context and is deliberately designed to incorporate three types of augmented retrieval noise. To introduce relevant retrieval noise, we choose the context most pertinent to the query from the set of ten retrieval texts, excluding the golden retrieval context. In the case of irrelevant retrieval noise, no selection is made from the retrieval texts associated with the current query. Instead, a passage is randomly chosen from the retrieval contents of other queries, ensuring its complete irrelevance to the current query. For the counterfactual retrieval noise, we randomly select one passage from the two golden retrieval contexts and substitute its answer entity with an incorrect one.  \n \nThe test set of RAG-Bench comprises 1000 randomly chosen samples from the test sets of three QA datasets, resulting in a total of 3000 samples. The training set consists of 1500 samples randomly selected from the training sets of the three datasets, totaling 4500 samples. The validation set, drawn from the training sets of three QA datasets, contains 300 samples. Notably, careful measures were taken to ensure no overlap with the training data of RAG-Bench.  \n \n   Datasets Train Test   #Full #Subset #Full #Subset   NQ 79,168 40,551 3,610 1,833   TriviaQA 78,785 51,202 11,313 7,010   WebQ 3,778 2,316 2,032 1,057    \nTable 1: The statistics of the three QA datasets.  \n \n   Method Golden Only Golden & cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT Golden & crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Golden & ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT Avg   F1 EM F1 EM F1 EM F1 EM F1 EM   LLaMA27B 65.56 51.80 56.14 42.87 53.10 39.73 51.81 38.37 56.68 43.19   Qwen7B 62.57 47.07 61.48 46.06 55.50 40.50 53.26 36.90 58.20 42.63   LLaMA213B 69.27 55.00 63.25 49.47 62.27 47.97 62.07 47.17 64.22 49.90   Qwen14B 67.45 51.43 66.71 51.20 61.88 46.16 58.65 41.30 63.67 47.52   LLaMA270B 71.43 56.56 70.05 55.13 65.97 51.33 63.91 48.27 67.84 52.82   ChatGPT3.5 73.98 60.50 72.24 60.30 70.65 56.89 69.00 54.64 71.47 58.10   RALMgolden 80.31 74.03 79.33 72.73 73.26 66.33 73.08 65.40 76.50 69.62   RetRobust 80.10 73.80 79.25 72.97 74.81 68.30 75.46 68.43 77.41 70.88   RALMretrieved 80.04 73.40 81.09 74.80 75.99 69.10 73.10 65.67 77.55 70.74   RALMmultiple 85.47 80.17 85.27 81.20 83.07 78.33 83.25 79.23 84.27 79.73   RAAT 87.15 83.07 86.80 82.73 85.14 81.00 86.29 82.10 86.35 82.23    \nTable 2: Experimental results on our RAG-Bench benchmark. \u201cGolden Only\u201d denotes a scenario where LLMs only consult the golden retrieval context. In \u201cGolden & cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT/crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT/ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT\u201d, LLMs consider both the golden retrieval context and irrelevant retrieval noise/relevant retrieval noise/counterfactual retrieval noise.    \n 4.2 Evaluation Metrics \n \nWe evaluate the effectiveness of our method using two metrics: exact match (EM) and F1 score (Chen et al., 2017). Concretely, EM assesses the extent to which the answer generated by the system aligns precisely with the standard answer without any disparities at the character level. In contrast, the F1 score incorporates precision and recall, accounting for the equilibrium between correctly identifying answers and avoiding omitting correct answers.    \n 4.3 Baseline Methods \n \nWe conduct a comparison of our RAAT method against zero-shot LLMs, as well as finetuning approaches applied to LLaMA27B, which shares a common backbone with RAAT.   \n Zero-Shot Methods \n \nWithin the open-source community, many foundational and supervised fine-tuning (SFT) models have emerged. In our experiments, we select six renowned LLMs as baselines: ChatGPT3.5, LLaMA27B (Touvron et al., 2023), LLaMA213B, LLaMA270B, Qwen7B (Bai et al., 2023), and Qwen14B.    \nFine-tuning Methods \n \nWe further compare RAAT with various fine-tuning methods. \n \n \u2022 \n \nRALMgolden This is a RALM with instruction tuning (Lin et al., 2023). It prepends a golden retrieval text cg\u2062o\u2062l\u2062d\u2062e\u2062nsubscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bc_{golden}italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT in context to fine-tune LLaMA27B.   \n \u2022 \n \nRetRobust To ensure that the model is exposed to both golden retrieval texts and various retrieval noise during training,  Yoran et al. (2023) proposes RetRobust. For each query, RetRobust selects top-1, low-ranked, or random retrieved passages with equal probability for training.   \n \u2022 \n \nRALMretrieved This variant is a RALM incorporating instruction tuning. In contrast to RALMgolden, RALMretrieved does not manually design retrieval noise in the training set but directly uses the top-2 retrieved passages. This training method is more aligned with real retrieval environments.   \n \u2022 \n \nRALMmultiple This approach closely resembles RetRobust, differing only in the construction of the training dataset. In RALMmultiple, rather than introducing one type of retrieval noise randomly for each query, each type of retrieval noise is combined with the sample and incorporated into the dataset. That is, each query is associated with four augmented noisy samples.     \n \n   Method Golden Only Golden & cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT Golden & crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Golden & ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT Avg   F1 EM F1 EM F1 EM F1 EM F1 EM   RAAT 87.15 83.07 86.80 82.73 85.14 81.00 86.29 82.10 86.35 82.23   RAAT w/o \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT 86.76 82.77 86.45 82.27 84.69 80.63 85.54 81.20 85.86 81.71   RAAT w/o \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT 86.87 83.03 83.92 79.86 84.69 80.57 87.02 82.80 85.63 81.57    \nTable 3: Ablation test results in terms of EM and F1 score.     \n 4.4 Implementation Details \n \nOur RAAT method relies on LLaMA2-7B as the foundational model. We set the weight parameters as follows: wregsubscript\ud835\udc64regw_{\\text{reg}}italic_w start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT to 0.1, wraatsubscript\ud835\udc64raatw_{\\text{raat}}italic_w start_POSTSUBSCRIPT raat end_POSTSUBSCRIPT to 2, and wclssubscript\ud835\udc64clsw_{\\text{cls}}italic_w start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT to 1. The sequence length, epoch, and learning rate are configured to 512, 2, and 5e-6, respectively. Our experiments are conducted on a computational cluster equipped with 4 NVIDIA A100 GPUs, each boasting a capacity of 80GB.     \n 5 Experimental Results   \n 5.1 Main Results \n \nTable 2 illustrates the efficacy of our RAAT method compared to the baselines in terms of F1 and EM scores. We observe that all models are affected by three different types of retrieval noise attacks. The influence of irrelevant retrieval noise is marginal, while counterfactual retrieval noise exerts the most significant impact. For the models sharing the same architecture, larger parameter sizes correlate with superior performance and better robustness against retrieval noise. For instance, LLaMA27B exhibits a 12.46% reduction in F1 score when confronted with relevant retrieval noise, whereas LLaMA213B only experiences a 7% decrease under identical conditions. This trend is also evident in Qwen.  \n \nFrom Table 2, we can also observe that fine-tuning enables LLMs to better utilize information from the retrieval texts. Fine-tuned models significantly outperform the zero-shot LLMs with varying parameter sizes. Moreover, RALMmultiple shows a significant improvement over RALMgolden, RALMretrieved and RetRobust, reflecting the sensitivity of retrieval noise to the training dataset and the importance of diversity in noise attacks during training. Our RAAT method achieves even better performance than RALMmultiple in all four environments, with an average increase of 2.08% in the F1 score and 2.5% in the EM score, demonstrating its superior ability to handle diverse retrieval noise.    \n 5.2 Ablation Study \n \nTo gain a comprehensive understanding of the individual contribution of each component within RAAT to the overall performance, we conducted an ablation study by removing the regularization term loss (denoted as w/o \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT) and the noise-aware classification loss (denoted as w/o \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT). The experimental results are shown in Table 3. After removing the classification loss, we observe that the average performance of the model decreased by 0.49% and 0.52% in terms of F1 score and EM score, respectively. While removing the regularization term, there was a significant performance decrease in handling irrelevant retrieval noise.  \n \nFigure 4: The number of queries and parameter updates are 4,500 and 9,000, respectively. The statistical content in this table pertains to different types of retrieval noises selected by RAAT each time the model parameters undergo an update.     \n 5.3 Further Discussion  \nWhat types of adversarial samples does RAAT employ during training? \n \nTo gain a comprehensive understanding of the underlying mechanisms of RAAT, particularly its utilization of specific retrieved data to augment model robustness, we undertook an in-depth examination of its training process, involving meticulously tracking the training iterations and conducting a thorough statistical analysis to quantify the number of different types of adversarial examples incorporated during the training phase. The statistical results are illustrated in Figure 4. We observe that RAAT prioritizes the selection of adversarial examples that can significantly improve model robustness, as reflected in its tendency to choose certain types of adversarial examples. This is consistent with our empirical findings described in Section 3.2. RAAT tends to select adversarial examples associated with relevant retrieval noise and counterfactual retrieval noise for training.      \n 6 Conclusion \n \nThis work initially investigated retrieval noises in RALMs and categorized them into three distinct types, reflecting real-world environments. In addition, we introduced RAAT as a solution to address the noise robustness challenges faced by RALMs, which leveraged adaptive adversarial learning and multi-task learning to enhance the model\u2019s capability. Moreover, we established a benchmark to verify the effectiveness of RAAT based on three open-domain QA datasets. Experimental results demonstrate substantial improvements in F1 and EM scores for the LLaMA2 7B model fine-tuned with RAAT across diverse noise conditions.    \n 7 Limitations \n \nIn this section, we delve into the limitations inherent in our work, with the objective of pinpointing areas for refinement and bolstering the performance of our model in future endeavors. Two principal limitations have been identified. Firstly, the benchmark constructed for our experiments relies exclusively on datasets sourced from three open-domain question answering repositories. Going forward, we intend to compile additional high-quality datasets from varying NLP tasks and endeavor to retrieve texts from a more extensive array of knowledge bases. This strategic expansion aims to facilitate the creation of a more diversified and expansive benchmark tailored for evaluating the retrieval noise robustness of large language models. Secondly, within the framework of RAAT, our efforts have been singularly concentrated on fortifying the retrieval noise robustness at the LLM end. However, the prospect of jointly training large language models and retrieval models emerges as a promising avenue for enhancing the overall robustness of RALMs. Although this dimension was not the primary focal point of our current work, in our subsequent investigations into retrieval noise robustness, we plan to delve into this avenue. This approach would facilitate the synchronized progress of both the large language model and the retrieval model, contributing to an overall improvement in their robustness.    \nAcknowledgments \n \nMin Yang was supported by National Key Research and Development Program of China (2022YFF0902100), National Natural Science Foundation of China (62376262), the Natural Science Foundation of Guangdong Province of China (2024A1515030166), Shenzhen Science and Technology Innovation Program (KQTD20190929172835662), Shenzhen Basic Research Foundation (JCYJ20210324115614039). This work was supported by Alibaba Group through Alibaba Innovative Research Program, Postdoctoral Fellowship Program of CPSF (GZC20232873), GuangDong Basic and Applied Basic Research Foundation (2023A1515110718 and 2024A1515012003).    \nReferences \n \n Bai et al. (2023)  Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023.  Qwen technical report.  arXiv preprint arXiv:2309.16609.   \n Bai et al. (2021)  Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.  Recent advances in adversarial training for adversarial robustness.  arXiv preprint arXiv:2102.01356.   \n Berant et al. (2013)  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.  Semantic parsing on freebase from question-answer pairs.  In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533\u20131544.   \n Brown et al. (2020)  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.  Language models are few-shot learners.  Advances in neural information processing systems, 33:1877\u20131901.   \n Bubeck et al. (2023)  S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.  Sparks of artificial general intelligence: Early experiments with gpt-4.  arXiv preprint arXiv:2303.12712.   \n Chen et al. (2017)  Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.  Reading wikipedia to answer open-domain questions.  arXiv preprint arXiv:1704.00051.   \n Chen et al. (2023)  Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023.  Benchmarking large language models in retrieval-augmented generation.  arXiv preprint arXiv:2309.01431.   \n Creswell et al. (2022)  Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022.  Selection-inference: Exploiting large language models for interpretable logical reasoning.  arXiv preprint arXiv:2205.09712.   \n Gao et al. (2023)  Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.  Retrieval-augmented generation for large language models: A survey.  arXiv preprint arXiv:2312.10997.   \n Goodfellow et al. (2014)  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014.  Explaining and harnessing adversarial examples.  arXiv preprint arXiv:1412.6572.   \n Gueta et al. (2023)  Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. 2023.  Knowledge is a region in weight space for fine-tuned language models.  arXiv preprint arXiv:2302.04863.   \n Ivgi and Berant (2021)  Maor Ivgi and Jonathan Berant. 2021.  Achieving model robustness through discrete adversarial training.  arXiv preprint arXiv:2104.05062.   \n Jain et al. (2023)  Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. 2023.  Neftune: Noisy embeddings improve instruction finetuning.  arXiv preprint arXiv:2310.05914.   \n Jia and Liang (2017)  Robin Jia and Percy Liang. 2017.  Adversarial examples for evaluating reading comprehension systems.  arXiv preprint arXiv:1707.07328.   \n Joshi et al. (2017)  Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017.  Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.  arXiv preprint arXiv:1705.03551.   \n Kandpal et al. (2023)  Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.  Large language models struggle to learn long-tail knowledge.  In International Conference on Machine Learning, pages 15696\u201315707. PMLR.   \n Karpukhin et al. (2020)  Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.  Dense passage retrieval for open-domain question answering.  arXiv preprint arXiv:2004.04906.   \n Kurakin et al. (2016)  Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016.  Adversarial machine learning at scale.  arXiv preprint arXiv:1611.01236.   \n Kwiatkowski et al. (2019)  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.  Natural questions: a benchmark for question answering research.  Transactions of the Association for Computational Linguistics, 7:453\u2013466.   \n Lewis et al. (2020)  Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020.  Retrieval-augmented generation for knowledge-intensive nlp tasks.  Advances in Neural Information Processing Systems, 33:9459\u20139474.   \n Li et al. (2022)  Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2022.  Large language models with controllable working memory.  arXiv preprint arXiv:2211.05110.   \n Liang et al. (2023)  Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, and Xinwang Liu. 2023.  Knowledge graph contrastive learning based on relation-symmetrical structure.  IEEE Transactions on Knowledge and Data Engineering.   \n Lin et al. (2023)  Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.  Ra-dit: Retrieval-augmented dual instruction tuning.  arXiv preprint arXiv:2310.01352.   \n Madry et al. (2017)  Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017.  Towards deep learning models resistant to adversarial attacks.  arXiv preprint arXiv:1706.06083.   \n Mao et al. (2020)  Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020.  Generation-augmented retrieval for open-domain question answering.  arXiv preprint arXiv:2009.08553.   \n Miyato et al. (2016)  Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016.  Adversarial training methods for semi-supervised text classification.  arXiv preprint arXiv:1605.07725.   \n Morris et al. (2020)  John X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020.  Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp.  arXiv preprint arXiv:2005.05909.   \n Ni et al. (2023)  Shiwen Ni, Jiawen Li, Min Yang, and Hung-Yu Kao. 2023.  Dropattack: A random dropped weight attack adversarial training for natural language understanding.  IEEE/ACM Transactions on Audio, Speech, and Language Processing.   \n Ruder (2017)  Sebastian Ruder. 2017.  An overview of multi-task learning in deep neural networks.  arXiv preprint arXiv:1706.05098.   \n Shen et al. (2023)  Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023.  In chatgpt we trust? measuring and characterizing the reliability of chatgpt.  arXiv preprint arXiv:2304.08979.   \n Shi et al. (2023)  Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023.  Large language models can be easily distracted by irrelevant context.  In International Conference on Machine Learning, pages 31210\u201331227. PMLR.   \n Thakur et al. (2023)  Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, et al. 2023.  Nomiracl: Knowing when you don\u2019t know for robust multilingual retrieval-augmented generation.  arXiv preprint arXiv:2312.11361.   \n Touvron et al. (2023)  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.  Llama 2: Open foundation and fine-tuned chat models.  arXiv preprint arXiv:2307.09288.   \n Wang et al. (2020)  Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020.  Infobert: Improving robustness of language models from an information theoretic perspective.  arXiv preprint arXiv:2010.02329.   \n Wu et al. (2017)  Yi Wu, David Bamman, and Stuart Russell. 2017.  Adversarial training for relation extraction.  In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783.   \n Yasunaga et al. (2017)  Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017.  Robust multilingual part-of-speech tagging via adversarial training.  arXiv preprint arXiv:1711.04903.   \n Yin et al. (2023)  Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023.  Alcuna: large language models meet new knowledge.  arXiv preprint arXiv:2310.14820.   \n Yoran et al. (2023)  Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.  Making retrieval-augmented language models robust to irrelevant context.  arXiv preprint arXiv:2310.01558.   \n Yu et al. (2023)  Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.  Chain-of-note: Enhancing robustness in retrieval-augmented language models.  arXiv preprint arXiv:2311.09210.   \n Zhu et al. (2019)  Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2019.  Freelb: Enhanced adversarial training for natural language understanding.  arXiv preprint arXiv:1909.11764.   \n Zhu et al. (2023)  Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023.  Large language models for information retrieval: A survey.  arXiv preprint arXiv:2308.07107.   \n Zou et al. (2023)  Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.  Universal and transferable adversarial attacks on aligned language models.  arXiv preprint arXiv:2307.15043.     \n \n \nFigure 5: The results of T-SNE visualization. Following the introduction of four types of adversarial samples (i.e., retrieval noises) into models tuned by various methods, the hidden state of the last token is extracted. Subsequently, dimensionality reduction using t-SNE, clustering, and visualization are performed. This visual representation includes three methods, namely RALMg\u2062o\u2062l\u2062d\u2062e\u2062nsubscriptRALM\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5b\\text{RALM}_{golden}RALM start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT, RetRobust, and RAAT.   \n Appendix A Has the Model Truly Attained Noise Awareness? \n \nOur preliminary investigation focused on the intrinsic capability of RALMgolden and RetRobust to classify the types of retrieval noises. Drawing inspiration from previous work Gueta et al. (2023), we approached this matter through the application of clustering algorithms. The results, illustrated in Figure  5, reveal suboptimal clustering of text vectors from RALMgolden and RetRobust, suggesting that the internal representations for noise classification in these models may lack clarity. Consequently, we introduced a noise classification loss \u2112c\u2062l\u2062ssubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\\mathcal{L}_{{cls}}caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT into our RAAT method. The experimental results demonstrated tangible benefits with the incorporation of the classification loss. Additionally, we assessed the clustering effectiveness in models fine-tuned with RAAT, observing minimal distances among samples of irrelevant, relevant, and no retrieval noises, in contrast to the considerable distance from counterfactual retrieval noise samples. In particular, counterfactual retrieval noise posed the most significant challenge to LLMs; however, after RAAT tuning, it exhibited superior clustering and representation learning outcomes, indirectly validating the efficacy of RAAT.     \n \nGenerated on Fri May 31 16:18:59 2024 by LaTeXML   ", "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "description": "<strong>Adversarial</strong> <strong>training</strong> <strong>for</strong> relation extraction. In Proceedings of the 2017 Conference on Empirical <strong>Methods</strong> in Natural <strong>Language</strong> Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. <strong>Robust</strong> <strong>multilingual</strong> <strong>part</strong>-<strong>of</strong>-<strong>speech</strong> <strong>tagging</strong> <strong>via</strong> <strong>adversarial</strong> ...", "usage": {"tokens": 15092}, "retrieved_text": ["2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Yin et al. (2023) Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. Alcuna: large language models meet new knowledge. arXiv preprint arXiv:2310.14820. Yoran et al. (2023) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558. Yu et al. (2023) Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210. Zhu et al. (2019)  Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2019. Freelb: Enhanced adversarial training for natural language understanding. arXiv preprint arXiv:1909.11764. Zhu et al. (2023)  Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107. Zou et al. (2023)  Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.", "(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wang et al. (2020) Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020. Infobert: Improving robustness of language models from an information theoretic perspective. arXiv preprint arXiv:2010.02329. Wu et al. (2017) Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Yin et al. (2023)  Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. Alcuna: large language models meet new knowledge. arXiv preprint arXiv:2310.14820. Yoran et al. (2023)  Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558. Yu et al. (2023)  Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210.", "the min-max paradigm of adversarial training (Morris et al., 2020; Ivgi and Berant, 2021). Moreover, RAAT utilizes multi-task learning (Ruder, 2017) to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance. The main contributions of this paper can be summarized as follows: \u2022 We systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises. \u2022 \n \nWe propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises. \u2022 \n \nWe set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments. 2 Related Work  \nRetrieval-Augmented Generation with Noisy Context \n \nRetrieval-Augmented Language Models (RALMs) have shown impressive performance in various NLP tasks (Gao et al., 2023; Zhu et al., 2023)."]}], "The paper titled \"Unsupervised Cross-lingual Representation Learning at Scale\" discusses an unsupervised approach to learning cross-lingual representations that can be used for tasks such as transfer learning and zero-shot learning.": [{"text": " \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.  \n \nI Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1911.02116 (cs)   \n \n \n [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] \nTitle:Unsupervised Cross-lingual Representation Learning at Scale \nAuthors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \nView a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF \n Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.   \n   Comments: ACL 2020 (+ updated results)   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1911.02116 [cs.CL]     (or  arXiv:1911.02116v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1911.02116\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Alexis Conneau [view email] \n [v1] Tue, 5 Nov 2019 22:42:00 UTC (569 KB)\n [v2] Wed, 8 Apr 2020 01:02:17 UTC (561 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2019-11 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 3 blog links (what is this?)   \n \nDBLP - CS Bibliography \n listing | bibtex  \n Alexis Conneau\nNaman Goyal\nVishrav Chaudhary\nGuillaume Wenzek\nFrancisco Guzm\u00e1n \n\u2026  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale", "description": "This <strong>paper</strong> shows that pretraining multilingual language models <strong>at</strong> <strong>scale</strong> leads to significant performance gains for a wide range of <strong>cross</strong>-<strong>lingual</strong> <strong>transfer</strong> <strong>tasks</strong>. We train a Transformer-based masked language model on one hundred languages, <strong>using</strong> more than two terabytes of filtered CommonCrawl data.", "usage": {"tokens": 1454}, "retrieved_text": [" \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1911.02116 (cs)   \n \n \n [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] \nTitle:Unsupervised Cross-lingual Representation Learning at Scale \nAuthors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \nView a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF \n Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1911.02116 (cs) [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] Title:Unsupervised Cross-lingual Representation Learning at Scale Authors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov View a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.", "Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\n\nAnthology ID:\n2020.acl-main.747\nVolume:\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nMonth:\nJuly\nYear:\n2020\nAddress:\nOnline\nEditors:\nDan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n8440\u20138451\nLanguage:\n\nURL:\nhttps://aclanthology.org/2020.acl-main.747/\nDOI:\n10.18653/v1/2020.acl-main.747\nBibkey:\n\nconneau-etal-2020-unsupervised\nCite (ACL):\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\n\nCite (Informal):\nUnsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/2020.acl-main.747.pdf\nVideo:\n http://slideslive.com/38928776\nCode\n facebookresearch/XLM +  additional community code\nData\nCC100, CoNLL 2002, GLUE, MLQA, QNLI, SQuAD, SST\nPDF Cite Search  Code  Video  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{conneau-etal-2020-unsupervised, title = \"Unsupervised Cross-lingual Representation Learning at Scale\", author = \"Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin\", editor = \"Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = jul, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.747/\", doi = \"10.18653/v1/2020.acl-main.747\", pages = \"8440--8451\", abstract = \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\\%} average accuracy on XNLI, +13{\\%} average F1 score on MLQA, and +2.4{\\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\\%} in XNLI accuracy for Swahili and 11.4{\\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"conneau-etal-2020-unsupervised\"> <titleInfo> <title>Unsupervised Cross-lingual Representation Learning at Scale</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Alexis</namePart> <namePart type=\"family\">Conneau</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Kartikay</namePart> <namePart type=\"family\">Khandelwal</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Naman</namePart> <namePart type=\"family\">Goyal</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Vishrav</namePart> <namePart type=\"family\">Chaudhary</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Guillaume</namePart> <namePart type=\"family\">Wenzek</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Francisco</namePart> <namePart type=\"family\">Guzm\u00e1n</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Edouard</namePart> <namePart type=\"family\">Grave</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Myle</namePart> <namePart type=\"family\">Ott</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Luke</namePart> <namePart type=\"family\">Zettlemoyer</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Veselin</namePart> <namePart type=\"family\">Stoyanov</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2020-07</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Dan</namePart> <namePart type=\"family\">Jurafsky</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Joyce</namePart> <namePart type=\"family\">Chai</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Natalie</namePart> <namePart type=\"family\">Schluter</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Joel</namePart> <namePart type=\"family\">Tetreault</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Online</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.</abstract> <identifier type=\"citekey\">conneau-etal-2020-unsupervised</identifier> <identifier type=\"doi\">10.18653/v1/2020.acl-main.747</identifier> <location> <url>https://aclanthology.org/2020.acl-main.747/</url> </location> <part> <date>2020-07</date> <extent unit=\"page\"> <start>8440</start> <end>8451</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Unsupervised Cross-lingual Representation Learning at Scale %A Conneau, Alexis %A Khandelwal, Kartikay %A Goyal, Naman %A Chaudhary, Vishrav %A Wenzek, Guillaume %A Guzm\u00e1n, Francisco %A Grave, Edouard %A Ott, Myle %A Zettlemoyer, Luke %A Stoyanov, Veselin %Y Jurafsky, Dan %Y Chai, Joyce %Y Schluter, Natalie %Y Tetreault, Joel %S Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics %D 2020 %8 July %I Association for Computational Linguistics %C Online %F conneau-etal-2020-unsupervised %X This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available. %R 10.18653/v1/2020.acl-main.747 %U https://aclanthology.org/2020.acl-main.747/ %U https://doi.org/10.18653/v1/2020.acl-main.747 %P 8440-8451 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Unsupervised Cross-lingual Representation Learning at Scale](https://aclanthology.org/2020.acl-main.747/) (Conneau et al., ACL 2020)\n\nUnsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020)\nACL\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Unsupervised Cross-lingual Representation Learning at Scale - ACL Anthology", "description": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.", "usage": {"tokens": 3716}, "retrieved_text": [" ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.", "ACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit AbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "(ACL): Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics. Cite (Informal): Unsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/2020.acl-main.747.pdf Video: http://slideslive.com/38928776 Code facebookresearch/XLM + additional community code Data CC100, CoNLL 2002, GLUE, MLQA, QNLI, SQuAD, SST PDF Cite Search Code Video Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{conneau-etal-2020-unsupervised, title = \"Unsupervised Cross-lingual Representation Learning at Scale\", author = \"Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin\", editor = \"Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = jul, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.747/\", doi = \"10.18653/v1/2020.acl-main.747\", pages = \"8440--8451\", abstract = \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data."]}, {"text": "Home\n\nChat with PDF\n\nLiterature Review\n\nAI Writer\n\nFind Topics\n\nParaphraser\n\nCitation Generator\n\nExtract Data\n\nAI Detector\n\nPDF to Video\n\nAffiliate Program\n\nChrome Extension\n\n\nUse on ChatGPT\n\n\nContact Us\n\n\n\n\nHome\nPapers\nUnsupervised Cross-lingual Representation Learning at Scale\n\n\n\nOpen AccessProceedings ArticleDOI\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University\n - 01 Jul 2020\n\n - pp 8440-8451\nShow Less\n\n\n\nPodcast\n3.7K\nPDF\n\nSave\nCite\n\n\n\n\nTL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time.\nread more\n\n\nAbstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\nread more\nShow Related Papers\n\n\n\n\nChat with Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitations\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nMAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\nJonas Pfeiffer,Ivan Vuli\u0107,Iryna Gurevych,Sebastian Ruder +3 moreTechnische Universit\u00e4t Darmstadt,University of Mannheim,Google\n - 30 Apr 2020\n\nShow Less\n\n\n\nTL;DR: This paper proposed MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations, and introduced a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language....read moreread less\n\n169\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Journal Article\u2022DOI\nPre-Trained Models: Past, Present and Future\nXu Han,Zhengyan Zhang,Ning Ding,Yuxian Gu,Xiao Liu,Yuqi Huo,Jiezhong Qiu,Liang Zhang,Wentao Han,Minlie Huang,Qin Jin,Yanyan Lan,Yang Liu,Zhiyuan Liu,Zhiwu Lu,Xipeng Qiu,Ruihua Song,Jie Tang,Ji-Rong Wen,Jinhui Yuan,Wayne Xin Zhao,Jun Zhu +21 moreTsinghua University,Renmin University of China,Fudan University\n - 14 Jun 2021\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum....read moreread less\n\n135\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nAdapterHub: A Framework for Adapting Transformers\nJonas Pfeiffer,Andreas R\u00fcckl\u00e9,Clifton Poth,Aishwarya Kamath,Ivan Vuli\u0107,Sebastian Ruder,Kyunghyun Cho,Iryna Gurevych +7 moreTechnische Universit\u00e4t Darmstadt,New York University,University of Oslo,Google,Canadian Institute for Advanced Research\n - 15 Jul 2020\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors propose a framework that allows dynamic \"stiching-in\" of pre-trained adapters for different tasks and languages, which enables extremely easy and quick adaptation of state-of-the-art pre-training models across tasks....read moreread less\n\n122\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nFlauBERT: Unsupervised Language Model Pre-training for French\nHang Le,Lo\u00efc Vial,Jibril Frej,Vincent Segonne,Maximin Coavoux,Benjamin Lecouteux,Alexandre Allauzen,Beno\u00eet Crabb\u00e9,Laurent Besacier,Didier Schwab +9 moreUniversity of Grenoble,Centre national de la recherche scientifique\n - 11 Dec 2019\n\nShow Less\n\n\n\nTL;DR: The authors proposed FlauBERT, a model learned on a very large and heterogeneous French corpus and applied it to various NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and showed that most of the time they outperformed other pre-training approaches....read moreread less\n\n120\nPodcast\n\n\n\n\n\n\n\n\n\u2022Posted Content\nmT5: A massively multilingual pre-trained text-to-text transformer\nLinting Xue,Noah Constant,Adam Roberts,Mihir Kale,Rami Al-Rfou,Aditya Siddhant,Aditya Barua,Colin Raffel +7 moreGoogle,University of Chester\n - 22 Oct 2020\n - arXiv: Computation and Language \nShow Less\n\n\n\nTL;DR: This article proposed a multilingual variant of T5, mT5, which was pre-trained on a new Common Crawl-based dataset covering 101 languages and achieved state-of-the-art performance on many multilingual benchmarks....read moreread less\n\n99\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n...\n\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\nProceedings Article\u2022DOI\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 moreGoogle\n - 11 Oct 2018\n\nShow Less\n\n\n\nTL;DR: BERT as mentioned in this paper pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks....read moreread less\n\n24.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nDistributed Representations of Words and Phrases and their Compositionality\nTomas Mikolov,Ilya Sutskever,Kai Chen,Greg S. Corrado,Jeffrey Dean +4 moreGoogle\n - 05 Dec 2013\n\nShow Less\n\n\n\nTL;DR: This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling....read moreread less\n\n24K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nDeep contextualized word representations\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 moreAllen Institute for Artificial Intelligence,University of Massachusetts Amherst,Google,University of Washington\n - 15 Feb 2018\n\nShow Less\n\n\n\nTL;DR: This paper introduced a new type of deep contextualized word representation that models both complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic contexts (i.e., to model polysemy)....read moreread less\n\n7.4K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nPranav Rajpurkar,Jian Zhang,Konstantin Lopyrev,Percy Liang +3 moreStanford University\n - 16 Jun 2016\n\nShow Less\n\n\n\nTL;DR: The Stanford Question Answering Dataset (SQuAD) as mentioned in this paper is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage....read moreread less\n\n3.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nAlex Wang,Amanpreet Singh,Julian Michael,Felix Hill,Omer Levy,Samuel R. Bowman +5 moreNew York University,University of Washington,Google\n - 20 Apr 2018\n\nShow Less\n\n\n\nTL;DR: A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks....read moreread less\n\n2.1K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n...\n\n\n\n\n\nRelated Papers (5)\n\n\n\n\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n[...]\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 more\n - 11 Oct 2018\n\nShow Less\n\n\n\n\nDeep contextualized word representations\n[...]\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 more\n - 15 Feb 2018\n\nShow Less\n\n\n\n\nBeto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\n[...]\nShijie Wu,Mark Dredze +1 more\n - 19 Apr 2019\n\nShow Less\n\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n[...]\nColin Raffel,Noam Shazeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena,Yanqi Zhou,Wei Li,Peter J. Liu +8 more\n - 01 Jan 2020\n - Journal of Machine Learning Research \nShow Less\n\n\n\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\n[...]\nTaku Kudo,John Richardson +1 more\n - 19 Aug 2018\n\nShow Less\n\n\n\n\n\n\nTools\nChat with PDFLiterature ReviewAI WriterFind TopicsParaphraserCitation GeneratorExtract DataAI DetectorCitation BoosterExtensions\nSciSpace Chrome Extension\nSciSpace\nAboutCareersResourcesSupportBrowse PapersPricingSciSpace Affiliate ProgramCancellation & Refund PolicyTermsPrivacyData Sources\nDirectories\nPapersTopicsJournalsAuthorsConferencesInstitutionsCitation Styles\nContact\nsupport@typeset.io+1 (760) 284-7800+91 9916292973\n\n\u00a9 2025 | PubGenius Inc. | Suite # 217 691 S Milpitas Blvd Milpitas CA 95035, USA", "title": "(PDF) Unsupervised Cross-lingual Representation Learning at Scale (2020) | Alexis Conneau | 3721 Citations", "description": "(DOI: 10.18653/V1/2020.ACL-MAIN.747) This <strong>paper</strong> shows that pretraining multilingual language models <strong>at</strong> <strong>scale</strong> leads to significant performance gains for a wide range of <strong>cross</strong>-<strong>lingual</strong> <strong>transfer</strong> <strong>tasks</strong>. We train a Transformer-based masked language model on one hundred languages, <strong>using</strong> more than ...", "usage": {"tokens": 2990}, "retrieved_text": ["(mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available. read more\nShow Related Papers\n\n\n\n\nChat with Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitations\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nMAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\nJonas Pfeiffer,Ivan Vuli\u0107,Iryna Gurevych,Sebastian Ruder +3 moreTechnische Universit\u00e4t Darmstadt,University of Mannheim,Google\n - 30 Apr 2020\n\nShow Less\n\n\n\nTL;DR: This paper proposed MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations, and introduced a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language....read moreread less\n\n169\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Journal Article\u2022DOI\nPre-Trained Models: Past, Present and Future\nXu Han,Zhengyan Zhang,Ning Ding,Yuxian Gu,Xiao Liu,Yuqi Huo,Jiezhong Qiu,Liang Zhang,Wentao Han,Minlie Huang,Qin Jin,Yanyan Lan,Yang Liu,Zhiyuan Liu,Zhiwu Lu,Xipeng Qiu,Ruihua Song,Jie Tang,Ji-Rong Wen,Jinhui Yuan,Wayne Xin Zhao,Jun Zhu +21 moreTsinghua University,Renmin University of China,Fudan University\n - 14 Jun 2021\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum....read moreread less\n\n135\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nAdapterHub: A Framework for Adapting Transformers\nJonas Pfeiffer,Andreas R\u00fcckl\u00e9,Clifton Poth,Aishwarya Kamath,Ivan Vuli\u0107,Sebastian Ruder,Kyunghyun Cho,Iryna Gurevych +7 moreTechnische Universit\u00e4t Darmstadt,New York University,University of Oslo,Google,Canadian Institute for Advanced Research\n - 15 Jul 2020\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors propose a framework that allows dynamic \"stiching-in\" of pre-trained adapters for different tasks and languages, which enables extremely easy and quick adaptation of state-of-the-art pre-training models across tasks....read moreread less\n\n122\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nFlauBERT: Unsupervised Language Model Pre-training for French\nHang Le,Lo\u00efc Vial,Jibril Frej,Vincent Segonne,Maximin Coavoux,Benjamin Lecouteux,Alexandre Allauzen,Beno\u00eet Crabb\u00e9,Laurent Besacier,Didier Schwab +9 moreUniversity of Grenoble,Centre national de la recherche scientifique\n - 11 Dec 2019\n\nShow Less\n\n\n\nTL;DR: The authors proposed FlauBERT, a model learned on a very large and heterogeneous French corpus and applied it to various NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and showed that most of the time they outperformed other pre-training approaches....read moreread less\n\n120\nPodcast\n\n\n\n\n\n\n\n\n\u2022Posted Content\nmT5: A massively multilingual pre-trained text-to-text transformer\nLinting Xue,Noah Constant,Adam Roberts,Mihir Kale,Rami Al-Rfou,Aditya Siddhant,Aditya Barua,Colin Raffel +7 moreGoogle,University of Chester\n - 22 Oct 2020\n - arXiv: Computation and Language \nShow Less\n\n\n\nTL;DR: This article proposed a multilingual variant of T5, mT5, which was pre-trained on a new Common Crawl-based dataset covering 101 languages and achieved state-of-the-art performance on many multilingual benchmarks....read moreread less\n\n99\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n...\n\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\nProceedings Article\u2022DOI\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 moreGoogle\n - 11 Oct 2018\n\nShow Less\n\n\n\nTL;DR: BERT as mentioned in this paper pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks....read moreread less\n\n24.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nDistributed Representations of Words and Phrases and their Compositionality\nTomas Mikolov,Ilya Sutskever,Kai Chen,Greg S. Corrado,Jeffrey Dean +4 moreGoogle\n - 05 Dec 2013\n\nShow Less\n\n\n\nTL;DR: This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling....read moreread less\n\n24K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nDeep contextualized word representations\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 moreAllen Institute for Artificial Intelligence,University of Massachusetts Amherst,Google,University of Washington\n - 15 Feb 2018\n\nShow Less\n\n\n\nTL;DR: This paper introduced a new type of deep contextualized word representation that models both complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic contexts (i.e., to model polysemy)....read moreread less\n\n7.4K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nPranav Rajpurkar,Jian Zhang,Konstantin Lopyrev,Percy Liang +3 moreStanford University\n - 16 Jun 2016\n\nShow Less\n\n\n\nTL;DR: The Stanford Question Answering Dataset (SQuAD) as mentioned in this paper is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage....read moreread less\n\n3.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nAlex Wang,Amanpreet Singh,Julian Michael,Felix Hill,Omer Levy,Samuel R. Bowman +5 moreNew York University,University of Washington,Google\n - 20 Apr 2018\n\nShow Less\n\n\n\nTL;DR: A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks....read moreread less\n\n2.1K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n...\n\n\n\n\n\nRelated Papers (5)\n\n\n\n\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n[...]\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 more\n - 11 Oct 2018\n\nShow Less\n\n\n\n\nDeep contextualized word representations\n[...]\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 more\n - 15 Feb 2018\n\nShow Less\n\n\n\n\nBeto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\n[...]\nShijie Wu,Mark Dredze +1 more\n - 19 Apr 2019\n\nShow Less\n\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n[...]\nColin Raffel,Noam Shazeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena,Yanqi Zhou,Wei Li,Peter J. Liu +8 more\n - 01 Jan 2020\n - Journal of Machine Learning Research \nShow Less\n\n\n\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\n[...]\nTaku Kudo,John Richardson +1 more\n - 19 Aug 2018\n\nShow Less\n\n\n\n\n\n\nTools\nChat with PDFLiterature ReviewAI WriterFind TopicsParaphraserCitation GeneratorExtract DataAI DetectorCitation BoosterExtensions\nSciSpace Chrome Extension\nSciSpace\nAboutCareersResourcesSupportBrowse PapersPricingSciSpace Affiliate ProgramCancellation & Refund PolicyTermsPrivacyData Sources\nDirectories\nPapersTopicsJournalsAuthorsConferencesInstitutionsCitation Styles\nContact\nsupport@typeset.io+1 (760) 284-7800+91 9916292973\n\n\u00a9 2025 | PubGenius Inc. | Suite # 217 691 S Milpitas Blvd Milpitas CA 95035, USA", "Home Chat with PDF Literature Review AI Writer Find Topics Paraphraser Citation Generator Extract Data AI Detector PDF to Video Affiliate Program Chrome Extension Use on ChatGPT Contact Us Home Papers Unsupervised Cross-lingual Representation Learning at Scale Open AccessProceedings ArticleDOI Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University - 01 Jul 2020 - pp 8440-8451 Show Less Podcast 3.7K PDF Save Cite TL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time. read more Abstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "Home\n\nChat with PDF\n\nLiterature Review\n\nAI Writer\n\nFind Topics\n\nParaphraser\n\nCitation Generator\n\nExtract Data\n\nAI Detector\n\nPDF to Video\n\nAffiliate Program\n\nChrome Extension\n\n\nUse on ChatGPT\n\n\nContact Us\n\n\n\n\nHome\nPapers\nUnsupervised Cross-lingual Representation Learning at Scale\n\n\n\nOpen AccessProceedings ArticleDOI\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University\n - 01 Jul 2020\n\n - pp 8440-8451\nShow Less\n\n\n\nPodcast\n3.7K\nPDF\n\nSave\nCite\n\n\n\n\nTL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time. read more\n\n\nAbstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER."]}, {"text": "         \n\n\n\ndblp\n\n\nBlog\nStatistics\n\nUpdate feed\nXML dump\nRDF dump\n\nbrowse\n\n\nPersons\n\nConferences\nJournals\nSeries\nRepositories\n\nsearch\n\n\nSearch dblp\nSPARQL queries \n\nLookup by ID\n\nabout\n\n\nF.A.Q.\nTeam\nAdvisory Board\n\nLicense\nPrivacy\nImprint\n\nnfdi\n\n\nNFDI - National Research Data Infrastructure Germany\nconsortia\n\nNFDIxCS\nevents | twitter\n\nNFDI4DataScience\nORKG\nCEUR\nMyBinder\nevents | twitter | publications \n\n\nstop the war!\n\n\n\u0417\u0443\u043f\u0438\u043d\u0456\u0442\u044c \u0432\u0456\u0439\u043d\u0443!\n\u041e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0435 \u0432\u043e\u0439\u043d\u0443!\nsolidarity - (ua) - (ru)\nnews - (ua) - (ru)\ndonate - donate - donate\n\nfor scientists\nERA4Ukraine\nAssistance in Germany\nUkrainian Global University\n#ScienceForUkraine\n   \nWe've just launched a new service: our brand new dblp SPARQL query service. Read more about it in our latest blog post or try out some of the SPARQL queries linked on the dblp web pages below.\n \n \n \n\n\n\n\ndefault search action\n\ncombined dblp search\nauthor search\nvenue search\npublication search\n\n\nAuthors:\n\nno matches\n\n\nVenues:\n\nno matches\n\n\nPublications:\n\nno matches\n\n\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nORCID    \n\n\n\n\n \n\nDagstuhl\n\n\nTrier \n\n\n> Home  \n\n\n\n\n\nrun query for this record\n\nreferenced publications\ncited by publications\nco-cited publications\nciting venues\nco-cited venues\nor build your own?  \n\n\n\nDOI: 10.18653/V1/2020.ACL-MAIN.747\naccess: open\ntype: Conference or Workshop Paper\nmetadata version: 2022-03-16\n\n\nload more data from openalex.org\nPrivacy notice:\nBy enabling the option above, your browser will contact the API of openalex.org to load information about this publication. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex.\nYour decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time.\nlooking up data   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\ncontained concepts\n\n\nrecent citations by year\n   \n\n\n\n\n\n\n\n\nview\n\nelectronic edition via DOI (open access)\nelectronic edition @ aclanthology.org (open access)\nauthority control:\n\n \n\n\n\nexport record\n\nBibTeX\nRIS\nRDF N-Triples\nRDF Turtle\nRDF/XML\nXML\ndblp key:\n\nconf/acl/ConneauKGCWGGOZ20\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nPubPeer\n\n\n\nshare record\n\nTwitter\nReddit\nBibSonomy\nLinkedIn\nFacebook\npersistent URL:\n\nhttps://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov:\n Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451\n\nPlease note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public. For more information please see the Initiative for Open Citations (I4OC). Please also note that there is no way of submitting missing references or citation data directly to dblp.\n\n\nPlease also note that this feature is work in progress and that it is still far from being perfect. That is, in particular,\n\nthe lists below may be incomplete due to unavailable citation data,\nreference strings may not have been successfully mapped to the items listed in dblp, and\nwe do not have complete and curated metadata for all items given in these lists.\n\n\nJavaScript is requires in order to retrieve and display any references and citations for this record.\n\n\n\n\nload references from crossref.org, opencitations.net, and semanticscholar.org\nPrivacy notice:\nBy enabling the option above, your browser will contact the APIs of crossref.org, opencitations.net, and semanticscholar.org to load citation information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Crossref privacy policy and the OpenCitations privacy policy, as well as the AI2 Privacy Policy covering Semantic Scholar.\nYour decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time.\n\nlooking up references   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\n\n\n\nload citations from opencitations.net and semanticscholar.org\nPrivacy notice:\nBy enabling the option above, your browser will contact the APIs of opencitations.net and semanticscholar.org to load citation information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the OpenCitations privacy policy as well as the AI2 Privacy Policy covering Semantic Scholar.\nYour decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time.\n\nlooking up citations   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\n\n\n\nmanage site settings\n\nTo protect your privacy, all features that rely on external API calls from your browser are turned off by default. You need to opt-in for them to become active. All settings here will be stored as cookies with your web browser. For more information see our F.A.Q.\n\n\n[+][\u2013] Unpaywalled article links\n\nAdd open access links from  to the list of external document links (if available).\nload links from unpaywall.org\nPrivacy notice: By enabling the option above, your browser will contact the API of unpaywall.org to load hyperlinks to open access articles. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Unpaywall privacy policy.\n\n[+][\u2013] Archived links via Wayback Machine\n\nFor web page which are no longer available, try to retrieve content from the  of the Internet Archive (if available).\nload content from archive.org\nPrivacy notice: By enabling the option above, your browser will contact the API of archive.org to check for archived content of web pages that are no longer available. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Internet Archive privacy policy.\n\n[+][\u2013] Reference lists\n\nAdd a list of references from , , and  to record detail pages.\nload references from crossref.org and opencitations.net\nPrivacy notice: By enabling the option above, your browser will contact the APIs of crossref.org, opencitations.net, and semanticscholar.org to load article reference information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Crossref privacy policy and the OpenCitations privacy policy, as well as the AI2 Privacy Policy covering Semantic Scholar.\n\n[+][\u2013] Citation data\n\nAdd a list of citing articles from  and  to record detail pages.\nload citations from opencitations.net\nPrivacy notice: By enabling the option above, your browser will contact the API of opencitations.net and semanticscholar.org to load citation information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the OpenCitations privacy policy as well as the AI2 Privacy Policy covering Semantic Scholar.\n\n[+][\u2013] OpenAlex data \n\nLoad additional information about publications from .\nload data from openalex.org\nPrivacy notice: By enabling the option above, your browser will contact the API of openalex.org to load additional information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex.\n\n\n\n  last updated on 2022-03-16 23:55 CET by the dblp team\n all metadata released as open data under CC0 1.0 license\nsee also: Terms of Use | Privacy Policy | Imprint\n\ndblp was originally created in 1993 at:   \n\nsince 2018, dblp has been operated and maintained by:   \n\nthe dblp computer science bibliography is funded and supported by:                   \n  ", "title": "dblp: Unsupervised Cross-lingual Representation Learning at Scale.", "description": "Bibliographic details on <strong>Unsupervised</strong> <strong>Cross</strong>-<strong>lingual</strong> <strong>Representation</strong> <strong>Learning</strong> <strong>at</strong> <strong>Scale</strong>.", "usage": {"tokens": 2041}, "retrieved_text": ["conf/acl/ConneauKGCWGGOZ20 ask others Google Google Scholar Semantic Scholar Internet Archive Scholar CiteSeerX PubPeer share record Twitter Reddit BibSonomy LinkedIn Facebook persistent URL: https://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451 Please note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public. For more information please see the Initiative for Open Citations (I4OC). Please also note that there is no way of submitting missing references or citation data directly to dblp. Please also note that this feature is work in progress and that it is still far from being perfect. That is, in particular,\n\nthe lists below may be incomplete due to unavailable citation data,\nreference strings may not have been successfully mapped to the items listed in dblp, and\nwe do not have complete and curated metadata for all items given in these lists.", "uses your data. So please proceed with care and consider checking the information given by OpenAlex. Your decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time. looking up data no open data available, sorry service temporarily unavailable, please try again later contained concepts recent citations by year view electronic edition via DOI (open access) electronic edition @ aclanthology.org (open access) authority control: export record BibTeX RIS RDF N-Triples RDF Turtle RDF/XML XML dblp key: conf/acl/ConneauKGCWGGOZ20 ask others Google Google Scholar Semantic Scholar Internet Archive Scholar CiteSeerX PubPeer share record Twitter Reddit BibSonomy LinkedIn Facebook persistent URL: https://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451\n\nPlease note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public.", "publications citing venues co-cited venues or build your own? DOI: 10.18653/V1/2020.ACL-MAIN.747 access: open type: Conference or Workshop Paper metadata version: 2022-03-16 load more data from openalex.org Privacy notice: By enabling the option above, your browser will contact the API of openalex.org to load information about this publication. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex. Your decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time. looking up data   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\ncontained concepts\n\n\nrecent citations by year\n   \n\n\n\n\n\n\n\n\nview\n\nelectronic edition via DOI (open access)\nelectronic edition @ aclanthology.org (open access)\nauthority control:\n\n \n\n\n\nexport record\n\nBibTeX\nRIS\nRDF N-Triples\nRDF Turtle\nRDF/XML\nXML\ndblp key:\n\nconf/acl/ConneauKGCWGGOZ20\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nPubPeer\n\n\n\nshare record\n\nTwitter\nReddit\nBibSonomy\nLinkedIn\nFacebook\npersistent URL:\n\nhttps://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov:\n Unsupervised Cross-lingual Representation Learning at Scale."]}, {"text": "     \n     \n   \n \n \n \n    \n   \n  Browse State-of-the-Art   \n  Datasets   \n Methods  \n  More  \n Newsletter RC2022 \n About Trends  Portals   Libraries     \n \n     \n Sign In      \n \n \n \n \nSubscribe to the PwC Newsletter \n \u00d7   \n \n \n Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\n\n Read previous issues     \n \nSubscribe       \n \n \n \n \nJoin the community \n \u00d7   \n You need to log in to edit.\n You can create a new account if you don't have one.\n\n       \n \n \n \n \nEdit Social Preview \n \u00d7   \n    \n Description\n \n  Default  \n  Custom   \n \n Implemented in 31 code libraries.  \n    \n Image\n \n \n  Default  \n  Custom  \n  None    \n \n   \n \n File is too large  \n Upload an image to customize your repository\u2019s social media preview.\n Images should be at least 640\u00d7320px (1280\u00d7640px for best display).    \n    \n \n Close  \n Save       \n \n \n \n \nAdd a new code entry for this paper \n \u00d7   \n \n  \n  GitHub, GitLab or BitBucket URL:*  \n    \n \n   Official code from paper authors     \n \nSubmit        \n \n \n \n \nRemove a code repository from this paper \n \u00d7   \n \n \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n    \n \n   \n-     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n   \n \n   \n-     \n \n \n   huggingface/transformers    \n \n  137,405   \n \n    \n \n   \n-     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n    \n \n   \n-     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n    \n \n   \n-     \n \n \n   Unbabel/COMET    \n \n  525   \n \n    \n \n   \n-     \n \n \n   google-deepmind/xquad    \n \n  184   \n \n   \n \n   \n-     \n \n \n   deepmind/xquad    \n \n  184   \n \n   \n \n   \n-     \n \n \n   apple/ml-mkqa    \n \n  171   \n \n   \n \n   \n-     \n \n \n   inzva/turkish-glove    \n \n  67   \n \n   \n \n   \n-     \n \n \n   bitextor/bicleaner-ai    \n \n  39   \n \n    \n \n   \n-     \n \n \n   ilhamfp/indonesian-text-classification-multilingual    \n \n  29   \n \n    \n \n   \n-     \n \n \n   Tikquuss/meta_XLM    \n \n  20   \n \n    \n \n   \n-     \n \n \n   ashwanitanwar/nmt-transfer-learning-xlm-r    \n \n  18   \n \n    \n \n   \n-     \n \n \n   heraclex12/vietpunc    \n \n  13   \n \n    \n \n   \n-     \n \n \n   huhailinguist/ChineseNLIProbing    \n \n  10   \n \n    \n \n   \n-     \n \n \n   martincjespersen/DaAnonymization    \n \n  9   \n \n   \n \n   \n-     \n \n \n   YuweiYin/HLT-MT    \n \n  4   \n \n    \n \n   \n-     \n \n \n   stanleyjzheng/ignitionhacks    \n \n  3   \n \n    \n \n   \n-     \n \n \n   Stanley-Zheng/ignitionhacks    \n \n  3   \n \n    \n \n   \n-     \n \n \n   deterministic-algorithms-lab/Large-XLM    \n \n  2   \n \n    \n \n   \n-     \n \n \n   MindCode-4/code-5    \n \n  1   \n \n    \n \n   \n-     \n \n \n   somiltg/xquad    \n \n  1   \n \n   \n \n   \n-     \n \n \n   pwc-1/Paper-9    \n \n  1   \n \n    \n \n   \n-     \n \n \n   bcaitech1/p2-klue-Heeseok-Jeong    \n \n  1   \n \n    \n \n   \n-     \n \n \n   MindCode-4/code-13    \n \n  0   \n \n    \n \n   \n-     \n \n \n   aleksac99/SQuAD-SR    \n \n  0   \n \n   \n \n   \n-     \n \n \n   lsqdecodebox/NLP-Comptition-review    \n \n  0   \n \n   \n \n   \n-     \n \n \n   feyzaakyurek/XLM-LwLL    \n \n  0   \n \n    \n \n   \n-     \n \n \n   kheeong/XLM_OWN    \n \n  0   \n \n    \n \n   \n-     \n \n \n   1-punchMan/CLTS    \n \n  0   \n \n    \n \n   \n-            \n \n \n \n \n Mark the official implementation from paper authors  \n \u00d7   \n   \n \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n    \n     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n   \n     \n \n \n   huggingface/transformers    \n \n  137,405   \n \n    \n     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n    \n     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n    \n     \n \n \n   Unbabel/COMET    \n \n  525   \n \n    \n     \n \n \n   google-deepmind/xquad    \n \n  184   \n \n   \n     \n \n \n   deepmind/xquad    \n \n  184   \n \n   \n     \n \n \n   apple/ml-mkqa    \n \n  171   \n \n   \n     \n \n \n   inzva/turkish-glove    \n \n  67   \n \n   \n     \n \n \n   bitextor/bicleaner-ai    \n \n  39   \n \n    \n     \n \n \n   ilhamfp/indonesian-text-classification-multilingual    \n \n  29   \n \n    \n     \n \n \n   Tikquuss/meta_XLM    \n \n  20   \n \n    \n     \n \n \n   ashwanitanwar/nmt-transfer-learning-xlm-r    \n \n  18   \n \n    \n     \n \n \n   heraclex12/vietpunc    \n \n  13   \n \n    \n     \n \n \n   huhailinguist/ChineseNLIProbing    \n \n  10   \n \n    \n     \n \n \n   martincjespersen/DaAnonymization    \n \n  9   \n \n   \n     \n \n \n   YuweiYin/HLT-MT    \n \n  4   \n \n    \n     \n \n \n   stanleyjzheng/ignitionhacks    \n \n  3   \n \n    \n     \n \n \n   Stanley-Zheng/ignitionhacks    \n \n  3   \n \n    \n     \n \n \n   deterministic-algorithms-lab/Large-XLM    \n \n  2   \n \n    \n     \n \n \n   MindCode-4/code-5    \n \n  1   \n \n    \n     \n \n \n   somiltg/xquad    \n \n  1   \n \n   \n     \n \n \n   pwc-1/Paper-9    \n \n  1   \n \n    \n     \n \n \n   bcaitech1/p2-klue-Heeseok-Jeong    \n \n  1   \n \n    \n     \n \n \n   MindCode-4/code-13    \n \n  0   \n \n    \n     \n \n \n   aleksac99/SQuAD-SR    \n \n  0   \n \n   \n     \n \n \n   lsqdecodebox/NLP-Comptition-review    \n \n  0   \n \n   \n     \n \n \n   feyzaakyurek/XLM-LwLL    \n \n  0   \n \n    \n     \n \n \n   kheeong/XLM_OWN    \n \n  0   \n \n    \n     \n \n \n   1-punchMan/CLTS    \n \n  0   \n \n    \n     \n  \n \n \n There is no official implementation   \n      \n \n \n Multiple official implementations   \n \n        \n \nSubmit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n  \n  Task:*  \n  --------- Multilingual NLP XLM-R Cross-Lingual Transfer Language Modelling Representation Learning    \n Not in the list? Add a task.  \n  Dataset:*  \n  ---------    \n  Model name:*  \n    \n  Metric name:*  \n  ---------    \n  \n \n   Higher is better (for the metric)    \n  Metric value:*  \n    \n  \n \n   Uses extra training data    \n  Data evaluated on  \n     \n \n Submit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n \n   TASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE          \n \n \n \n \nAdd a task \n \u00d7   \n \nAttached tasks: \n \n  CROSS-LINGUAL TRANSFER    \n \n  LANGUAGE MODELLING    \n \n  MULTILINGUAL NLP    \n \n  REPRESENTATION LEARNING    \n \n  XLM-R    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new task.  \n \n  New task name:  \n    \n  Top-level area:  \n  --------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series    \n  Parent task (if any):  \n  ---------    \n  Description:  \n     \n \n Submit         \n \n \n \n \nRemove a task \n \u00d7   \n \n \n \n \n    Cross-Lingual Transfer     \n-     \n \n \n    Language Modelling     \n-     \n \n \n    Multilingual NLP     \n-     \n \n \n    Representation Learning     \n-     \n \n \n    XLM-R     \n-           \n \n \n \n \nAdd a method \n \u00d7   \n \nAttached methods: \n \n  ADAM    \n \n  ATTENTION DROPOUT    \n \n  BERT    \n \n  BPE    \n \n  DENSE CONNECTIONS    \n \n  DROPOUT    \n \n  GELU    \n \n  LAYER NORMALIZATION    \n \n  LINEAR LAYER    \n \n  LINEAR WARMUP WITH LINEAR DECAY    \n \n  MULTI-HEAD ATTENTION    \n \n  RESIDUAL CONNECTION    \n \n  SCALED DOT-PRODUCT ATTENTION    \n \n  SOFTMAX    \n \n  WEIGHT DECAY    \n \n  WORDPIECE    \n \n  XLM    \n \n  XLM-R    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new method.  \n \n  New method name (e.g. ReLU):  \n    \n  New method full name (e.g. Rectified Linear Unit):  \n    \n  Paper where method was first introduced:  \n  ---------    \n  Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.  \n  ---------    \n  Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.  \n     \n \n Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n \n \n \n  ADAM    \n-     \n \n \n  ATTENTION DROPOUT    \n-     \n \n \n  BERT    \n-     \n \n \n  BPE    \n-     \n \n \n  DENSE CONNECTIONS    \n-     \n \n \n  DROPOUT    \n-     \n \n \n  GELU    \n-     \n \n \n  LAYER NORMALIZATION    \n-     \n \n \n  LINEAR LAYER    \n-     \n \n \n  LINEAR WARMUP WITH LINEAR DECAY    \n-     \n \n \n  MULTI-HEAD ATTENTION    \n-     \n \n \n  RESIDUAL CONNECTION    \n-     \n \n \n  SCALED DOT-PRODUCT ATTENTION    \n-     \n \n \n  SOFTMAX    \n-     \n \n \n  WEIGHT DECAY    \n-     \n \n \n  WORDPIECE    \n-     \n \n \n  XLM    \n-     \n \n \n  XLM-R    \n-           \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  CC100 \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  CoNLL 2002 GLUE MLQA QNLI SQuAD SST \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? \n   Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Cross-lingual Representation Learning at Scale  \n \n   ACL 2020    \u00b7   Alexis Conneau,  Kartikay Khandelwal,  Naman Goyal,  Vishrav Chaudhary,  Guillaume Wenzek,  Francisco Guzm\u00e1n,  Edouard Grave,  Myle Ott,  Luke Zettlemoyer,  Veselin Stoyanov \u00b7 \n  Edit social preview       \n \n \n \n This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.    PDF    Abstract     ACL 2020 PDF     ACL 2020 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n    \n \n \n   huggingface/transformers    \n \n  137,405   \n \n     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n     \n See all 31 implementations   \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n    \n \n \n   huggingface/transformers    \n \n  137,405   \n \n     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n     \n \n \n   Unbabel/COMET    \n \n  525   \n \n     \n \n \n   google-deepmind/xquad    \n \n  184   \n \n    \n \n \n   deepmind/xquad    \n \n  184   \n \n    \n \n \n   apple/ml-mkqa    \n \n  171   \n \n    \n \n \n   inzva/turkish-glove    \n \n  67   \n \n    \n \n \n   bitextor/bicleaner-ai    \n \n  39   \n \n     \n \n \n   ilhamfp/indonesian-text-classificat\u2026    \n \n  29   \n \n     \n \n \n   Tikquuss/meta_XLM   \n   \u21b3 Quickstart in   \n   Colab         \n \n  20   \n \n     \n \n \n   ashwanitanwar/nmt-transfer-learning\u2026    \n \n  18   \n \n     \n \n \n   heraclex12/vietpunc    \n \n  13   \n \n     \n \n \n   huhailinguist/ChineseNLIProbing    \n \n  10   \n \n     \n \n \n   martincjespersen/DaAnonymization    \n \n  9   \n \n    \n \n \n   YuweiYin/HLT-MT    \n \n  4   \n \n     \n \n \n   stanleyjzheng/ignitionhacks    \n \n  3   \n \n     \n \n \n   Stanley-Zheng/ignitionhacks    \n \n  3   \n \n     \n \n \n   deterministic-algorithms-lab/Large-\u2026    \n \n  2   \n \n     \n \n \n   MindCode-4/code-5    \n \n  1   \n \n     \n \n \n   somiltg/xquad    \n \n  1   \n \n    \n \n \n   pwc-1/Paper-9    \n \n  1   \n \n     \n \n \n   bcaitech1/p2-klue-Heeseok-Jeong    \n \n  1   \n \n     \n \n \n   MindCode-4/code-13    \n \n  0   \n \n     \n \n \n   aleksac99/SQuAD-SR    \n \n  0   \n \n    \n \n \n   lsqdecodebox/NLP-Comptition-review    \n \n  0   \n \n    \n \n \n   feyzaakyurek/XLM-LwLL    \n \n  0   \n \n     \n \n \n   kheeong/XLM_OWN    \n \n  0   \n \n     \n \n \n   1-punchMan/CLTS    \n \n  0   \n \n     \n Collapse 31 implementations     \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Cross-Lingual Transfer      Language Modelling      Multilingual NLP      Representation Learning      XLM-R         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \nIntroduced in the Paper:    CC100   \nUsed in the Paper:    GLUE      SST      SQuAD      QNLI      MLQA      CoNLL 2002          \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.       \n \n \n    \n \n \n \n \n \n Methods \n \n \n  Edit  \n   Add   Remove           \n  Adam \u2022  Attention Dropout \u2022  BERT \u2022  BPE \u2022  Dense Connections \u2022  Dropout \u2022  GELU \u2022  Layer Normalization \u2022  Linear Layer \u2022  Linear Warmup With Linear Decay \u2022  Multi-Head Attention \u2022  Residual Connection \u2022  Scaled Dot-Product Attention \u2022  Softmax \u2022  Weight Decay \u2022  WordPiece \u2022  XLM \u2022  XLM-R    \n \n Contact us on:   hello@paperswithcode.com .  Papers With Code is a free resource with all data licensed under CC-BY-SA.   \n Terms Data policy Cookies policy  from       ", "title": "Unsupervised Cross-lingual Representation Learning at Scale | Papers With Code", "description": "Upload an image to customize your repository\u2019s social media preview. Images should be at least 640\u00d7320px (1280\u00d7640px for best display) \u00b7 ACL 2020 \u00b7 Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke ...", "usage": {"tokens": 3837}, "retrieved_text": ["in this paper: CC100 [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: CoNLL 2002 GLUE MLQA QNLI SQuAD SST [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Cross-lingual Representation Learning at Scale ACL 2020 \u00b7 Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \u00b7 Edit social preview This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "Rectified Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 ADAM - ATTENTION DROPOUT - BERT - BPE - DENSE CONNECTIONS - DROPOUT - GELU - LAYER NORMALIZATION - LINEAR LAYER - LINEAR WARMUP WITH LINEAR DECAY - MULTI-HEAD ATTENTION - RESIDUAL CONNECTION - SCALED DOT-PRODUCT ATTENTION - SOFTMAX - WEIGHT DECAY - WORDPIECE - XLM - XLM-R - Edit Datasets \u00d7 Add or remove datasets introduced in this paper: CC100 [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: CoNLL 2002 GLUE MLQA QNLI SQuAD SST [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Cross-lingual Representation Learning at Scale  \n \n   ACL 2020    \u00b7   Alexis Conneau,  Kartikay Khandelwal,  Naman Goyal,  Vishrav Chaudhary,  Guillaume Wenzek,  Francisco Guzm\u00e1n,  Edouard Grave,  Myle Ott,  Luke Zettlemoyer,  Veselin Stoyanov \u00b7 \n  Edit social preview       \n \n \n \n This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.", "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \u00b7 Edit social preview This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks."]}], "The paper titled \"Learning to Translate in Real-time with Neural Machine Translation\" describes a method for real-time neural machine translation that can handle streaming inputs without waiting for the entire input sequence.": [{"text": " \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1610.00388 (cs)   \n \n \n [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] \nTitle:Learning to Translate in Real-time with Neural Machine Translation \nAuthors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li \nView a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF \n Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.   \n   Comments: 10 pages, camera ready   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1610.00388 [cs.CL]     (or  arXiv:1610.00388v3 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1610.00388\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Jiatao Gu [view email] \n [v1] Mon, 3 Oct 2016 02:11:03 UTC (1,464 KB)\n [v2] Thu, 6 Oct 2016 00:46:39 UTC (1,467 KB)\n [v3] Tue, 10 Jan 2017 21:07:56 UTC (2,974 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2016-10 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Jiatao Gu\nGraham Neubig\nKyunghyun Cho\nVictor O. K. Li  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1610.00388] Learning to Translate in Real-time with Neural Machine Translation", "description": "<strong>Translating</strong> <strong>in</strong> <strong>real</strong>-<strong>time</strong>, <strong>a</strong>.k.a. simultaneous <strong>translation</strong>, outputs <strong>translation</strong> words before <strong>the</strong> <strong>input</strong> sentence ends, which is a challenging problem for conventional <strong>machine</strong> <strong>translation</strong> <strong>methods</strong>. We propose <strong>a</strong> <strong>neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) framework for simultaneous <strong>translation</strong> <strong>in</strong> which an ...", "usage": {"tokens": 1314}, "retrieved_text": [" \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1610.00388 (cs)   \n \n \n [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] \nTitle:Learning to Translate in Real-time with Neural Machine Translation \nAuthors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li \nView a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF \n Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.", "Computer Science > Computation and Language arXiv:1610.00388 (cs) [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] Title:Learning to Translate in Real-time with Neural Machine Translation Authors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. Comments: 10 pages, camera ready   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1610.00388 [cs.CL]     (or  arXiv:1610.00388v3 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1610.00388\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Jiatao Gu [view email] \n [v1] Mon, 3 Oct 2016 02:11:03 UTC (1,464 KB)\n [v2] Thu, 6 Oct 2016 00:46:39 UTC (1,467 KB)\n [v3] Tue, 10 Jan 2017 21:07:56 UTC (2,974 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2016-10 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Jiatao Gu\nGraham Neubig\nKyunghyun Cho\nVictor O. K. Li  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)", "(What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack"]}, {"warning": "Target URL returned error 403: Forbidden", "text": " \n     \n \n\n\n\n\n\n\n\nWe've noticed some unusual traffic coming from your network. To continue, please check the box below.\n\n\nEnable JavaScript and cookies to continue\n\n\n\n\n\nAre you a researcher?\nTo avoid being denied access, log in if you're a ResearchGate member or create an account if you're not.\n\n\n\n\nLog in\nCreate account\n\n\nRay ID: 8ffd03d73b82aa45\nClient IP: 2600:1900:0:2d07::c00\n\u00a9 2008-2025 ResearchGate GmbH. All rights reserved.  ", "title": "Learning to Translate in Real-time with Neural Machine Translation | Request PDF", "description": "Request PDF | On Jan 1, 2017, Jiatao Gu and others published <strong>Learning</strong> <strong>to</strong> <strong>Translate</strong> <strong>in</strong> <strong>Real</strong>-<strong>time</strong> <strong>with</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> | Find, read and cite all the research you need on ResearchGate", "usage": {"tokens": 219}, "retrieved_text": ["Request PDF | On Jan 1, 2017, Jiatao Gu and others published <strong>Learning</strong> <strong>to</strong> <strong>Translate</strong> <strong>in</strong> <strong>Real</strong>-<strong>time</strong> <strong>with</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> | Find, read and cite all the research you need on ResearchGate"]}, {"text": "     \n     \n   \n \n \n \n    \n   \n  Browse State-of-the-Art   \n  Datasets   \n Methods  \n  More  \n Newsletter RC2022 \n About Trends  Portals   Libraries     \n \n     \n Sign In      \n \n \n \n \nSubscribe to the PwC Newsletter \n \u00d7   \n \n \n Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\n\n Read previous issues     \n \nSubscribe       \n \n \n \n \nJoin the community \n \u00d7   \n You need to log in to edit.\n You can create a new account if you don't have one.\n\n       \n \n \n \n \nEdit Social Preview \n \u00d7   \n    \n Description\n \n  Default  \n  Custom   \n \n Implemented in one code library.  \n    \n Image\n \n \n  Default  \n  Custom  \n  None    \n \n   \n \n File is too large  \n Upload an image to customize your repository\u2019s social media preview.\n Images should be at least 640\u00d7320px (1280\u00d7640px for best display).    \n    \n \n Close  \n Save       \n \n \n \n \nAdd a new code entry for this paper \n \u00d7   \n \n  \n  GitHub, GitLab or BitBucket URL:*  \n    \n \n   Official code from paper authors     \n \nSubmit        \n \n \n \n \nRemove a code repository from this paper \n \u00d7   \n \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n   \n \n   \n-            \n \n \n \n \n Mark the official implementation from paper authors  \n \u00d7   \n   \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n   \n     \n  \n \n \n There is no official implementation   \n      \n \n \n Multiple official implementations   \n \n        \n \nSubmit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n  \n  Task:*  \n  --------- Sentence NMT Translation Machine Translation    \n Not in the list? Add a task.  \n  Dataset:*  \n  ---------    \n  Model name:*  \n    \n  Metric name:*  \n  ---------    \n  \n \n   Higher is better (for the metric)    \n  Metric value:*  \n    \n  \n \n   Uses extra training data    \n  Data evaluated on  \n     \n \n Submit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n \n   TASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE          \n \n \n \n \nAdd a task \n \u00d7   \n \nAttached tasks: \n \n  MACHINE TRANSLATION    \n \n  NMT    \n \n  SENTENCE    \n \n  TRANSLATION    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new task.  \n \n  New task name:  \n    \n  Top-level area:  \n  --------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series    \n  Parent task (if any):  \n  ---------    \n  Description:  \n     \n \n Submit         \n \n \n \n \nRemove a task \n \u00d7   \n \n \n \n \n    Machine Translation     \n-     \n \n \n    NMT     \n-     \n \n \n    Sentence     \n-     \n \n \n    Translation     \n-           \n \n \n \n \nAdd a method \n \u00d7   \n \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new method.  \n \n  New method name (e.g. ReLU):  \n    \n  New method full name (e.g. Rectified Linear Unit):  \n    \n  Paper where method was first introduced:  \n  ---------    \n  Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.  \n  ---------    \n  Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.  \n     \n \n Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? \n   Add a new dataset here     \n \n Save        \n  \n \n \n \n Learning to Translate in Real-time with Neural Machine Translation  \n \n   EACL 2017    \u00b7   Jiatao Gu,  Graham Neubig,  Kyunghyun Cho,  Victor O. K. Li \u00b7 \n  Edit social preview       \n \n \n \n Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.    PDF    Abstract     EACL 2017 PDF     EACL 2017 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n     \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n       \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Machine Translation      NMT      Sentence      Translation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n \n  Edit  \n   Add   Remove      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.       \n \n \n    \n \n \n \n \n \n Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com .  Papers With Code is a free resource with all data licensed under CC-BY-SA.   \n Terms Data policy Cookies policy  from       ", "title": "Papers with Code - Learning to Translate in Real-time with Neural Machine Translation", "description": "Upload an image to customize your repository\u2019s social media preview. Images should be at least 640\u00d7320px (1280\u00d7640px for best display) \u00b7 EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview", "usage": {"tokens": 1370}, "retrieved_text": ["to Translate in Real-time with Neural Machine Translation EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. PDF    Abstract     EACL 2017 PDF     EACL 2017 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n     \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n       \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Machine Translation      NMT      Sentence      Translation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n \n  Edit  \n   Add   Remove      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers. Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper.", "Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 Edit Datasets \u00d7 Add or remove datasets introduced in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Learning to Translate in Real-time with Neural Machine Translation EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment.", "task \u00d7 Machine Translation - NMT - Sentence - Translation - Add a method \u00d7 Add: --------- Not in the list? Create a new method. New method name (e.g. ReLU): New method full name (e.g. Rectified Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? Add a new dataset here     \n \n Save        \n  \n \n \n \n Learning to Translate in Real-time with Neural Machine Translation  \n \n   EACL 2017    \u00b7   Jiatao Gu,  Graham Neubig,  Kyunghyun Cho,  Victor O. K. Li \u00b7 \n  Edit social preview       \n \n \n \n Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods."]}, {"text": "   \n\n\n\nSkip to search formSkip to main contentSkip to account menu\n\n\n\n\n\n\n\n\n\nDOI:10.18653/V1/E17-1099\nCorpus ID: 2782776\nLearning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} }\n\nGraham Neubig, Kyunghyun Cho, \n+1 author V. Li\nPublished in Conference of the European\u2026 3 October 2016\nComputer Science, Linguistics\n\n\n\nTLDR\nA neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed.\nExpand\n\n\n[PDF] Semantic Reader\n\n\n\n\n\n\n\n\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n\n\n\n209 Citations\n\n\n\nHighly Influential Citations\n36\n\n\n\nBackground Citations\n142\n\n\n Methods Citations\n73\n\n\n Results Citations\n4\nView All\n\n\n\n\n\nFigures from this paper\n\n\n\n\n\n\n\nfigure 1\n\n\n\nfigure 2\n\n\n\nfigure 3\n\n\n\nfigure 4\n\n\n\nfigure 5\n\n\n\nfigure 6\n\n\n\nfigure 7\n\n\n\nfigure 8\n\nView All 8 Figures & Tables\n\n\n\n\nTopics\n\n\nAI-Generated\n\n\nSimultaneous Translation (opens in a new tab)Simultaneous Machine Translation (opens in a new tab)Translation Delays (opens in a new tab)Simultaneous Translation Task (opens in a new tab)READ/WRITE Decision (opens in a new tab)Simultaneous Translation System (opens in a new tab)Average Proportion (opens in a new tab)Simultaneous Mt (opens in a new tab)Neural Machine Translation (opens in a new tab)Translate (opens in a new tab)\n\n\n\n\n209 Citations\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nA General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation\nYun ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nAACL\n2020\n\n\n\nTLDR\nA general framework for adapting neural machine translation to translate simultaneously and contains a prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation.\nExpand\n\n\n\n\n2\n\nHighly Influenced\n\n\nPDF\n\n\n\n\n\n12 Excerpts\n\n\nSave\n\n\nHow to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?\nYuxiang ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nArXiv\n2019\n\n\n\nTLDR\nA general framework to improve simultaneous translation with a pretrained consecutive neural machine translation (CNMT) model is proposed that contains two parts: prefix translation that utilizes a Pretrained CNMT model to better translate source prefixes and a stopping criterion that determines when to stop the prefix translation.\nExpand\n\n\n\n\n2\n\nHighly Influenced[PDF]\n\n\n\n\n\n9 Excerpts\n\n\nSave\n\n\nNeural Simultaneous Speech Translation Using Alignment-Based Chunking\nP. WilkenTamer AlkhouliE. MatusovPavel Golik\nComputer Science\nIWSLT\n2020\n\n\n\nTLDR\nA neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words, and compares models with bidirectional and unidirectional encoders of different depths, both on real speech and text input.\nExpand\n\n\n\n\n14\n\nHighly Influenced[PDF]\n\n\n\n\n\n4 Excerpts\n\n\nSave\n\n\nSimultaneous Neural Machine Translation with Constituent Label Prediction\nYasumasa KanoKatsuhito SudohSatoshi Nakamura\nComputer Science, Linguistics\nWMT\n2021\n\n\n\nTLDR\nMotivated by the concept of pre-reordering, a couple of simple decision rules are proposed using the label of the next constituent predicted by incremental constituent label prediction to decide when to start the translation process.\nExpand\n\n\n\n\n3[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nA Generative Framework for Simultaneous Machine Translation\nYishu MiaoPhil BlunsomLucia Specia\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work formulate simultaneous translation as a structural sequence-to-sequence learning problem, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.\nExpand\n\n\n\n\n24\n\nHighly Influenced\n\n\nPDF\n\n\n\n\n\n8 Excerpts\n\n\nSave\n\n\nReinforcement Learning for on-line Sequence Transformation\nGrzegorz RypescLukasz LepakPawel Wawrzy'nski\nComputer Science\n2022 17th Conference on Computer Science and\u2026\n2022\n\n\n\nTLDR\nThis paper introduces an SMT system that learns with reinforcement and is able to find the optimal delay in training, and conducts experiments on Tatoeba and IWSLT2014 datasets against state-of-the-art translation architectures.\nExpand\n\n\n[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nTowards Multimodal Simultaneous Neural Machine Translation\nAizhan ImankulovaMasahiro KanekoTosho HirasawaMamoru Komachi\nComputer Science\nWMT\n2020\n\n\n\nTLDR\nThe experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency, and verified the importance of visual information during decoding by performing an adversarial evaluation ofMSNMT.\nExpand\n\n\n\n\n15[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nSimultaneous Neural Machine Translation using Connectionist Temporal Classification\nKatsuki ChousaKatsuhito SudohSatoshi Nakamura\nComputer Science\nArXiv\n2019\n\n\n\nTLDR\nA neural machine translation method is proposed to determine when to start the translation for observed inputs so far in an adaptive manner and introduces a special token ' ', which is generated when the translation model chooses to read the next input token instead of generating an output token.\nExpand\n\n\n\n\n5[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nTranslation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation\nAshkan AlinejadHassan S. ShavaraniAnoop Sarkar\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work proposes a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences.\nExpand\n\n\n\n\n10\n\n\nPDF\n\n\n\n\n\n3 Excerpts\n\n\nSave\n\n\nAnticipation-Free Training for Simultaneous Machine Translation\nChih-Chiang ChangShun-Po ChuangHung-yi Lee\nComputer Science\nIWSLT\n2022\n\n\n\nTLDR\nA new framework is proposed that decompose the translation process into the monotonic translation step and the reordering step, and the latter is model by the auxiliary sorting network (ASN), so that the SimulMT model could learn to translate more reasonably.\nExpand\n\n\n\n\n6[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\n\n\n\n\n...\n\n1\n\n2\n\n3\n\n4\n\n5\n\n...\n\n\n\n\n\n\n24 References\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nSimultaneous Machine Translation using Deep Reinforcement Learning\n\nComputer Science, Linguistics\n2016\n\n\n\nTLDR\nA framework which uses the existing neural machine translation systems to function as a simultaneous machine translation system and shows that the policy learnt by the system out-performs the monotone and the batch translation policies while maintaining a delay-accuracy trade-off.\nExpand\n\n\n\n\n40\n\n\nPDF\n\n\n\nSave\n\n\nNeural Machine Translation by Jointly Learning to Align and Translate\nDzmitry BahdanauKyunghyun ChoYoshua Bengio\nComputer Science\nICLR\n2015\n\n\n\nTLDR\nIt is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.\nExpand\n\n\n\n\n26,339[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nCan neural machine translation do simultaneous translation?\nKyunghyun ChoMasha Esipova\nComputer Science, Linguistics\nArXiv\n2016\n\n\n\nTLDR\nA novel decoding algorithm is introduced that allows an existing neural machine translation model to begin translating before a full source sentence is received and is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality.\nExpand\n\n\n\n\n149\n\nHighly Influential[PDF]\n\n\n\n\n\n7 Excerpts\n\n\nSave\n\n\nDon't Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation\nAlvin Grissom IIHe HeJordan L. Boyd-GraberJohn MorganHal Daum\u00e9\nComputer Science, Linguistics\nEMNLP\n2014\n\n\n\nTLDR\nA reinforcement learningbased approach to simultaneous machine translation\u2014producing a translation while receiving input words\u2014 between languages with drastically different word orders, which shows that the new translation model outperforms batch and monotone translation strategies.\nExpand\n\n\n\n\n120\n\n\nPDF\n\n\n\nSave\n\n\nSimple, lexicalized choice of translation timing for simultaneous speech translation\nTomoki FujitaGraham NeubigS. SaktiT. TodaSatoshi Nakamura\nComputer Science\nINTERSPEECH\n2013\n\n\n\nTLDR\nThis work proposes a method that uses lexicalized information to perform translation unit segmentation considering the relationship between the source and target languages and shows that this system can achieve a delay reduction of 20% compared to pause segmentation with identical accuracy.\nExpand\n\n\n\n\n46\n\n\nPDF\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nIncremental Segmentation and Decoding Strategies for Simultaneous Translation\nM. YarmohammadiV. SridharS. BangaloreB. Sankaran\nComputer Science, Linguistics\nIJCNLP\n2013\n\n\n\nTLDR\nA novel input segmentation method is proposed using the phrase alignment structure of the language pair to model the human interpretation process and compare and contrast three incremental decoding and two differentinput segmentation strategies for simultaneous translation.\nExpand\n\n\n\n\n35\n\n\nPDF\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nLearning online alignments with continuous rewards policy gradient\nYuping LuoChung-Cheng ChiuN. JaitlyI. Sutskever\nComputer Science\n2017 IEEE International Conference on Acoustics\u2026\n2017\n\n\n\nTLDR\nThis work presents a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments, which achieves encouraging performance on TIMIT and Wall Street Journal speech recognition datasets.\nExpand\n\n\n\n\n46[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nSegmentation Strategies for Streaming Speech Translation\nV. SridharJohn ChenS. BangaloreA. LjoljeR. Chengalvarayan\nComputer Science, Linguistics\nNAACL\n2013\n\n\n\nTLDR\nThe study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics, and demonstrates that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma- based segmentation.\nExpand\n\n\n\n\n69\n\n\nPDF\n\n\n\nSave\n\n\nOnline Segment to Segment Neural Transduction\nLei YuJan BuysPhil Blunsom\nComputer Science\nEMNLP\n2016\n\n\n\nTLDR\nAn online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read that tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states.\nExpand\n\n\n\n\n82[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nSequence to Sequence Learning with Neural Networks\nI. SutskeverO. VinyalsQuoc V. Le\nComputer Science\nNIPS\n2014\n\n\n\nTLDR\nThis paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing the order of the words in all source sentences improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\nExpand\n\n\n\n\n19,796[PDF]\n\n\n\nSave\n\n\n\n\n\n\n...\n\n1\n\n2\n\n3\n\n...\n\n\n\nRelated Papers\n\nShowing 1 through 3 of 0 Related Papers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures\nTopics\n209 Citations\n24 References\nRelated Papers\n\n\n\n\nStay Connected With Semantic Scholar\n\nSign Up\n\n\nWhat Is Semantic Scholar?\nSemantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.Learn More\n\n\nAboutAbout UsMeet the TeamPublishersBlog (opens in a new tab)Ai2 Careers (opens in a new tab)\n\nProductProduct OverviewSemantic ReaderScholar's HubBeta ProgramRelease Notes\n\nAPIAPI OverviewAPI TutorialsAPI Documentation (opens in a new tab)API Gallery\n\nResearchPublicationsResearchersResearch CareersPrototypesResources\n\nHelpFAQLibrariansTutorials\nContact\n\n\nProudly built by Ai2 (opens in a new tab)\n\nCollaborators & Attributions \u2022Terms of Service (opens in a new tab)\u2022Privacy Policy (opens in a new tab)\u2022API License AgreementThe Allen Institute for AI (opens in a new tab)\n\n\n\nBy clicking accept or continuing to use the site, you agree to the terms outlined in our Privacy Policy (opens in a new tab), Terms of Service (opens in a new tab), and Dataset License (opens in a new tab)\n\nACCEPT & CONTINUE\nor Only Accept Required            ", "title": "[PDF] Learning to Translate in Real-time with Neural Machine Translation | Semantic Scholar", "description": "<strong>A</strong> <strong>neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) framework for simultaneous <strong>translation</strong> <strong>in</strong> which an agent <strong>learns</strong> <strong>to</strong> make decisions on when <strong>to</strong> <strong>translate</strong> from the interaction with a pre-trained NMT environment is proposed. <strong>Translating</strong> <strong>in</strong> <strong>real</strong>-<strong>time</strong>, <strong>a</strong>.k.a.simultaneous <strong>translation</strong>, outputs <strong>translation</strong> words ...", "usage": {"tokens": 3061}, "retrieved_text": ["   \n\n\n\nSkip to search formSkip to main contentSkip to account menu\n\n\n\n\n\n\n\n\n\nDOI:10.18653/V1/E17-1099\nCorpus ID: 2782776\nLearning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} }\n\nGraham Neubig, Kyunghyun Cho, \n+1 author V. Li\nPublished in Conference of the European\u2026 3 October 2016\nComputer Science, Linguistics\n\n\n\nTLDR\nA neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed.", "Machine Translation Aizhan ImankulovaMasahiro KanekoTosho HirasawaMamoru Komachi Computer Science WMT 2020 TLDR The experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency, and verified the importance of visual information during decoding by performing an adversarial evaluation ofMSNMT. Expand 15[PDF] 2 Excerpts Save Simultaneous Neural Machine Translation using Connectionist Temporal Classification Katsuki ChousaKatsuhito SudohSatoshi Nakamura Computer Science ArXiv 2019 TLDR A neural machine translation method is proposed to determine when to start the translation for observed inputs so far in an adaptive manner and introduces a special token ' ', which is generated when the translation model chooses to read the next input token instead of generating an output token. Expand\n\n\n\n\n5[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nTranslation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation\nAshkan AlinejadHassan S. ShavaraniAnoop Sarkar\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work proposes a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences.", "Skip to search formSkip to main contentSkip to account menu DOI:10.18653/V1/E17-1099 Corpus ID: 2782776 Learning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} } Graham Neubig, Kyunghyun Cho, +1 author V. Li Published in Conference of the European\u2026 3 October 2016 Computer Science, Linguistics TLDR A neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed. Expand\n\n\n[PDF] Semantic Reader\n\n\n\n\n\n\n\n\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n\n\n\n209 Citations\n\n\n\nHighly Influential Citations\n36\n\n\n\nBackground Citations\n142\n\n\n Methods Citations\n73\n\n\n Results Citations\n4\nView All\n\n\n\n\n\nFigures from this paper\n\n\n\n\n\n\n\nfigure 1\n\n\n\nfigure 2\n\n\n\nfigure 3\n\n\n\nfigure 4\n\n\n\nfigure 5\n\n\n\nfigure 6\n\n\n\nfigure 7\n\n\n\nfigure 8\n\nView All 8 Figures & Tables\n\n\n\n\nTopics\n\n\nAI-Generated\n\n\nSimultaneous Translation (opens in a new tab)Simultaneous Machine Translation (opens in a new tab)Translation Delays (opens in a new tab)Simultaneous Translation Task (opens in a new tab)READ/WRITE Decision (opens in a new tab)Simultaneous Translation System (opens in a new tab)Average Proportion (opens in a new tab)Simultaneous Mt (opens in a new tab)Neural Machine Translation (opens in a new tab)Translate (opens in a new tab)\n\n\n\n\n209 Citations\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nA General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation\nYun ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nAACL\n2020\n\n\n\nTLDR\nA general framework for adapting neural machine translation to translate simultaneously and contains a prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation."]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nLearning to Translate in Real-time with Neural Machine Translation\nJiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\n\nAnthology ID:\nE17-1099\nVolume:\nProceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\nMonth:\nApril\nYear:\n2017\nAddress:\nValencia, Spain\nEditors:\nMirella Lapata, Phil Blunsom, Alexander Koller\nVenue:\nEACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1053\u20131062\nLanguage:\n\nURL:\nhttps://aclanthology.org/E17-1099/\nDOI:\n\nBibkey:\n\ngu-etal-2017-learning\nCite (ACL):\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. 2017. Learning to Translate in Real-time with Neural Machine Translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1053\u20131062, Valencia, Spain. Association for Computational Linguistics.\n\nCite (Informal):\nLearning to Translate in Real-time with Neural Machine Translation (Gu et al., EACL 2017)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/E17-1099.pdf\nCode\n nyu-dl/dl4mt-simul-trans\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{gu-etal-2017-learning, title = \"Learning to Translate in Real-time with Neural Machine Translation\", author = \"Gu, Jiatao and Neubig, Graham and Cho, Kyunghyun and Li, Victor O.K.\", editor = \"Lapata, Mirella and Blunsom, Phil and Koller, Alexander\", booktitle = \"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\", month = apr, year = \"2017\", address = \"Valencia, Spain\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/E17-1099/\", pages = \"1053--1062\", abstract = \"Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"gu-etal-2017-learning\"> <titleInfo> <title>Learning to Translate in Real-time with Neural Machine Translation</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Jiatao</namePart> <namePart type=\"family\">Gu</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Graham</namePart> <namePart type=\"family\">Neubig</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Kyunghyun</namePart> <namePart type=\"family\">Cho</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Victor</namePart> <namePart type=\"given\">O.K.</namePart> <namePart type=\"family\">Li</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2017-04</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Mirella</namePart> <namePart type=\"family\">Lapata</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Phil</namePart> <namePart type=\"family\">Blunsom</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Alexander</namePart> <namePart type=\"family\">Koller</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Valencia, Spain</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.</abstract> <identifier type=\"citekey\">gu-etal-2017-learning</identifier> <location> <url>https://aclanthology.org/E17-1099/</url> </location> <part> <date>2017-04</date> <extent unit=\"page\"> <start>1053</start> <end>1062</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Learning to Translate in Real-time with Neural Machine Translation %A Gu, Jiatao %A Neubig, Graham %A Cho, Kyunghyun %A Li, Victor O.K. %Y Lapata, Mirella %Y Blunsom, Phil %Y Koller, Alexander %S Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers %D 2017 %8 April %I Association for Computational Linguistics %C Valencia, Spain %F gu-etal-2017-learning %X Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. %U https://aclanthology.org/E17-1099/ %P 1053-1062 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Learning to Translate in Real-time with Neural Machine Translation](https://aclanthology.org/E17-1099/) (Gu et al., EACL 2017)\n\nLearning to Translate in Real-time with Neural Machine Translation (Gu et al., EACL 2017)\nACL\n\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. 2017. Learning to Translate in Real-time with Neural Machine Translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1053\u20131062, Valencia, Spain. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Learning to Translate in Real-time with Neural Machine Translation - ACL Anthology", "description": "Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long <strong>Papers</strong>. 2017.", "usage": {"tokens": 2413}, "retrieved_text": [" ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nLearning to Translate in Real-time with Neural Machine Translation\nJiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting.", "ACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Learning to Translate in Real-time with Neural Machine Translation Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit AbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.", "MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/E17-1099.pdf Code nyu-dl/dl4mt-simul-trans PDF Cite Search Code Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{gu-etal-2017-learning, title = \"Learning to Translate in Real-time with Neural Machine Translation\", author = \"Gu, Jiatao and Neubig, Graham and Cho, Kyunghyun and Li, Victor O.K. \", editor = \"Lapata, Mirella and Blunsom, Phil and Koller, Alexander\", booktitle = \"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\", month = apr, year = \"2017\", address = \"Valencia, Spain\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/E17-1099/\", pages = \"1053--1062\", abstract = \"Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting."]}], "The paper titled \"Unsupervised Natural Language Generation with Denoising Autoencoders\" proposes a method for unsupervised natural language generation using denoising autoencoders, which can be used for tasks such as text summarization and paraphrasing.": [{"text": " \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1804.07899 (cs)   \n \n \n [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] \nTitle:Unsupervised Natural Language Generation with Denoising Autoencoders \nAuthors:Markus Freitag, Scott Roy \nView a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF \n Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.   \n   Comments: Accepted at EMNLP 2018   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1804.07899 [cs.CL]     (or  arXiv:1804.07899v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1804.07899\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Markus Freitag [view email] \n [v1] Sat, 21 Apr 2018 06:16:57 UTC (39 KB)\n [v2] Fri, 24 Aug 2018 19:53:33 UTC (40 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2018-04 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Markus Freitag\nScott Roy  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1804.07899] Unsupervised Natural Language Generation with Denoising Autoencoders", "description": "<strong>Generating</strong> <strong>text</strong> from structured data is important for various <strong>tasks</strong> <strong>such</strong> <strong>as</strong> question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled <strong>text</strong>, we are able to build <strong>a</strong> <strong>Natural</strong> <strong>Language</strong> <strong>Generation</strong> (NLG) system with higher performance ...", "usage": {"tokens": 1206}, "retrieved_text": [" \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1804.07899 (cs)   \n \n \n [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] \nTitle:Unsupervised Natural Language Generation with Denoising Autoencoders \nAuthors:Markus Freitag, Scott Roy \nView a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF \n Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.", "Computer Science > Computation and Language arXiv:1804.07899 (cs) [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] Title:Unsupervised Natural Language Generation with Denoising Autoencoders Authors:Markus Freitag, Scott Roy View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. Comments: Accepted at EMNLP 2018   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1804.07899 [cs.CL]     (or  arXiv:1804.07899v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1804.07899\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Markus Freitag [view email] \n [v1] Sat, 21 Apr 2018 06:16:57 UTC (39 KB)\n [v2] Fri, 24 Aug 2018 19:53:33 UTC (40 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2018-04 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Markus Freitag\nScott Roy  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)", "(What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Natural Language Generation with Denoising Autoencoders\nMarkus Freitag, Scott Roy\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractGenerating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.\n\nAnthology ID:\nD18-1426\nVolume:\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\nMonth:\nOctober-November\nYear:\n2018\nAddress:\nBrussels, Belgium\nEditors:\nEllen Riloff, David Chiang, Julia Hockenmaier, Jun\u2019ichi Tsujii\nVenue:\nEMNLP\nSIG:\nSIGDAT\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n3922\u20133929\nLanguage:\n\nURL:\nhttps://aclanthology.org/D18-1426/\nDOI:\n10.18653/v1/D18-1426\nBibkey:\n\nfreitag-roy-2018-unsupervised\nCite (ACL):\nMarkus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics.\n\nCite (Informal):\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/D18-1426.pdf\nPDF Cite Search  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"freitag-roy-2018-unsupervised\"> <titleInfo> <title>Unsupervised Natural Language Generation with Denoising Autoencoders</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Markus</namePart> <namePart type=\"family\">Freitag</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Scott</namePart> <namePart type=\"family\">Roy</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2018-oct-nov</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Ellen</namePart> <namePart type=\"family\">Riloff</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">David</namePart> <namePart type=\"family\">Chiang</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Julia</namePart> <namePart type=\"family\">Hockenmaier</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Jun\u2019ichi</namePart> <namePart type=\"family\">Tsujii</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Brussels, Belgium</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.</abstract> <identifier type=\"citekey\">freitag-roy-2018-unsupervised</identifier> <identifier type=\"doi\">10.18653/v1/D18-1426</identifier> <location> <url>https://aclanthology.org/D18-1426/</url> </location> <part> <date>2018-oct-nov</date> <extent unit=\"page\"> <start>3922</start> <end>3929</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Unsupervised Natural Language Generation with Denoising Autoencoders %A Freitag, Markus %A Roy, Scott %Y Riloff, Ellen %Y Chiang, David %Y Hockenmaier, Julia %Y Tsujii, Jun\u2019ichi %S Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing %D 2018 %8 oct nov %I Association for Computational Linguistics %C Brussels, Belgium %F freitag-roy-2018-unsupervised %X Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. %R 10.18653/v1/D18-1426 %U https://aclanthology.org/D18-1426/ %U https://doi.org/10.18653/v1/D18-1426 %P 3922-3929 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Unsupervised Natural Language Generation with Denoising Autoencoders](https://aclanthology.org/D18-1426/) (Freitag & Roy, EMNLP 2018)\n\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\nACL\n\nMarkus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Unsupervised Natural Language Generation with Denoising Autoencoders - ACL Anthology", "description": "Markus Freitag, Scott Roy. Proceedings of the 2018 Conference on Empirical <strong>Methods</strong> <strong>in</strong> <strong>Natural</strong> <strong>Language</strong> Processing. 2018.", "usage": {"tokens": 2340}, "retrieved_text": ["for Computational Linguistics Note: Pages: 3922\u20133929 Language: URL: https://aclanthology.org/D18-1426/ DOI: 10.18653/v1/D18-1426 Bibkey: freitag-roy-2018-unsupervised Cite (ACL): Markus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal): Unsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/D18-1426.pdf PDF Cite Search Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches.", " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Natural Language Generation with Denoising Autoencoders\nMarkus Freitag, Scott Roy\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractGenerating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence.", "corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. Anthology ID: D18-1426 Volume: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing Month: October-November Year: 2018 Address: Brussels, Belgium Editors: Ellen Riloff, David Chiang, Julia Hockenmaier, Jun\u2019ichi Tsujii Venue: EMNLP SIG: SIGDAT Publisher: Association for Computational Linguistics Note: Pages: 3922\u20133929 Language: URL: https://aclanthology.org/D18-1426/ DOI: 10.18653/v1/D18-1426 Bibkey: freitag-roy-2018-unsupervised Cite (ACL): Markus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal):\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/D18-1426.pdf\nPDF Cite Search  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems."]}, {"text": "       \n  \n \n Home   \n Publications     \n \n \nUnsupervised Natural Language Generation with Denoising Autoencoders \n \n Markus Freitag  \n \n Scott Roy  \n \n Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929   \n   Download   Google Scholar  \n  \n Copy Bibtex         \n \n \nAbstract  \n Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network.     \n \n \nResearch Areas  \n \n \n  \n \n \n \n Natural Language Processing   \n         \n  \n \n \n \n Machine Intelligence   \n              \n \n \nMeet the teams driving innovation \nOur teams advance the state of the art through research, systems engineering, and collaboration across Google.  See our teams   \n \n      \n \n  \n \n Follow us   \n \n     \n     \n     \n          \n   \n     \n \n  About Google   \n  Google Products   \n  Privacy   \n  Terms    \n \n   Help   \n \n Submit feedback            ", "title": "Unsupervised Natural Language Generation with Denoising Autoencoders", "description": "We strive to create an environment conducive to many different types of research across many different time scales and levels of risk \u00b7 Our researchers drive advancements in computer science through both fundamental and applied research", "usage": {"tokens": 368}, "retrieved_text": ["how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network. Research Areas Natural Language Processing Machine Intelligence Meet the teams driving innovation Our teams advance the state of the art through research, systems engineering, and collaboration across Google. See our teams   \n \n      \n \n  \n \n Follow us   \n \n     \n     \n     \n          \n   \n     \n \n  About Google   \n  Google Products   \n  Privacy   \n  Terms    \n \n   Help   \n \n Submit feedback", "Home Publications Unsupervised Natural Language Generation with Denoising Autoencoders Markus Freitag Scott Roy Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929 Download Google Scholar Copy Bibtex Abstract Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network. Research Areas  \n \n \n  \n \n \n \n Natural Language Processing   \n         \n  \n \n \n \n Machine Intelligence   \n              \n \n \nMeet the teams driving innovation \nOur teams advance the state of the art through research, systems engineering, and collaboration across Google.", "       \n  \n \n Home   \n Publications     \n \n \nUnsupervised Natural Language Generation with Denoising Autoencoders \n \n Markus Freitag  \n \n Scott Roy  \n \n Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929   \n   Download   Google Scholar  \n  \n Copy Bibtex         \n \n \nAbstract  \n Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network."]}, {"warning": "Target URL returned error 403: Forbidden", "text": " \n     \n \n\n\n\n\n\n\n\nWe've noticed some unusual traffic coming from your network. To continue, please check the box below.\n\n\nEnable JavaScript and cookies to continue\n\n\n\n\n\nAre you a researcher?\nTo avoid being denied access, log in if you're a ResearchGate member or create an account if you're not.\n\n\n\n\nLog in\nCreate account\n\n\nRay ID: 8ffd044408f86176\nClient IP: 2600:1900:0:2d01::3e01\n\u00a9 2008-2025 ResearchGate GmbH. All rights reserved.  ", "title": "Unsupervised Natural Language Generation with Denoising ...", "description": "Access 160+ million publication pages and connect with 25+ million researchers. Join for free and gain visibility by uploading your research.", "usage": {"tokens": 141}, "retrieved_text": ["Access 160+ million publication pages and connect with 25+ million researchers. Join for free and gain visibility by uploading your research."]}, {"text": "     \n     \n   \n \n \n \n    \n   \n  Browse State-of-the-Art   \n  Datasets   \n Methods  \n  More  \n Newsletter RC2022 \n About Trends  Portals   Libraries     \n \n     \n Sign In      \n \n \n \n \nSubscribe to the PwC Newsletter \n \u00d7   \n \n \n Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\n\n Read previous issues     \n \nSubscribe       \n \n \n \n \nJoin the community \n \u00d7   \n You need to log in to edit.\n You can create a new account if you don't have one.\n\n       \n \n \n \n \nEdit Social Preview \n \u00d7   \n    \n Description\n \n  Default  \n  Custom   \n \n Implemented in one code library.  \n    \n Image\n \n \n  Default  \n  Custom  \n  None    \n \n   \n \n File is too large  \n Upload an image to customize your repository\u2019s social media preview.\n Images should be at least 640\u00d7320px (1280\u00d7640px for best display).    \n    \n \n Close  \n Save       \n \n \n \n \nAdd a new code entry for this paper \n \u00d7   \n \n  \n  GitHub, GitLab or BitBucket URL:*  \n    \n \n   Official code from paper authors     \n \nSubmit        \n \n \n \n \nRemove a code repository from this paper \n \u00d7   \n \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n    \n \n   \n-            \n \n \n \n \n Mark the official implementation from paper authors  \n \u00d7   \n   \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n    \n     \n  \n \n \n There is no official implementation   \n      \n \n \n Multiple official implementations   \n \n        \n \nSubmit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n  \n  Task:*  \n  --------- Sentence Text Generation Denoising Question Answering    \n Not in the list? Add a task.  \n  Dataset:*  \n  ---------    \n  Model name:*  \n    \n  Metric name:*  \n  ---------    \n  \n \n   Higher is better (for the metric)    \n  Metric value:*  \n    \n  \n \n   Uses extra training data    \n  Data evaluated on  \n     \n \n Submit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n \n   TASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE          \n \n \n \n \nAdd a task \n \u00d7   \n \nAttached tasks: \n \n  DENOISING    \n \n  QUESTION ANSWERING    \n \n  SENTENCE    \n \n  TEXT GENERATION    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new task.  \n \n  New task name:  \n    \n  Top-level area:  \n  --------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series    \n  Parent task (if any):  \n  ---------    \n  Description:  \n     \n \n Submit         \n \n \n \n \nRemove a task \n \u00d7   \n \n \n \n \n    Denoising     \n-     \n \n \n    Question Answering     \n-     \n \n \n    Sentence     \n-     \n \n \n    Text Generation     \n-           \n \n \n \n \nAdd a method \n \u00d7   \n \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new method.  \n \n  New method name (e.g. ReLU):  \n    \n  New method full name (e.g. Rectified Linear Unit):  \n    \n  Paper where method was first introduced:  \n  ---------    \n  Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.  \n  ---------    \n  Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.  \n     \n \n Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? \n   Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Natural Language Generation with Denoising Autoencoders  \n \n   EMNLP 2018    \u00b7   Markus Freitag,  Scott Roy \u00b7 \n  Edit social preview       \n \n \n \n Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.    PDF    Abstract     EMNLP 2018 PDF     EMNLP 2018 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n      \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n        \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Denoising      Question Answering      Sentence      Text Generation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.       \n \n \n    \n \n \n \n \n \n Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com .  Papers With Code is a free resource with all data licensed under CC-BY-SA.   \n Terms Data policy Cookies policy  from       ", "title": "Unsupervised Natural Language Generation with Denoising Autoencoders | Papers With Code", "description": "Upload an image to customize your repository\u2019s social media preview. Images should be at least 640\u00d7320px (1280\u00d7640px for best display) \u00b7 EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview", "usage": {"tokens": 1322}, "retrieved_text": ["true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Natural Language Generation with Denoising Autoencoders EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. PDF    Abstract     EMNLP 2018 PDF     EMNLP 2018 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n      \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n        \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Denoising      Question Answering      Sentence      Text Generation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.", "data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. PDF Abstract EMNLP 2018 PDF EMNLP 2018 Abstract Code Edit Add Remove Mark official mcleonard/NLG_Autoencoder 16 mcleonard/NLG_Autoencoder 16 Tasks Edit Add Remove Denoising Question Answering Sentence Text Generation Datasets Edit Add Datasets introduced or used in this paper Results from the Paper Edit Submit results from this paper to get state-of-the-art GitHub badges and help the community compare results to other papers. Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com . Papers With Code is a free resource with all data licensed under CC-BY-SA.", "Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 Edit Datasets \u00d7 Add or remove datasets introduced in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Natural Language Generation with Denoising Autoencoders EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches."]}], "The paper titled \"Neural Machine Translation of Rare Words with Subword Units\" discusses the use of subword units for neural machine translation, which improves translation quality for words that are rare or out-of-vocabulary.": [{"text": " \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.  \n \nI Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.   \n   Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)   \n \nDBLP - CS Bibliography \n listing | bibtex  \n Rico Sennrich\nBarry Haddow\nAlexandra Birch  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1508.07909] Neural Machine Translation of Rare Words with Subword Units", "description": "<strong>Neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) models typically operate with a fixed <strong>vocabulary</strong>, <strong>but</strong> <strong>translation</strong> is an open-<strong>vocabulary</strong> problem. Previous work addresses <strong>the</strong> <strong>translation</strong> <strong>of</strong> <strong>out</strong>-<strong>of</strong>-<strong>vocabulary</strong> <strong>words</strong> by backing off to a dictionary. In this <strong>paper</strong>, we introduce a simpler and more effective approach, ...", "usage": {"tokens": 1451}, "retrieved_text": [" \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).", "Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1508.07909 (cs) [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] Title:Neural Machine Translation of Rare Words with Subword Units Authors:Rico Sennrich, Barry Haddow, Alexandra Birch View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.", "by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively. Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nNeural Machine Translation of Rare Words with Subword Units\nRico Sennrich, Barry Haddow, Alexandra Birch\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\nAnthology ID:\nP16-1162\nVolume:\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\nMonth:\nAugust\nYear:\n2016\nAddress:\nBerlin, Germany\nEditors:\nKatrin Erk, Noah A. Smith\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1715\u20131725\nLanguage:\n\nURL:\nhttps://aclanthology.org/P16-1162/\nDOI:\n10.18653/v1/P16-1162\nBibkey:\n\nsennrich-etal-2016-neural\nCite (ACL):\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\n\nCite (Informal):\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/P16-1162.pdf\nSoftware:\n  P16-1162.Software.zip\nCode\n rsennrich/subword-nmt +  additional community code\nData\nWMT 2015\nPDF Cite Search  Code  Software  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", editor = \"Erk, Katrin and Smith, Noah A.\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/P16-1162/\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"sennrich-etal-2016-neural\"> <titleInfo> <title>Neural Machine Translation of Rare Words with Subword Units</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Rico</namePart> <namePart type=\"family\">Sennrich</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Barry</namePart> <namePart type=\"family\">Haddow</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Alexandra</namePart> <namePart type=\"family\">Birch</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2016-08</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Katrin</namePart> <namePart type=\"family\">Erk</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Noah</namePart> <namePart type=\"given\">A</namePart> <namePart type=\"family\">Smith</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Berlin, Germany</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <identifier type=\"citekey\">sennrich-etal-2016-neural</identifier> <identifier type=\"doi\">10.18653/v1/P16-1162</identifier> <location> <url>https://aclanthology.org/P16-1162/</url> </location> <part> <date>2016-08</date> <extent unit=\"page\"> <start>1715</start> <end>1725</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Neural Machine Translation of Rare Words with Subword Units %A Sennrich, Rico %A Haddow, Barry %A Birch, Alexandra %Y Erk, Katrin %Y Smith, Noah A. %S Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) %D 2016 %8 August %I Association for Computational Linguistics %C Berlin, Germany %F sennrich-etal-2016-neural %R 10.18653/v1/P16-1162 %U https://aclanthology.org/P16-1162/ %U https://doi.org/10.18653/v1/P16-1162 %P 1715-1725 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/) (Sennrich et al., ACL 2016)\n\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\nACL\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Neural Machine Translation of Rare Words with Subword Units - ACL Anthology", "description": "Rico Sennrich, Barry Haddow, Alexandra Birch. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long <strong>Papers</strong>). 2016.", "usage": {"tokens": 1787}, "retrieved_text": ["1715-1725 Download as File Copy to Clipboard Markdown (Informal) [Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/) (Sennrich et al., ACL 2016) Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016) ACL Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License. The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nNeural Machine Translation of Rare Words with Subword Units\nRico Sennrich, Barry Haddow, Alexandra Birch\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\nAnthology ID:\nP16-1162\nVolume:\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\nMonth:\nAugust\nYear:\n2016\nAddress:\nBerlin, Germany\nEditors:\nKatrin Erk, Noah A. Smith\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1715\u20131725\nLanguage:\n\nURL:\nhttps://aclanthology.org/P16-1162/\nDOI:\n10.18653/v1/P16-1162\nBibkey:\n\nsennrich-etal-2016-neural\nCite (ACL):\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.", "ACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Neural Machine Translation of Rare Words with Subword Units Rico Sennrich, Barry Haddow, Alexandra Birch Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit Anthology ID: P16-1162 Volume: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Month: August Year: 2016 Address: Berlin, Germany Editors: Katrin Erk, Noah A. Smith Venue: ACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 1715\u20131725 Language: URL: https://aclanthology.org/P16-1162/ DOI: 10.18653/v1/P16-1162 Bibkey: sennrich-etal-2016-neural Cite (ACL): Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. Cite (Informal):\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/P16-1162.pdf\nSoftware:\n  P16-1162.Software.zip\nCode\n rsennrich/subword-nmt +  additional community code\nData\nWMT 2015\nPDF Cite Search  Code  Software  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", editor = \"Erk, Katrin and Smith, Noah A."]}, {"text": " \n \n Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. \n  Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    rsennrich   /  subword-nmt  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 464   \n \n   Star  2.2k      \n \n \n Unsupervised Word Segmentation for Neural Machine Translation and Text Generation  \nLicense \n   MIT license   \n   2.2k stars    464 forks    Branches    Tags    Activity   \n \n \n   Star    \n  Notifications  You must be signed in to change notification settings        \n \n   Code   \n   Issues 3  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Wiki   \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Wiki    \n      Security    \n      Insights             \n \nrsennrich/subword-nmt \n \n \n \n              \n   \n\n\n\n\n\n\n\n\n\n master\n\nBranchesTags\n\n\n\n\n\n\nGo to file\n\n\nCode\n\n\n\n\n\n\nFolders and filesNameName\nLast commit message\nLast commit date\n\nLatest commit\n \n\n\n\nHistory144 Commits\n\n\n\n\n\n.github/workflows\n\n\n\n.github/workflows\n \n \n\n\n\nsubword_nmt\n\n\n\nsubword_nmt\n \n \n\n\n\n.gitignore\n\n\n\n.gitignore\n \n \n\n\n\nCHANGELOG.md\n\n\n\nCHANGELOG.md\n \n \n\n\n\nLICENSE\n\n\n\nLICENSE\n \n \n\n\n\nREADME.md\n\n\n\nREADME.md\n \n \n\n\n\napply_bpe.py\n\n\n\napply_bpe.py\n \n \n\n\n\nget_vocab.py\n\n\n\nget_vocab.py\n \n \n\n\n\nlearn_bpe.py\n\n\n\nlearn_bpe.py\n \n \n\n\n\nlearn_joint_bpe_and_vocab.py\n\n\n\nlearn_joint_bpe_and_vocab.py\n \n \n\n\n\nsetup.py\n\n\n\nsetup.py\n \n \n\nView all files\n\n\n\nRepository files navigation\n\nREADME\nMIT license\n\n\n\n\nSubword Neural Machine Translation \nThis repository contains preprocessing scripts to segment text into subword units. The primary purpose is to facilitate the reproduction of our experiments on Neural Machine Translation with subword units (see below for reference). \n\nINSTALLATION \ninstall via pip (from PyPI): \npip install subword-nmt  \ninstall via pip (from Github): \npip install https://github.com/rsennrich/subword-nmt/archive/master.zip  \nalternatively, clone this repository; the scripts are executable stand-alone. \n\nUSAGE INSTRUCTIONS \nCheck the individual files for usage instructions. \nTo apply byte pair encoding to word segmentation, invoke these commands: \nsubword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file} subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}  \nTo segment rare words into character n-grams, do the following: \nsubword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file} subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}  \nThe original segmentation can be restored with a simple replacement: \nsed -r 's/(@@ )|(@@ ?$)//g'  \nIf you cloned the repository and did not install a package, you can also run the individual commands as scripts: \n./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}  \n\nBEST PRACTICE ADVICE FOR BYTE PAIR ENCODING IN NMT \nWe found that for languages that share an alphabet, learning BPE on the concatenation of the (two or more) involved languages increases the consistency of segmentation, and reduces the problem of inserting/deleting characters when copying/transliterating names. \nHowever, this introduces undesirable edge cases in that a word may be segmented in a way that has only been observed in the other language, and is thus unknown at test time. To prevent this, apply_bpe.py accepts a --vocabulary and a --vocabulary-threshold option so that the script will only produce symbols which also appear in the vocabulary (with at least some frequency). \nTo use this functionality, we recommend the following recipe (assuming L1 and L2 are the two languages): \nLearn byte pair encoding on the concatenation of the training text, and get resulting vocabulary for each: \ncat {train_file}.L1 {train_file}.L2 | subword-nmt learn-bpe -s {num_operations} -o {codes_file} subword-nmt apply-bpe -c {codes_file} < {train_file}.L1 | subword-nmt get-vocab > {vocab_file}.L1 subword-nmt apply-bpe -c {codes_file} < {train_file}.L2 | subword-nmt get-vocab > {vocab_file}.L2  \nmore conventiently, you can do the same with with this command: \nsubword-nmt learn-joint-bpe-and-vocab --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2  \nre-apply byte pair encoding with vocabulary filter: \nsubword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L1 --vocabulary-threshold 50 < {train_file}.L1 > {train_file}.BPE.L1 subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L2 --vocabulary-threshold 50 < {train_file}.L2 > {train_file}.BPE.L2  \nas a last step, extract the vocabulary to be used by the neural network. Example with Nematus: \nnematus/data/build_dictionary.py {train_file}.BPE.L1 {train_file}.BPE.L2  \n[you may want to take the union of all vocabularies to support multilingual systems] \nfor test/dev data, re-use the same options for consistency: \nsubword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L1 --vocabulary-threshold 50 < {test_file}.L1 > {test_file}.BPE.L1  \n\nADVANCED FEATURES \nOn top of the basic BPE implementation, this repository supports: \n \n \nBPE dropout (Provilkov, Emelianenko and Voita, 2019): https://arxiv.org/abs/1910.13267 use the argument --dropout 0.1 for subword-nmt apply-bpe to randomly drop out possible merges. Doing this on the training corpus can improve quality of the final system; at test time, use BPE without dropout. In order to obtain reproducible results, argument --seed can be used to set the random seed. \nNote: In the original paper, the authors used BPE-Dropout on each new batch separately. You can copy the training corpus several times to get similar behavior to obtain multiple segmentations for the same sentence.  \n \nsupport for glossaries: use the argument --glossaries for subword-nmt apply-bpe to provide a list of subwords and/or regular expressions that should always be passed to the output without subword segmentation   \necho \"I am flying to <country>Switzerland</country> at noon .\" | subword-nmt apply-bpe --codes subword_nmt/tests/data/bpe.ref I am fl@@ y@@ ing to <@@ coun@@ tr@@ y@@ >@@ S@@ w@@ it@@ z@@ er@@ l@@ and@@ <@@ /@@ coun@@ tr@@ y@@ > at no@@ on . echo \"I am flying to <country>Switzerland</country> at noon .\" | subword-nmt apply-bpe --codes subword_nmt/tests/data/bpe.ref --glossaries \"<country>\\w*</country>\" \"fly\" I am fly@@ ing to <country>Switzerland</country> at no@@ on .  \n \nbyte-level BPE: while BPE uses characters as basic units in Sennrich et al., 2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file.  \n\nPUBLICATIONS \nThe segmentation methods are described in: \n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and  Haddow, Barry and  Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } \nThe best practice advice is described in: \n@inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and  Birch, Alexandra and  Currey, Anna and  Germann, Ulrich and  Haddow, Barry and  Heafield, Kenneth and  Miceli Barone, Antonio Valerio and  Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } \n\nHOW IMPLEMENTATION DIFFERS FROM Sennrich et al. (2016) \nThis repository implements the subword segmentation as described in Sennrich et al. (2016), but since version 0.2, there is one core difference related to end-of-word tokens. \nIn Sennrich et al. (2016), the end-of-word token </w> is initially represented as a separate token, which can be merged with other subwords over time: \nu n d </w> f u n d </w>  \nSince 0.2, end-of-word tokens are initially concatenated with the word-final character: \nu n d</w> f u n d</w>  \nThe new representation ensures that when BPE codes are learned from the above examples and then applied to new text, it is clear that a subword unit und is unambiguously word-final, and un is unambiguously word-internal, preventing the production of up to two different subword units from each BPE merge operation. \napply_bpe.py is backward-compatible and continues to accept old-style BPE files. New-style BPE files are identified by having the following first line: #version: 0.2 \n\nACKNOWLEDGMENTS \nThis project has received funding from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland, and from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 645452 (QT21).        \n \n \n \n \n \nAbout \n Unsupervised Word Segmentation for Neural Machine Translation and Text Generation  \nTopics \n \n  machine-translation   segmentation   neural-machine-translation   nmt   subword-units   bpe    \nResources \n   Readme   \nLicense \n   MIT license     \n  Activity  \nStars \n  2.2k stars  \nWatchers \n  55 watching  \nForks \n  464 forks  \n  Report repository      \n \n \n Releases 1   \n \n 0.3.8  Latest   \nDec 8, 2021     \n \n \n Packages 0 \n No packages published \n    \n \n     \n \n \n Contributors 14 \n \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n        \n \n \nLanguages \n    \n \n   Python 100.0%               \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time.      \n     \n    \n \n    \n            \n            \n \n ", "title": "GitHub - rsennrich/subword-nmt: Unsupervised Word Segmentation for Neural Machine Translation and Text Generation", "description": "Unsupervised <strong>Word</strong> Segmentation <strong>for</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> and Text Generation - rsennrich/<strong>subword</strong>-nmt", "usage": {"tokens": 2711}, "retrieved_text": ["2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file. PUBLICATIONS The segmentation methods are described in: @inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } The best practice advice is described in: @inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and Birch, Alexandra and Currey, Anna and Germann, Ulrich and Haddow, Barry and Heafield, Kenneth and Miceli Barone, Antonio Valerio and Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } HOW IMPLEMENTATION DIFFERS FROM Sennrich et al. (2016) \nThis repository implements the subword segmentation as described in Sennrich et al.", "--codes subword_nmt/tests/data/bpe.ref --glossaries \"<country>\\w*</country>\" \"fly\" I am fly@@ ing to <country>Switzerland</country> at no@@ on . byte-level BPE: while BPE uses characters as basic units in Sennrich et al., 2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file. PUBLICATIONS \nThe segmentation methods are described in: \n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and  Haddow, Barry and  Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } \nThe best practice advice is described in: \n@inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and  Birch, Alexandra and  Currey, Anna and  Germann, Ulrich and  Haddow, Barry and  Heafield, Kenneth and  Miceli Barone, Antonio Valerio and  Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } \n\nHOW IMPLEMENTATION DIFFERS FROM Sennrich et al.", "setup.py setup.py View all files Repository files navigation README MIT license Subword Neural Machine Translation This repository contains preprocessing scripts to segment text into subword units. The primary purpose is to facilitate the reproduction of our experiments on Neural Machine Translation with subword units (see below for reference). INSTALLATION install via pip (from PyPI): pip install subword-nmt install via pip (from Github): pip install https://github.com/rsennrich/subword-nmt/archive/master.zip alternatively, clone this repository; the scripts are executable stand-alone. USAGE INSTRUCTIONS Check the individual files for usage instructions. To apply byte pair encoding to word segmentation, invoke these commands: \nsubword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file} subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}  \nTo segment rare words into character n-grams, do the following: \nsubword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file} subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}  \nThe original segmentation can be restored with a simple replacement: \nsed -r 's/(@@ )|(@@ ?$)//g'  \nIf you cloned the repository and did not install a package, you can also run the individual commands as scripts: \n./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}  \n\nBEST PRACTICE ADVICE FOR BYTE PAIR ENCODING IN NMT \nWe found that for languages that share an alphabet, learning BPE on the concatenation of the (two or more) involved languages increases the consistency of segmentation, and reduces the problem of inserting/deleting characters when copying/transliterating names."]}, {"text": " \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.  \n \nI Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909v5 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.   \n   Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)   \n \nDBLP - CS Bibliography \n listing | bibtex  \n Rico Sennrich\nBarry Haddow\nAlexandra Birch  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1508.07909v5] Neural Machine Translation of Rare Words with Subword Units", "description": "<strong>Neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) models typically operate with a fixed <strong>vocabulary</strong>, <strong>but</strong> <strong>translation</strong> is an open-<strong>vocabulary</strong> problem. Previous work addresses <strong>the</strong> <strong>translation</strong> <strong>of</strong> <strong>out</strong>-<strong>of</strong>-<strong>vocabulary</strong> <strong>words</strong> by backing off to a dictionary. In this <strong>paper</strong>, we introduce a simpler and more effective approach, ...", "usage": {"tokens": 1453}, "retrieved_text": [" \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909v5 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).", "Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1508.07909v5 (cs) [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] Title:Neural Machine Translation of Rare Words with Subword Units Authors:Rico Sennrich, Barry Haddow, Alexandra Birch View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.", "by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively. Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)"]}, {"warning": "Target URL returned error 429: Too Many Requests", "text": "www.researchgate.net\nVerifying you are human. This may take a few seconds.\n\n\n\n\n\n\n\nwww.researchgate.net needs to review the security of your connection before proceeding.\n\nVerification successful\nWaiting for www.researchgate.net to respond...\nEnable JavaScript and cookies to continue\n\n\n\nRay ID: 8ffd053bbf0b8114\nPerformance & security by Cloudflare", "title": "Neural Machine Translation of Rare Words with Subword Units | Request PDF", "description": "Request PDF | On Jan 1, 2016, Rico Sennrich and others published <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> <strong>of</strong> <strong>Rare</strong> <strong>Words</strong> <strong>with</strong> <strong>Subword</strong> <strong>Units</strong> | Find, read and cite all the research you need on ResearchGate", "usage": {"tokens": 173}, "retrieved_text": ["Request PDF | On Jan 1, 2016, Rico Sennrich and others published <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> <strong>of</strong> <strong>Rare</strong> <strong>Words</strong> <strong>with</strong> <strong>Subword</strong> <strong>Units</strong> | Find, read and cite all the research you need on ResearchGate"]}]}, "retrieved_search_results": {"The paper titled \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\" presents a method for improving part-of-speech tagging across multiple languages by using adversarial training to generate more robust models.": [{"text": " \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1711.04903 (cs)   \n \n \n [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] \nTitle:Robust Multilingual Part-of-Speech Tagging via Adversarial Training \nAuthors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev \nView a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF \n Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.   \n   Comments: NAACL 2018   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1711.04903 [cs.CL]     (or  arXiv:1711.04903v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1711.04903\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Michihiro Yasunaga [view email] \n [v1] Tue, 14 Nov 2017 01:50:30 UTC (359 KB)\n [v2] Fri, 20 Apr 2018 15:49:22 UTC (187 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2017-11 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Michihiro Yasunaga\nJungo Kasai\nDragomir R. Radev  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1711.04903] Robust Multilingual Part-of-Speech Tagging via Adversarial Training", "description": "<strong>Adversarial</strong> <strong>training</strong> (AT) is a powerful regularization <strong>method</strong> <strong>for</strong> neural networks, aiming to achieve <strong>robustness</strong> <strong>to</strong> input perturbations. Yet, the specific effects of <strong>the</strong> <strong>robustness</strong> obtained from AT are still unclear in the context of natural <strong>language</strong> processing.", "usage": {"tokens": 1315}, "retrieved_text": [" \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1711.04903 (cs)   \n \n \n [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] \nTitle:Robust Multilingual Part-of-Speech Tagging via Adversarial Training \nAuthors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev \nView a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF \n Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations.", "Computer Science > Computation and Language arXiv:1711.04903 (cs) [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] Title:Robust Multilingual Part-of-Speech Tagging via Adversarial Training Authors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks.", "for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. Comments: NAACL 2018   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1711.04903 [cs.CL]     (or  arXiv:1711.04903v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1711.04903\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Michihiro Yasunaga [view email] \n [v1] Tue, 14 Nov 2017 01:50:30 UTC (359 KB)\n [v2] Fri, 20 Apr 2018 15:49:22 UTC (187 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2017-11 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Michihiro Yasunaga\nJungo Kasai\nDragomir R. Radev  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training\nMichihiro Yasunaga, Jungo Kasai, Dragomir Radev\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractAdversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.\n\nAnthology ID:\nN18-1089\nVolume:\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\nMonth:\nJune\nYear:\n2018\nAddress:\nNew Orleans, Louisiana\nEditors:\nMarilyn Walker, Heng Ji, Amanda Stent\nVenue:\nNAACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n976\u2013986\nLanguage:\n\nURL:\nhttps://aclanthology.org/N18-1089/\nDOI:\n10.18653/v1/N18-1089\nBibkey:\n\nyasunaga-etal-2018-robust\nCite (ACL):\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics.\n\nCite (Informal):\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/N18-1089.pdf\nCode\n michiyasunaga/pos_adv\nData\nCoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"yasunaga-etal-2018-robust\"> <titleInfo> <title>Robust Multilingual Part-of-Speech Tagging via Adversarial Training</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Michihiro</namePart> <namePart type=\"family\">Yasunaga</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Jungo</namePart> <namePart type=\"family\">Kasai</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Dragomir</namePart> <namePart type=\"family\">Radev</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2018-06</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Marilyn</namePart> <namePart type=\"family\">Walker</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Heng</namePart> <namePart type=\"family\">Ji</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Amanda</namePart> <namePart type=\"family\">Stent</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">New Orleans, Louisiana</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.</abstract> <identifier type=\"citekey\">yasunaga-etal-2018-robust</identifier> <identifier type=\"doi\">10.18653/v1/N18-1089</identifier> <location> <url>https://aclanthology.org/N18-1089/</url> </location> <part> <date>2018-06</date> <extent unit=\"page\"> <start>976</start> <end>986</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Robust Multilingual Part-of-Speech Tagging via Adversarial Training %A Yasunaga, Michihiro %A Kasai, Jungo %A Radev, Dragomir %Y Walker, Marilyn %Y Ji, Heng %Y Stent, Amanda %S Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) %D 2018 %8 June %I Association for Computational Linguistics %C New Orleans, Louisiana %F yasunaga-etal-2018-robust %X Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. %R 10.18653/v1/N18-1089 %U https://aclanthology.org/N18-1089/ %U https://doi.org/10.18653/v1/N18-1089 %P 976-986 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Robust Multilingual Part-of-Speech Tagging via Adversarial Training](https://aclanthology.org/N18-1089/) (Yasunaga et al., NAACL 2018)\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\nACL\n\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training - ACL Anthology", "description": "Michihiro Yasunaga, Jungo Kasai, Dragomir Radev. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human <strong>Language</strong> Technologies, Volume 1 (Long <strong>Papers</strong>). 2018.", "usage": {"tokens": 2785}, "retrieved_text": ["(ACL): Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics. Cite (Informal): Robust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/N18-1089.pdf Code michiyasunaga/pos_adv Data CoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies PDF Cite Search Code Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing.", "Year: 2018 Address: New Orleans, Louisiana Editors: Marilyn Walker, Heng Ji, Amanda Stent Venue: NAACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 976\u2013986 Language: URL: https://aclanthology.org/N18-1089/ DOI: 10.18653/v1/N18-1089 Bibkey: yasunaga-etal-2018-robust Cite (ACL): Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics. Cite (Informal):\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/N18-1089.pdf\nCode\n michiyasunaga/pos_adv\nData\nCoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations.", "%C New Orleans, Louisiana %F yasunaga-etal-2018-robust %X Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. %R 10.18653/v1/N18-1089 %U https://aclanthology.org/N18-1089/ %U https://doi.org/10.18653/v1/N18-1089 %P 976-986 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Robust Multilingual Part-of-Speech Tagging via Adversarial Training](https://aclanthology.org/N18-1089/) (Yasunaga et al., NAACL 2018)\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\nACL\n\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev."]}, {"text": " \n \n \n \nRobust Multilingual Part-of-Speech Tagging via Adversarial Training \n  Michihiro Yasunaga Jungo Kasai Dragomir Radev \nDepartment of Computer Science, Yale University \n  {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu    \n \nAbstract \nAdversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks.   \n 1 Introduction \n \nFigure 1: Illustration of our architecture for adversarial POS tagging. Given a sentence, we input the normalized word embeddings (\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0,\ud835\udc98\ud835\udfd1subscript\ud835\udc981subscript\ud835\udc982subscript\ud835\udc983\\mbox{\\boldmath$\\uline{w}_{1}$},\\mbox{\\boldmath$\\uline{w}_{2}$},\\mbox{\\boldmath$\\uline{w}_{3}$}) and character embeddings (showing \ud835\udc84\ud835\udfcf,\ud835\udc84\ud835\udfd0,\ud835\udc84\ud835\udfd1subscript\ud835\udc841subscript\ud835\udc842subscript\ud835\udc843\\mbox{\\boldmath$\\uline{c}_{1}$},\\mbox{\\boldmath$\\uline{c}_{2}$},\\mbox{\\boldmath$\\uline{c}_{3}$} for w1subscript\ud835\udc641w_{1}). Each word is represented by concatenating its word embedding and its character-level BiLSTM output. They are fed into the main BiLSTM-CRF network for POS tagging. In adversarial training, we compute and add the worst-case perturbation \u03b7\ud835\udf02\\eta to all the input embeddings for regularization.   \n \nRecently, neural network-based approaches have become popular in many natural language processing (NLP) tasks including tagging, parsing, and translation Chen and Manning (2014); Bahdanau et al. (2015); Ma and Hovy (2016). However, it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models Szegedy et al. (2014). Such maliciously perturbed inputs are called adversarial examples. Adversarial training Goodfellow et al. (2015) aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples. Previous work Goodfellow et al. (2015); Shaham et al. (2015) on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial training and has provided theoretical explanations of the regularization effects.  \n \nDespite its potential as a powerful regularizer, adversarial training (AT) has yet to be explored extensively in natural language tasks. Recently, Miyato et al. (2017) applied AT on text classification, achieving state-of-the-art accuracy. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of NLP. For example, research studies have yet to answer questions such as 1) how can we interpret perturbations or robustness on natural language inputs? 2) how are they related to linguistic factors like vocabulary statistics? 3) are the effects of AT language-dependent? Answering such questions is crucial to understand and motivate the application of adversarial training on natural language tasks.  \n \nIn this paper, spotlighting a well-studied core problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training. With a BiLSTM-CRF model Huang et al. (2015); Ma and Hovy (2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word \u200b/\u200b character embeddings. In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset (27 languages), with thorough analyses of the following points: \n \n \u2022 \n \nEffects on different target languages   \n \u2022 \n \nVocabulary statistics and tagging accuracy   \n \u2022 \n \nInfluence on downstream tasks   \n \u2022 \n \nRepresentation learning of words     \n \nIn our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages. Furthermore, our analyses reveal the following insights into adversarial training in the context of NLP:  \n \n \n \u2022 \n \nThe regularization effects of adversarial training (AT) are general across different languages. AT can prevent overfitting especially well when training examples are scarce, providing an effective tool to process low resource languages.   \n \u2022 \n \nAT can boost the tagging performance for rare \u200b/\u200b unseen words and increase the sentence-level accuracy. This positively affects the performance of down-stream tasks such as dependency parsing, where low sentence-level POS accuracy can be a bottleneck Manning (2011).   \n \u2022 \n \nAT helps the network learn cleaner word embeddings, showing stronger correlations with their POS tags.    \nWe argue that the effects of AT can be interpreted from the perspective of natural language. Finally, we demonstrate that the proposed AT model is generally effective across different sequence labeling tasks. This work therefore provides a strong motivation and basis for utilizing adversarial training in NLP tasks.    \n 2 Related Work  \n 2.1 POS Tagging \n \nPart-of-speech (POS) tagging is a fundamental NLP task that facilitates downstream tasks such as syntactic parsing. While current state-of-the-art POS taggers Ling et al. (2015); Ma and Hovy (2016) yield accuracy over 97.5% on PTB-WSJ, there still remain issues. The per token accuracy metric is easy since taggers can easily assign correct POS tags to highly unambiguous tokens, such as punctuation Manning (2011). Sentence-level accuracy serves as a more realistic metric for POS taggers but it still remains low. Another problem with current POS taggers is that their accuracy deteriorates drastically on low resource languages and rare words Plank et al. (2016). In this work, we demonstrate that adversarial training (AT) can mitigate these issues.  \n \nIt is empirically shown that POS tagging performance can greatly affect downstream tasks such as dependency parsing Dozat et al. (2017). In this work, we also demonstrate that the improvements obtained from our AT POS tagger actually contribute to dependency parsing. Nonetheless, parsing with gold POS tags still yields better results, bolstering the view that POS tagging is an essential task in NLP that needs further development.    \n 2.2 Adversarial Training \n \nThe concept of adversarial training Szegedy et al. (2014); Goodfellow et al. (2015) was originally introduced in the context of image classification to improve the robustness of a model by training on input images with malicious perturbations. Previous work Goodfellow et al. (2015); Shaham et al. (2015); Wang et al. (2017) has provided a theoretical framework to understand adversarial examples and the regularization effects of adversarial training (AT) in image recognition.  \n \nRecently, Miyato et al. (2017) applied AT to a natural language task (text classification) by extending the concept of adversarial perturbations to word embeddings. Wu et al. (2017) further explored the possibility of AT in relation extraction. Both report improved performance on their tasks via AT, but the specific effects of AT have yet to be analyzed. In our work, we aim to address this issue by providing detailed analyses on the effects of AT from the perspective of NLP, such as different languages, vocabulary statistics, word embedding distribution, and aim to motivate future research that exploits AT in NLP tasks.  \n \nAT is related to other regularization methods that add noise to data such as dropout Srivastava et al. (2014) and its variant for NLP tasks, word dropout Iyyer et al. (2015). Xie et al. (2017) discuss various data noising techniques for language modeling. While these methods produce random noise, AT generates perturbations that the current model is particularly vulnerable to, and thus is claimed to be effective Goodfellow et al. (2015).  \n \nIt should be noted that while related in name, adversarial training (AT) differs from Generative Adversarial Networks (GANs) Goodfellow et al. (2014). GANs have already been applied to NLP tasks such as dialogue generation Li et al. (2017) and transfer learning Kim et al. (2017); Gui et al. (2017). Adversarial training also differs from adversarial evaluation, recently proposed for reading comprehension tasks Jia and Liang (2017).     \n 3 Method \n \nIn this section, we introduce our baseline POS tagging model and explain how we implement adversarial training on top.   \n 3.1 Baseline POS Tagging Model \n \nFollowing the recent top-performing models for sequence labeling tasks Plank et al. (2016); Lample et al. (2016); Ma and Hovy (2016), we employ a Bi-directional LSTM-CRF model as our baseline (see Figure 1 for an illustration).   \nCharacter-level BiLSTM. \n \nPrior work has shown that incorporating character-level representations of words can boost POS tagging accuracy by capturing morphological information present in each language. Major neural character-level models include the character-level CNN Ma and Hovy (2016) and (Bi)LSTM Dozat et al. (2017). A Bi-directional LSTM (BiLSTM) Hochreiter and Schmidhuber (1997); Schuster and Paliwal (1997) processes each sequence both forward and backward to capture sequential information, while preventing the vanishing \u200b/\u200b exploding gradient problem. We observed that the character-level BiLSTM outperformed the CNN by 0.1% on the PTB-WSJ development set, and hence in all of our experiments we use the character-level BiLSTM. Specifically, we generate a character-level representation for each word by feeding its character embeddings into the BiLSTM and obtaining the concatenated final states.    \nWord-level BiLSTM. \n \nEach word in a sentence is represented by concatenating its word embedding and its character-level representation. They are fed into another level of BiLSTM (word-level BiLSTM) to process the entire sentence.    \nCRF. \n \nIn sequence labeling tasks it is beneficial to consider the correlations between neighboring labels and jointly decode the best chain of labels for a given sentence. With this motivation, we apply a conditional random field (CRF) Lafferty et al. (2001) on top of the word-level BiLSTM to perform POS tag inference with global normalization, addressing the \u201clabel bias\u201d problem. Specifically, given an input sentence, we pass the output sequence of the word-level BiLSTM to a first-order chain CRF to compute the conditional probability of the target label sequence:    p\u200b(\ud835\udc9a|\ud835\udc94;\ud835\udf3d)\ud835\udc5dconditional\ud835\udc9a\ud835\udc94\ud835\udf3dp(\\mbox{\\boldmath$y$}\\,|\\,\\mbox{\\boldmath$s$};\\mbox{\\boldmath$\\theta$})    \nwhere \ud835\udf3d\ud835\udf3d\\theta represents all of the model parameters (in the BiLSTMs and CRF), \ud835\udc94\ud835\udc94s and \ud835\udc9a\ud835\udc9ay denote the input embeddings and the target POS tag sequence, respectively, for the given sentence.  \n \nFor training, we minimize the negative log-likelihood (loss function)    L\u200b(\ud835\udf3d;\ud835\udc94,\ud835\udc9a)=\u2212log\u2061p\u200b(\ud835\udc9a|\ud835\udc94;\ud835\udf3d)\ud835\udc3f\ud835\udf3d\ud835\udc94\ud835\udc9a\ud835\udc5dconditional\ud835\udc9a\ud835\udc94\ud835\udf3dL(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$})=-\\log p(\\mbox{\\boldmath$y$}\\,|\\,\\mbox{\\boldmath$s$};\\mbox{\\boldmath$\\theta$})  (1)   \nwith respect to the model parameters. Decoding searches for the POS tag sequence \ud835\udc9a\u2217superscript\ud835\udc9a\\mbox{\\boldmath$y$}^{*} with the highest conditional probability using the Viterbi algorithm. For more detail about the BiLSTM-CRF formulation, refer to Ma and Hovy (2016).     \n 3.2 Adversarial Training \n \nAdversarial training Goodfellow et al. (2015) is a powerful regularization method, primarily explored in image recognition to improve the robustness of classifiers to input perturbations. Given a classifier, we first generate input examples that are very close to original inputs (so should yield the same labels) yet are likely to be misclassified by the current model. Specifically, these adversarial examples are generated by adding small perturbations to the inputs in the direction that significantly increases the loss function of the classifier (worst-case perturbations). Then, the classifier is trained on the mixture of clean examples and adversarial examples to improve the stability to input perturbations. In this work, we incorporate adversarial training into our baseline POS tagger, aiming to achieve better regularization effects and to provide their interpretations in the context of NLP.   \nGenerating adversarial examples. \n \nAdversarial training (AT) considers continuous perturbations to inputs, so we define perturbations at the level of dense word / character embeddings rather than one-hot vector representations, similarly to Miyato et al. (2017). Specifically, given an input sentence, we consider the concatenation of all the word \u200b/\u200b character embeddings in the sentence: \ud835\udc94=[\ud835\udc981,\ud835\udc982,\u2026,\ud835\udc841,\ud835\udc842,\u2026]\ud835\udc94subscript\ud835\udc981subscript\ud835\udc982\u2026subscript\ud835\udc841subscript\ud835\udc842\u2026\\mbox{\\boldmath$s$}=[\\mbox{\\boldmath$w$}_{1},\\mbox{\\boldmath$w$}_{2},\\dots,\\mbox{\\boldmath$c$}_{1},\\mbox{\\boldmath$c$}_{2},\\dots]. To prepare an adversarial example, we aim to generate the worst-case perturbation of a small bounded norm \u03f5italic-\u03f5\\epsilon that maximizes the loss function L\ud835\udc3fL of the current model:    \ud835\udf3c=arg\u200bmax\ud835\udf3c\u2032:\u2016\ud835\udf3c\u2032\u20162\u2264\u03f5\u2061L\u200b(\ud835\udf3d^;\ud835\udc94+\ud835\udf3c\u2032,\ud835\udc9a)\ud835\udf3csubscriptargmax:superscript\ud835\udf3c\u2032subscriptnormsuperscript\ud835\udf3c\u20322italic-\u03f5\ud835\udc3f^\ud835\udf3d\ud835\udc94superscript\ud835\udf3c\u2032\ud835\udc9a\\mbox{\\boldmath$\\eta$}=\\operatorname*{arg\\,max}_{\\scalebox{0.75}{$\\mbox{\\boldmath$\\eta$}^{\\prime}$}:\\,\\|\\scalebox{0.75}{$\\mbox{\\boldmath$\\eta$}^{\\prime}$}\\|_{2}\\,\\leq\\,\\epsilon}L(\\hat{\\mbox{\\boldmath$\\theta$}};\\mbox{\\boldmath$s$}\\!\\!~{}+\\!\\!~{}\\mbox{\\boldmath$\\eta$}^{\\prime},\\mbox{\\boldmath$y$})    \nwhere \ud835\udf3d^^\ud835\udf3d\\hat{\\mbox{\\boldmath$\\theta$}} is the current value of the model parameters, treated as a constant, and \ud835\udc9a\ud835\udc9ay denotes the target labels. Since the exact computation of such \ud835\udf3c\ud835\udf3c\\eta is intractable in complex neural networks, we employ the Fast Gradient Method Liu et al. (2017); Miyato et al. (2017) i.e. first order approximation to obtain an approximate worst-case perturbation of norm \u03f5italic-\u03f5\\epsilon, by a single gradient computation:    \ud835\udf3c=\u03f5\u200b\ud835\udc88/\u2016\ud835\udc88\u20162\u200b,  where \ud835\udc88=\u2207\ud835\udc94L\u200b(\ud835\udf3d^;\ud835\udc94,\ud835\udc9a)\ud835\udf3citalic-\u03f5\ud835\udc88subscriptnorm\ud835\udc882,  where \ud835\udc88subscript\u2207\ud835\udc94\ud835\udc3f^\ud835\udf3d\ud835\udc94\ud835\udc9a\\mbox{\\boldmath$\\eta$}=\\epsilon\\,\\mbox{\\boldmath$g$}/\\|\\mbox{\\boldmath$g$}\\|_{2}\\mbox{, ~{}where }\\mbox{\\boldmath$g$}=\\nabla_{\\scalebox{0.8}{$\\mbox{\\boldmath$s$}$}}L(\\hat{\\mbox{\\boldmath$\\theta$}};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$})  (2)   \n\u03f5italic-\u03f5\\epsilon is a hyperparameter to be determined in the development dataset. Note that the perturbation \ud835\udf3c\ud835\udf3c\\eta is generated in the direction that significantly increases the loss L\ud835\udc3fL. We find such \ud835\udf3c\ud835\udf3c\\eta against the current model parameterized by \ud835\udf3d^^\ud835\udf3d\\hat{\\mbox{\\boldmath$\\theta$}}, at each training step, and construct an adversarial example by    \ud835\udc94adv=\ud835\udc94+\ud835\udf3csubscript\ud835\udc94adv\ud835\udc94\ud835\udf3c\\mbox{\\boldmath$s$}_{\\mathrm{adv}}=\\mbox{\\boldmath$s$}+\\mbox{\\boldmath$\\eta$}\\vspace{-1.5mm}     \n \nHowever, if we do not restrict the norm of word \u200b/\u200b character embeddings, the model could trivially learn embeddings of large norms to make the perturbations insignificant. To prevent this issue, we normalize word \u200b/\u200b character embeddings so that they have mean 0 and variance 1 for every entry, as in Miyato et al. (2017). The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples. To ensure a fair comparison, we also normalize input embeddings in our baseline model.  \n \nWhile Miyato et al. (2017) set the norm of a perturbation \u03f5italic-\u03f5\\epsilon (Eq 2) to be a fixed value for all input sentences, to generate adversarial examples for an entire sentence of a variable length and to include character embeddings besides word embeddings, we make the perturbation size \u03f5italic-\u03f5\\epsilon adaptive to the dimension of the concatenated input embedding \ud835\udc94\u2208\u211dD\ud835\udc94superscript\u211d\ud835\udc37\\mbox{\\boldmath$s$}\\in\\mathbb{R}^{D}. We set \u03f5italic-\u03f5\\epsilon to be \u03b1\u200bD\ud835\udefc\ud835\udc37\\alpha\\mbox{\\scalebox{0.95}[0.95]{$\\sqrt{D}$}} (i.e., proportional to   D\ud835\udc37\\sqrt{D} ), as the expected squared norm of \ud835\udc94\ud835\udc94s after the embedding normalization is D\ud835\udc37D. The scaling factor \u03b1\ud835\udefc\\alpha is selected from {{\\{0.001, 0.005, 0.01, 0.05, 0.1}}\\} based on the development performance in each treebank. We used 0.01 for PTB-WSJ and UD-Spanish, and 0.05 for the rest. Note that \u03b1=0\ud835\udefc0\\alpha\\!=\\!0 would generate no noise (identical to the baseline); if \u03b1=1\ud835\udefc1\\alpha\\!=\\!1, the generated adversarial perturbation would have a norm comparable to the original embedding, which could change the semantics of the input sentence Wu et al. (2017). Hence, the optimal perturbation scale \u03b1\ud835\udefc\\alpha should lie in between and be small enough to preserve the semantics of the original input.    \nAdversarial training. \n \nAt each training step, we generate adversarial examples against the current model, and train on the mixture of clean examples and adversarial examples to achieve robustness to input perturbations. To this end, we define the loss function for adversarial training as:    L~=\u03b3\u200bL\u200b(\ud835\udf3d;\ud835\udc94,\ud835\udc9a)+(1\u2212\u03b3)\u200bL\u200b(\ud835\udf3d;\ud835\udc94adv,\ud835\udc9a)~\ud835\udc3f\ud835\udefe\ud835\udc3f\ud835\udf3d\ud835\udc94\ud835\udc9a1\ud835\udefe\ud835\udc3f\ud835\udf3dsubscript\ud835\udc94adv\ud835\udc9a\\tilde{L}=\\gamma L(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$})+(1-\\gamma)L(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$}_{\\mathrm{adv}},\\mbox{\\boldmath$y$})    \nwhere L\u200b(\ud835\udf3d;\ud835\udc94,\ud835\udc9a)\ud835\udc3f\ud835\udf3d\ud835\udc94\ud835\udc9aL(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$},\\mbox{\\boldmath$y$}), L\u200b(\ud835\udf3d;\ud835\udc94adv,\ud835\udc9a)\ud835\udc3f\ud835\udf3dsubscript\ud835\udc94adv\ud835\udc9aL(\\mbox{\\boldmath$\\theta$};\\mbox{\\boldmath$s$}_{\\mathrm{adv}},\\mbox{\\boldmath$y$}) represent the loss from a clean example and the loss from its adversarial example, respectively, and \u03b3\ud835\udefe\\gamma determines the weighting between them. We used \u03b3=0.5\ud835\udefe0.5\\gamma=0.5 in all our experiments. This objective function can be optimized with respect to the model parameters \ud835\udf3d\ud835\udf3d\\theta, in the same manner as the baseline model.      \n 4 Experiments \n \nTo fully analyze the effects of adversarial training, we train and evaluate our baseline \u200b/\u200b adversarial POS tagging models on both a standard English dataset and a multilingual dataset.   \n 4.1 Datasets \n \nAs a standard English dataset, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) Marcus et al. (1993), containing 45 different POS tags. We adopt the standard split: sections 0-18 for training, 19-21 for development and 22-24 for testing Collins (2002); Manning (2011).  \n \nFor multilingual POS tagging experiments, to compare with prior work, we use treebanks from Universal Dependencies (UD) v1.2 Nivre et al. (2015) (17 POS) with the given data splits. We experiment on languages for which pre-trained Polyglot word embeddings Al-Rfou et al. (2013) are available, resulting in 27 languages listed in Table 2. We regard languages with less than 60k tokens of training data as low-resource (Table 2, bottom), as in Plank et al. (2016).    \n 4.2 Training & Evaluation Details  \nModel settings. \n \nWe initialize word embeddings with 100-dimensional GloVe Pennington et al. (2014) for English, and with 64-dimensional Polyglot Al-Rfou et al. (2013) for other languages. We use 30-dimensional character embeddings, and set the state sizes of character \u200b/\u200b word-level BiLSTM to be 50, 200 for English, 50, 100 for low resource languages, and 50, 150 for other languages. The model parameters and character embeddings are randomly initialized, as in Ma and Hovy (2016). We apply dropout Srivastava et al. (2014) to input embeddings and BiLSTM outputs for both baseline and adversarial training, with dropout rate 0.5.    \nOptimization. \n \nWe train the model parameters and word \u200b/\u200b character embeddings by the mini-batch stochastic gradient descent (SGD) with batch size 10, momentum 0.9, initial learning rate 0.01 and decay rate 0.05. We also use a gradient clipping of 5.0 Pascanu et al. (2012). The models are trained with early stopping Caruana et al. (2001) based on the development performance.    \nEvaluation. \n \nWe evaluate per token tagging accuracy on test sets. We repeat the experiment three times and report the statistical significance.     \n 4.3 Results  \nPTB-WSJ dataset. \n \n   Model Accuracy   Toutanova et al. (2003) 97.27   Manning (2011) 97.28   Collobert et al. (2011) 97.29   S\u00f8gaard (2011) 97.50   Ling et al. (2015) 97.78   Ma and Hovy (2016) 97.55   Yang et al. (2017) 97.55    Hashimoto et al. (2017)  97.55   Ours \u2013 Baseline (BiLSTM-CRF) 97.54   Ours \u2013 Adversarial 97.58    \nTable 1: POS tagging accuracy on the PTB-WSJ test set, with other top-performing systems.   \n \nTable 1 shows the POS tagging results. As expected, our baseline (BiLSTM-CRF) model (accuracy 97.54%) performs on par with other state-of-the-art systems. Built upon this baseline, our adversarial training (AT) model reaches accuracy 97.58% thanks to its regularization power, outperforming recent POS taggers except Ling et al. (2015). The improvement over the baseline is statistically significant, with p\ud835\udc5dp-value  <<  0.05 on the t\ud835\udc61t-test. We provide additional analysis on this result in later sections.    \nMultilingual dataset (UD). \n \nFigure 2: Learning curves for three representative languages (Romanian is low-resource). We show the transition of loss (defined in Eq 1) on the development sets.   \n \nTable 2:  POS tagging accuracy (test) for 27 UD v1.2 treebanks, with other recent works, Plank et al. (2016), Berend (2017) and Nguyen et al. (2017). For Plank et al. (2016), we include the traditional baselines TNT and CRF, and their state-of-the-art model that employs a multi-task BiLSTM. Languages with \u2219 are morphologically rich, and those at the bottom (\u2018el\u2019 to \u2018ta\u2019) are low-resource, containing less than 60k tokens in their training sets.   \n \nExperimental results are summarized in Table 2. Our AT model shows clear advantages over the baseline in all of the 27 languages (average improvement \u223csimilar-to\\sim0.25%; see the two shaded columns). Considering that our baseline (BiLSTM-CRF) is already a top performing model for POS tagging, these improvements made by AT are substantial. The improvements are also statistically significant for all the languages, with p\ud835\udc5dp-value  <<  0.05 on the t\ud835\udc61t-test, suggesting that the regularization by AT is generally effective across different languages. Moreover, our AT model achieves state-of-the-art on nearly all of the languages, except the five where Plank et al. (2016)\u2019s multi-task BiLSTM yielded better results. Among the five, most languages are morphologically rich (\u2219).222We followed the criteria of morphological richness used in Nguyen et al. (2017). We suspect that their joint training of word rarity may be of particular help in processing morphologically complex words.  \n \n \n \n \n \nEnglish (WSJ)   \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 3240 7687 20908 97819 129654   Baseline 92.25 95.36 96.03 98.19 97.53    Adversarial 92.01 95.52 96.10 98.23 97.57              \n \n \nFrench (UD)   \n \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 356 839 1492  *4523*   *7210*    Baseline 87.64 94.05 94.03 98.43 96.48    Adversarial 87.92 94.88 94.03 98.50 96.63              \n  \nTable 3: POS tagging accuracy (test) on different subsets of words, categorized by their frequency of occurrence in training. The second row shows the number of tokens in the test set that are in each category. The third and fourth rows show the performance of our two models. Better scores are underlined. The biggest improvement is in bold.   \n \nAdditionally, we see that our AT model achieves notably large improvements over the baseline in resource-poor languages (the bottom of Table 2), with average improvement 0.35%, as compared to that for resource-rich languages, 0.20%. To further visualize the regularization effects, we present the learning curves for three representative languages, English (WSJ), French (UD-fr) and Romanian (UD-ro, low-resource), based on the development loss (see Figure 2). For all the three languages, we can observe that the AT model (red solid line) prevents overfitting better than the baseline (black dotted line), and this advantage is more significant in low resource languages. For example, in Romanian, the baseline model starts to increase development loss after 1,000 iterations even with dropout, whereas the AT model keeps improving until 2,500 iterations, achieving notably lower development loss (0.4 down). These results illustrate that AT can prevent overfitting especially well on small datasets and can augment the regularization power beyond dropout. AT can also be viewed as an effective means of data augmentation, where we generate and train with new examples the current model is particularly vulnerable to at every time step, enhancing the robustness of the model. AT can therefore be a promising tool to process low resource languages.      \n 5 Analysis \n \n \n \n \n \nEnglish (WSJ)   \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 6480 15374 41815 \u200b195637 259306   Baseline 97.76 97.71 97.80 97.45 97.53    Adversarial 98.06 97.71 97.89 97.47 97.57              \n \n \n \nFrench (UD)   \n \n       \n \nWord Frequency   0 1-10 10-100 100- Total      # Tokens 712  *1678*  2983  *9045*   *14418*    Baseline 95.08 97.08 97.58 96.11 96.48    Adversarial 95.37 97.26 97.79 96.23 96.63              \n  \nTable 4: POS tagging accuracy (test) on neighboring words. We cluster all words in the test set in the same way as Table 3 and consider the tagging performance on the neighbors (left and right) of these words in the test text.   \n \nIn the previous sections, we demonstrated the regularization power of adversarial training (AT) on different languages, based on the overall POS tagging performance and learning curves. In this section, we conduct further analyses on the robustness of AT from NLP specific aspects such as word statistics, sequence modeling, downstream tasks, and word representation learning.  \n \nWe find that AT can boost tagging accuracy on rare words and neighbors of unseen words (\u00a75.1). Furthermore, this robustness against rare \u200b/\u200b unseen words leads to better sentence-level accuracy and downstream dependency parsing (\u00a75.2). We illustrate these findings using two major languages, English (WSJ) and French (UD), which have substantially large training and testing data to discuss vocabulary statistics and sentence-level performance. Finally, we study the effects of AT on word representation learning (\u00a75.3), and the applicability of AT to different sequential tasks (\u00a75.4).   \n 5.1 Word-level Analysis \n \nPoor tagging accuracy on rare \u200b/\u200b unseen words is one of the bottlenecks in current POS taggers Manning (2011); Plank et al. (2016). Aiming to reveal the effects of AT on rare \u200b/\u200b unseen words, we analyze tagging performance at the word level, considering vocabulary statistics.   \nWord frequency. \n \nTo define rare \u200b/\u200b unseen words, we consider each word\u2019s frequency of occurrence in the training set. We categorize all words in the test set based on this frequency and study the test tagging accuracy for each group (see Table 3).333 To conduct the analysis, we picked the median result from the three repeated experiments. In both languages, the AT model achieves large improvements over the baseline on rare words (e.g., frequency 1-10 in training), as opposed to more frequent words. This result again corroborates the data augmentation power of AT under small training examples. On the other hand, we did not observe meaningful improvements on unseen words (frequency 0 in training). A possible explanation is that AT can facilitate the learning of words with at least a few occurrences in training (rare words), but is not particularly effective in inferring the POS tags of words for which no training examples are given (unseen words).    \nNeighboring words. \n \nOne important characteristic of natural language tasks is the sequential nature of inputs (i.e., sequence of words), where each word influences the function of its neighboring words. Since our model uses BiLSTM-CRF for that reason, we also study the tagging performance on the neighbors of rare \u200b/\u200b unseen words, and analyze the effects of AT with the sequence model in mind. In Table 4, we cluster all words in the test set based on their frequency in training again, and consider the tagging accuracy on the neighbors (left and right) of these words in the test text. We observe that AT tends to achieve large improvements over the baseline on the neighbors of unseen words (training frequency 0), while the improvements on the neighbors of more frequent words remain moderate. Our AT model thus exhibits strong stability to uncertain neighbors, as compared to the baseline. We suspect that because we generate adversarial examples against entire input sentences, training with adversarial examples makes the model more robust not only to perturbations in each word but also to perturbations in its neighboring words, leading to greater stability to uncertain neighbors.     \n 5.2 Sentence-level & Downstream Analysis \n \n \n \n \n \nEnglish (WSJ)   \n \n \n        Sentence-         \n \nStanford Parser          \n \nParsey McParseface        level Acc.\u200b  UAS LAS    UAS LAS      Baseline 59.08 91.53 89.30 91.68 87.92   Adversarial 59.61 91.57 89.35 91.73 87.97    \n \n(w/ gold tags)\u200b  \u2013 (92.07) (90.63) (91.98) (88.60)              \n \n \n \nFrench (UD)   \n \n \n        Sentence-         \n \nParsey Universal        level Acc.\u200b    UAS LAS      Baseline 52.35 84.85 80.36   Adversarial 53.36 85.01 80.55    \n \n(w/ gold tags)\u200b  \u2013 (85.05) (80.75)            \n  \nTable 5: Sentence-level accuracy and downstream dependency parsing performance by our baseline \u200b/\u200b adversarial POS taggers.   \n \nIn the word-level analysis, we showed that AT can boost tagging accuracy on rare words and the neighbors of unseen words, enhancing overall robustness on rare \u200b/\u200b unseen words. In this section, we discuss the benefit of our improved POS tagger in a major downstream task, dependency parsing.  \n \nMost of the recent state-of-the-art dependency parsers take predicted POS tags as input (e.g. Chen and Manning (2014); Andor et al. (2016); Dozat and Manning (2017)). Dozat et al. (2017) empirically show that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger, while POS tags predicted by the UDPipe tagger Straka et al. (2016) do not contribute to parsing performance as much. This observation illustrates that POS tagging performance has a great influence on dependency parsing, motivating the hypothesis that the POS tagging improvements gained from our adversarial training help dependency parsing.  \n \nTo test the hypothesis, we consider three settings in dependency parsing of English and French: using POS tags predicted by the baseline model, using POS tags predicted by the AT model, and using gold POS tags. For English (PTB-WSJ), we first convert the treebank into Stanford Dependencies (SD) using Stanford CoreNLP (ver 3.8.0) Manning et al. (2014), and then apply two well-known dependency parsers: Stanford Parser (ver 3.5.0) Chen and Manning (2014) and Parsey McParseface (SyntaxNet) Andor et al. (2016). For French (UD), we use Parsey Universal from SyntaxNet. The three parsers are all publicly available and pre-trained on corresponding treebanks.  \n \nTable 5 shows the results of the experiments. We can observe improvements in both languages by using the POS tags predicted by our AT POS tagger. As Manning (2011) points out, when predicted POS tags are used for downstream dependency parsing, a single bad mistake in a sentence can greatly damage the usefulness of the POS tagger. The robustness of our AT POS tagger against rare \u200b/\u200b unseen words helps to mitigate such an issue. This advantage can also be observed from the AT POS tagger\u2019s notably higher sentence-level accuracy than the baseline (see Table 5 left). Nonetheless, gold POS tags still yield better parsing results as compared to the baseline \u200b/\u200b AT POS taggers, supporting the claim that POS tagging needs further improvement for downstream tasks.    \n 5.3 Effects on Representation Learning \n \n \n \n \n \nEnglish (WSJ)   \n \n      POS Cluster NN VB JJ RB Avg.      1) Initial \n \n(GloVe)  0.243 0.426 0.220 0.549 0.359   2) Baseline 0.280 0.431 0.309 0.667 0.422   3) Adversarial 0.281 0.436 0.306 0.675 0.424              \n \n \nFrench (UD)   \n \n \n      POS Cluster NOUN VERB ADJ ADV Avg.      1) Initial \n \n(polyglot)   0.215 0.233 0.210 0.540 0.299   2) Baseline 0.258 0.271 0.262 0.701 0.373   3) Adversarial 0.263 0.272 0.263 0.720 0.379              \n  \nTable 6:  Cluster tightness evaluation for word embeddings, based on the cosine similarity measure. Higher scores indicate better clustering (cleaner word vector distribution). Each row corresponds to word vectors 1) at the beginning, 2) after baseline training, and 3) after adversarial training.   \n \n \n \n \nEnglish (WSJ)   \n \n \n      \n \nPerturbation scale \u03b1\ud835\udefc\\alpha  0 0.001 0.01 0.05 0.1 0.5      \n \nAvg. cluster tightness   0.422 0.423 0.424 0.429 0.436 0.429               \n  \nTable 7: Average cluster tightness for word embeddings trained with varied perturbation scale \u03b1\ud835\udefc\\alpha (0 indicates baseline training).   \n \nNext, we perform an analysis on representation learning of words (word embeddings) for the English (PTB-WSJ) and French (UD) experiments. We hypothesize that adversarial training (AT) helps to learn better word embeddings so that the POS tag prediction of a word cannot be influenced by a small perturbation in the input embedding.  \n \nTo verify this hypothesis, we cluster all words in the test set based on their correct POS tags444We excluded words with multiple tags in the test text. and evaluate the tightness of the word vector distribution within each cluster. We compare this clustering quality among the three settings: 1) beginning (initialized with GloVe or Polyglot), 2) after baseline training (50 epochs), and 3) after adversarial training (50 epochs), to study the effects of AT on word representation learning.  \n \nFor evaluating the tightness of word vector distribution, we employ the cosine similarity metric, which is widely used as a measure of the closeness between two word vectors (e.g., Mikolov et al. (2013); Pennington et al. (2014)). To measure the tightness of each cluster, we compute the cosine similarity for every pair of words within, and then take the average. We also report the average tightness across all the clusters.  \n \nThe evaluation results are summarized in Table 6. We report the tightness scores for the four major clusters: noun, verb, adjective, and adverb (from left to right). As can be seen from the table, for both languages, adversarial training (AT) results in cleaner word embedding distributions than the baseline, with a higher cosine similarity within each POS cluster, and with a clear advantage in the average tightness across all the clusters. In other words, the learned word vectors show stronger correlations with their POS tags. This result confirms that training with adversarial examples can help to learn cleaner word embeddings so that the meaning \u200b/\u200b grammatical function of a word cannot be altered by a small perturbation in its embedding. This analysis provides a means to interpret the robustness to input perturbations, from the perspective of NLP.   \nRelation with perturbation size \u03f5bold-italic-\u03f5\\boldsymbol{\\epsilon}.  \n \nWe also study how the size of added perturbations influences word representation learning in adversarial training. Recall that we set the norm of a perturbation \u03f5italic-\u03f5\\epsilon to be \u03b1\u200bD\ud835\udefc\ud835\udc37\\alpha\\mbox{\\scalebox{0.95}[0.95]{$\\sqrt{D}$}}, where D\ud835\udc37D is the dimension of the concatenated input embeddings (see \u00a73.2). For instance, \u03b1=0\ud835\udefc0\\alpha\\!=\\!0 would produce no noise; \u03b1=1\ud835\udefc1\\alpha\\!=\\!1 would generate a perturbation of a norm equivalent to the original word embeddings. We hypothesize that AT facilitates word representation learning when \u03b1\ud835\udefc\\alpha is small enough to preserve the semantics of input words, but can hinder the learning when \u03b1\ud835\udefc\\alpha is too large. To test the hypothesis, we repeat the clustering evaluation for word embeddings trained with varied perturbation scale \u03b1\ud835\udefc\\alpha: 0, 0.001, 0.01, 0.05, 0.1, 0.5 (see Table 7). We observe that the quality of learned word embedding distribution keeps improving as \u03b1\ud835\udefc\\alpha goes up from 0 to 0.1, but starts to drop around \u03b1=0.5\ud835\udefc0.5\\alpha\\!=\\!0.5. We also find that this optimal \u03b1\ud835\udefc\\alpha in word embedding learning (i.e., 0.1) is larger than the \u03b1\ud835\udefc\\alpha which yielded the best tagging performance on development sets (i.e., 0.01 or 0.05). A possible explanation is that while word embeddings can adapt to relatively large \u03b1\ud835\udefc\\alpha (e.g., 0.1) during training, as adversarial perturbations are generated at the embedding level, such \u03b1\ud835\udefc\\alpha could change the semantics of the input from the current tagging model\u2019s perspective and hinder the training of tagging.     \n 5.4 Other Sequence Labeling Tasks \n \nFinally, to further confirm the applicability of AT, we experiment with our BiLSTM-CRF AT model in different sequence labeling tasks: chunking and named entity recognition (NER).  \n \n   Model F1   Tsuruoka et al. (2011) 93.81   Collobert et al. (2011) 94.32   Yang et al. (2017) 94.66   Suzuki and Isozaki (2008) 95.15   S\u00f8gaard and Goldberg (2016) 95.56   Hashimoto et al. (2017) 95.77    Peters et al. (2017)  96.37   Ours \u2013 Baseline (BiLSTM-CRF) 95.18   Ours \u2013 Adversarial 95.25    \nTable 8: Chunking F1 scores on the CoNLL-2000 task, with other top performing models.  \n \nChunking can be performed as a sequence labeling task that assigns a chunking tag (B-NP, I-VP, etc.) to each word. We conduct experiments on the CoNLL 2000 shared task with the standard data split: PTB-WSJ Sections 15-18 for training and 20 for testing. We use Section 19 as the development set and employ the IOBES tagging scheme, following Hashimoto et al. (2017).  \n \n   Model F1   Collobert et al. (2011) 89.59   Huang et al. (2015) 90.10   Chiu and Nichols (2016) 90.91   Lample et al. (2016) 90.94   Luo et al. (2015) 91.20   Ma and Hovy (2016) 91.21    Peters et al. (2017)  91.93   Ours \u2013 Baseline (BiLSTM-CRF) 91.22   Ours \u2013 Adversarial 91.56    \nTable 9: NER F1 scores on the CoNLL-2003 (English) task, with other top performing models.  \n \nNER aims to assign an entity type to each word, such as person, location, organization, and misc. We conduct experiments on the CoNLL-2003 (English) shared task Tjong Kim Sang and De Meulder (2003), adopting the IOBES tagging scheme as in Lample et al. (2016); Ma and Hovy (2016).  \n \nThe results are summarized in Table 8 and 9. AT enhanced F1 score from the baseline BiLSTM-CRF model\u2019s 95.18 to 95.25 for chunking, and from 91.22 to 91.56 for NER, also significantly outperforming Ma and Hovy (2016). These improvements made by AT are bigger than that for English POS tagging, most likely due to the larger room for improvement in chunking and NER. The improvements are again statistically significant, with p\ud835\udc5dp-value  <<  0.05 on the t\ud835\udc61t-test. The experimental results suggest that the proposed adversarial training scheme is generally effective across different sequence labeling tasks.  \n \nOur BiLSTM-CRF AT model did not reach the performance by Hashimoto et al. (2017)\u2019s multi-task model and Peters et al. (2017)\u2019s state-of-the-art system that incorporates pretrained language models. It would be interesting future work to combine the strengths of these joint models (e.g., syntactic and semantic aids) and adversarial training (e.g., robustness).     \n 6 Conclusion \n \nWe proposed and carefully analyzed a POS tagging model that exploits adversarial training (AT). In our multilingual experiments, we find that AT achieves substantial improvements on all the languages tested, especially on low resource ones. AT also enhances the robustness to rare \u200b/\u200b unseen words and sentence-level accuracy, alleviating the major issues of current POS taggers, and contributing to the downstream task, dependency parsing. Furthermore, our analyses on different languages, word \u200b/\u200b neighbor statistics and word representation learning reveal the effects of AT from the perspective of NLP. The proposed AT model is applicable to general sequence labeling tasks. This work therefore provides a strong basis and motivation for utilizing AT in natural language tasks.     \nAcknowledgements \n \nWe would like to thank Rui Zhang, Jonathan Kummerfeld, Yutaro Yamada, as well as all the anonymous reviewers for their helpful feedback and suggestions on this work.  \n   \nReferences \n \n Al-Rfou et al. (2013)  Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013.  Polyglot: Distributed word representations for multilingual nlp.  In CoNLL.   \n Andor et al. (2016)  Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016.  Globally normalized transition-based neural networks.  In ACL.   \n Bahdanau et al. (2015)  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.  Neural machine translation by jointly learning to align and translate.  In ICLR.   \n Berend (2017)  G\u00e1bor Berend. 2017.  Sparse coding of neural word embeddings for multilingual sequence labeling.  TACL .   \n Caruana et al. (2001)  Rich Caruana, Steve Lawrence, and C Lee Giles. 2001.  Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping.  In NIPS.   \n Chen and Manning (2014)  Danqi Chen and Christopher D Manning. 2014.  A fast and accurate dependency parser using neural networks.  In EMNLP.   \n Chiu and Nichols (2016)  Jason PC Chiu and Eric Nichols. 2016.  Named entity recognition with bidirectional lstm-cnns.  In TACL.   \n Collins (2002)  Michael Collins. 2002.  Discriminative training methods for hidden markov models.  In EMNLP.   \n Collobert et al. (2011)  Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.  Natural language processing (almost) from scratch.  The Journal of Machine Learning Research 12:2493\u20132537.   \n Dozat and Manning (2017)  Timothy Dozat and Christopher D. Manning. 2017.  Deep biaffine attention for neural dependency parsing.  In ICLR.   \n Dozat et al. (2017)  Timothy Dozat, Peng Qi, and Christopher D. Manning. 2017.  Stanford\u2019s graph-based neural dependency parser at the conll 2017 shared task.  In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. pages 20\u201330.   \n Goodfellow et al. (2014)  Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.  Generative adversarial nets.  In NIPS.   \n Goodfellow et al. (2015)  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015.  Explaining and harnessing adversarial examples.  In ICLR.   \n Gui et al. (2017)  Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and Xuanjing Huang. 2017.  Part-of-speech tagging for twitter with adversarial neural networks.  In EMNLP.   \n Hashimoto et al. (2017)  Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017.  A joint many-task model: Growing a neural network for multiple NLP tasks.  In EMNLP.   \n Hochreiter and Schmidhuber (1997)  Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997.  Long short-term memory.  Neural Computation 9(8):1735\u20131780.   \n Huang et al. (2015)  Zhiheng Huang, Wei Xu, and Kai Yu. 2015.  Bidirectional lstm-crf models for sequence tagging.  arXiv preprint arXiv:1508.01991 .   \n Iyyer et al. (2015)  Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum\u00e9 III. 2015.  Deep unordered composition rivals syntactic methods for text classification.  In ACL.   \n Jia and Liang (2017)  Robin Jia and Percy Liang. 2017.  Adversarial examples for evaluating reading comprehension systems.  In EMNLP.   \n Kim et al. (2017)  Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and Eric Fosler-Lussier. 2017.  Cross-lingual transfer learning for pos tagging without cross-lingual resources.  In EMNLP.   \n Lafferty et al. (2001)  John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001.  Conditional random fields: Probabilistic models for segmenting and labeling sequence data.  In ICML.   \n Lample et al. (2016)  Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016.  Neural architectures for named entity recognition.  In NAACL.   \n Li et al. (2017)  Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. 2017.  Adversarial learning for neural dialogue generation.  In EMNLP.   \n Ling et al. (2015)  Wang Ling, Tiago Lu\u00eds, Lu\u00eds Marujo, R\u00e1mon Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015.  Finding function in form: Compositional character models for open vocabulary word representation.  In EMNLP.   \n Liu et al. (2017)  Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017.  Delving into transferable adversarial examples and black-box attacks.  In ICLR.   \n Luo et al. (2015)  Gang Luo, Xiaojiang Huang, Chin-Yew Lin, and Zaiqing Nie. 2015.  Joint entity recognition and disambiguation.  In EMNLP.   \n Ma and Hovy (2016)  Xuezhe Ma and Eduard Hovy. 2016.  End-to-end sequence labeling via bi-directional lstm-cnns-crf.  In ACL.   \n Manning et al. (2014)  Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014.  The Stanford CoreNLP natural language processing toolkit.  In Association for Computational Linguistics (ACL) System Demonstrations.   \n Manning (2011)  Christopher D Manning. 2011.  Part-of-speech tagging from 97% to 100%: is it time for some linguistics?  Computational Linguistics and Intelligent Text Processing pages 171\u2013189.   \n Marcus et al. (1993)  Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.  Building a large annotated corpus of english: The penn treebank.  Computational Linguistics 19(2):313\u2013330.   \n Mikolov et al. (2013)  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.  Distributed representations of words and phrases and their compositionality.  In NIPS.   \n Miyato et al. (2017)  Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2017.  Adversarial training methods for semi-supervised text classification.  In ICLR.   \n Nguyen et al. (2017)  Dat Quoc Nguyen, Mark Dras, and Mark Johnson. 2017.  A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing.  In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.   \n Nivre et al. (2015)  Joakim Nivre, \u017deljko Agi\u0107, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina Bosco, Sam Bowman, Giuseppe G. A. Celano, Miriam Connor, Marie-Catherine de Marneffe, Arantza Diaz de Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Toma\u017e Erjavec, Rich\u00e1rd Farkas, Jennifer Foster, Daniel Galbraith, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Berta Gonzales, Bruno Guillaume, Jan Haji\u010d, Dag Haug, Radu Ion, Elena Irimia, Anders Johannsen, Hiroshi Kanayama, Jenna Kanerva, Simon Krek, Veronika Laippala, Alessandro Lenci, Nikola Ljube\u0161i\u0107, Teresa Lynn, Christopher Manning, C\u0103t\u0103lina M\u0103r\u0103nduc, David Mare\u010dek, H\u00e9ctor Mart\u00ednez Alonso, Jan Ma\u0161ek, Yuji Matsumoto, Ryan McDonald, Anna Missil\u00e4, Verginica Mititelu, Yusuke Miyao, Simonetta Montemagni, Shunsuke Mori, Hanna Nurmi, Petya Osenova, Lilja \u00d8vrelid, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen, Barbara Plank, Martin Popel, Prokopis Prokopidis, Sampo Pyysalo, Loganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simk\u00f3, Kiril Simov, Aaron Smith, Jan \u0160t\u011bp\u00e1nek, Alane Suhr, Zsolt Sz\u00e1nt\u00f3, Takaaki Tanaka, Reut Tsarfaty, Sumire Uematsu, Larraitz Uria, Viktor Varga, Veronika Vincze, Zden\u011bk \u017dabokrtsk\u00fd, Daniel Zeman, and Hanzhi Zhu. 2015.  Universal dependencies 1.2.  LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University.   \n Pascanu et al. (2012)  Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012.  On the difficulty of training recurrent neural networks.  arXiv preprint arXiv:1211.5063 .   \n Pennington et al. (2014)  Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.  Glove: Global vectors for word representation.  In EMNLP.   \n Peters et al. (2017)  Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017.  Semi-supervised sequence tagging with bidirectional language models.  In ACL.   \n Plank et al. (2016)  Barbara Plank, Anders S\u00f8gaard, and Yoav Goldberg. 2016.  Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss.  In ACL.   \n Schuster and Paliwal (1997)  M. Schuster and K.K. Paliwal. 1997.  Bidirectional recurrent neural networks.  Trans. Sig. Proc. 45(11):2673\u20132681.   \n Shaham et al. (2015)  Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2015.  Understanding adversarial training: Increasing local stability of neural nets through robust optimization.  arXiv preprint arXiv:1511.05432 .   \n S\u00f8gaard (2011)  Anders S\u00f8gaard. 2011.  Semi-supervised condensed nearest neighbor for part-of-speech tagging.  In ACL-HLT.   \n S\u00f8gaard and Goldberg (2016)  Anders S\u00f8gaard and Yoav Goldberg. 2016.  Deep multi-task learning with low level tasks supervised at lower layers.  In ACL.   \n Srivastava et al. (2014)  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.  Dropout: A simple way to prevent neural networks from overfitting.  Journal of Machine Learning Research 15:1929\u20131958.   \n Straka et al. (2016)  Milan Straka, Jan Hajic, and Jana Strakov\u00e1. 2016.  Udpipe: Trainable pipeline for processing conll-u files performing tokenization, morphological analysis, pos tagging and parsing.  In LREC.   \n Suzuki and Isozaki (2008)  Jun Suzuki and Hideki Isozaki. 2008.  Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.  ACL-HLT .   \n Szegedy et al. (2014)  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014.  Intriguing properties of neural networks.  In ICLR.   \n Tjong Kim Sang and De Meulder (2003)  Erik F Tjong Kim Sang and Fien De Meulder. 2003.  Introduction to the conll-2003 shared task: Language-independent named entity recognition.  In CoNLL.   \n Toutanova et al. (2003)  Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003.  Feature-rich part-of-speech tagging with a cyclic dependency network.  In HLT-NAACL.   \n Tsuruoka et al. (2011)  Yoshimasa Tsuruoka, Yusuke Miyao, and Jun\u2019ichi Kazama. 2011.  Learning with lookahead: Can history-based models rival globally optimized models?  In CoNLL.   \n Wang et al. (2017)  Beilun Wang, Ji Gao, and Yanjun Qi. 2017.  A theoretical framework for robustness of (deep) classifiers against adversarial samples.  In ICLR.   \n Wu et al. (2017)  Yi Wu, David Bamman, and Stuart Russell. 2017.  Adversarial training for relation extraction.  In EMNLP.   \n Xie et al. (2017)  Ziang Xie, Sida I Wang, Jiwei Li, Daniel Levy, Aiming Nie, Dan Jurafsky, and Andrew Y Ng. 2017.  Data noising as smoothing in neural network language models.  In ICLR.   \n Yang et al. (2017)  Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2017.  Transfer learning for sequence tagging with hierarchical recurrent networks.  In ICLR.       \n\u25c4  Feeling\nlucky?  Conversion\nreport Report\nan issue View original\non arXiv\u25ba \n  Copyright Privacy Policy \nGenerated on Sun Mar 3 20:57:27 2024 by LaTeXML     ", "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training \u2013 arXiv Vanity", "description": "<strong>Adversarial</strong> <strong>training</strong> (AT)111We distinguish this from <strong>the</strong> <strong>Generative</strong> <strong>Adversarial</strong> Networks (GANs). is a powerful regularization <strong>method</strong> <strong>for</strong> neural networks, aiming to achieve <strong>robustness</strong> <strong>to</strong> input perturbations. Yet, the specific effects of <strong>the</strong> <strong>robustness</strong> obtained by AT are still unclear in the ...", "usage": {"tokens": 14193}, "retrieved_text": [" \n \n \n \nRobust Multilingual Part-of-Speech Tagging via Adversarial Training \n  Michihiro Yasunaga Jungo Kasai Dragomir Radev \nDepartment of Computer Science, Yale University \n  {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu    \n \nAbstract \nAdversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. 1 Introduction \n \nFigure 1: Illustration of our architecture for adversarial POS tagging.", "Robust Multilingual Part-of-Speech Tagging via Adversarial Training Michihiro Yasunaga Jungo Kasai Dragomir Radev Department of Computer Science, Yale University {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu Abstract Adversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. 1 Introduction Figure 1: Illustration of our architecture for adversarial POS tagging. Given a sentence, we input the normalized word embeddings (\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0,\ud835\udc98\ud835\udfd1subscript\ud835\udc981subscript\ud835\udc982subscript\ud835\udc983\\mbox{\\boldmath$\\uline{w}_{1}$},\\mbox{\\boldmath$\\uline{w}_{2}$},\\mbox{\\boldmath$\\uline{w}_{3}$}) and character embeddings (showing \ud835\udc84\ud835\udfcf,\ud835\udc84\ud835\udfd0,\ud835\udc84\ud835\udfd1subscript\ud835\udc841subscript\ud835\udc842subscript\ud835\udc843\\mbox{\\boldmath$\\uline{c}_{1}$},\\mbox{\\boldmath$\\uline{c}_{2}$},\\mbox{\\boldmath$\\uline{c}_{3}$} for w1subscript\ud835\udc641w_{1}).", "core problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training. With a BiLSTM-CRF model Huang et al. (2015); Ma and Hovy (2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word \u200b/\u200b character embeddings. In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset (27 languages), with thorough analyses of the following points: \u2022 Effects on different target languages \u2022 Vocabulary statistics and tagging accuracy \u2022 Influence on downstream tasks \u2022 Representation learning of words In our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages. Furthermore, our analyses reveal the following insights into adversarial training in the context of NLP:  \n \n \n \u2022 \n \nThe regularization effects of adversarial training (AT) are general across different languages. AT can prevent overfitting especially well when training examples are scarce, providing an effective tool to process low resource languages."]}, {"text": " \n \n Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. \n  Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    26hzhang   /  DL-NLP-Readings  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 268   \n \n   Star  850      \n   \n \n   Code   \n   Issues 0  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Security    \n      Insights             \n   \n       \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time.      \n     \n    \n \n    \n            \n            \n \n ", "title": "DL-NLP-Readings/Robust Multilingual Part-of-Speech Tagging via ...", "description": "My Reading Lists of Deep Learning and Natural <strong>Language</strong> Processing - DL-NLP-Readings/<strong>Robust</strong> <strong>Multilingual</strong> <strong>Part</strong>-<strong>of</strong>-<strong>Speech</strong> <strong>Tagging</strong> <strong>via</strong> <strong>Adversarial</strong> <strong>Training</strong>.bib at master \u00b7 IsaacChanghau/DL-NLP-Readings", "usage": {"tokens": 339}, "retrieved_text": [" \n \n Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    26hzhang   /  DL-NLP-Readings  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 268   \n \n   Star  850      \n   \n \n   Code   \n   Issues 0  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Security    \n      Insights             \n   \n       \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time."]}, {"text": "   \n \n1 Introduction \n 2 Related Work \n \nRetrieval-Augmented Generation with Noisy Context \nAdversarial Training   \n 3 Methodology \n \n3.1 Problem Setup \n3.2 Diverse Retrieval Noises \n3.3 Retrieval-augmented Adaptive Adversarial Training \n3.4 Incorporating Noise Awareness   \n 4 Experiments \n \n4.1 Dataset Construction \n4.2 Evaluation Metrics \n 4.3 Baseline Methods \n \nZero-Shot Methods \nFine-tuning Methods   \n4.4 Implementation Details   \n 5 Experimental Results \n \n5.1 Main Results \n5.2 Ablation Study \n 5.3 Further Discussion \n \nWhat types of adversarial samples does RAAT employ during training?     \n6 Conclusion \n7 Limitations \nA Has the Model Truly Attained Noise Awareness?   \n \n \n \nEnhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training \n  Feiteng Fang1,2, Yuelin Bai211footnotemark: 1, Shiwen Ni2, Min Yang222footnotemark: 2, Xiaojun Chen3, Ruifeng Xu4 \n1University of Science and Technology of China \n2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences \n3Shenzhen University 4Harbin Institute of Technology (Shenzhen) \nfeitengfang@mail.ustc.edu.cn, {yl.bai, sw.ni, min.yang}@siat.ac.cn, \nxjchen@szu.edu.cn, xuruifeng@hit.edu.cn Equal contribution.Corresponding author.  \n \nAbstract \nLarge Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs\u2019 capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model\u2019s training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model\u2019s capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.  \n \n \nEnhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training \n \n   Feiteng Fang1,2\u2020\u2020thanks: Equal contribution., Yuelin Bai211footnotemark: 1, Shiwen Ni2\u2020\u2020thanks: Corresponding author., Min Yang222footnotemark: 2, Xiaojun Chen3, Ruifeng Xu4  1University of Science and Technology of China  2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences  3Shenzhen University 4Harbin Institute of Technology (Shenzhen)  feitengfang@mail.ustc.edu.cn, {yl.bai, sw.ni, min.yang}@siat.ac.cn,  xjchen@szu.edu.cn, xuruifeng@hit.edu.cn  \n    \n 1 Introduction \n \nFigure 1:  An illustrative example of the RAG process applied to question answering. The model predicts the correct answer with accurate retrieved text. However, it fails to produce the right answer when the retrieved text contains misleading or inaccurate information.   \n \nLarge language models (LLMs) have garnered substantial attention in both academic and industrial research within the domain of artificial intelligence due to their remarkable capabilities (Brown et al., 2020; Bubeck et al., 2023). Despite their immense power, LLMs face challenges such as hallucinations and outdated knowledge (Gao et al., 2023). Moreover, a lack of domain knowledge may hinder their performance on domain-specific tasks (Kandpal et al., 2023). To mitigate these challenges, recent studies improve LLMs by retrieving passages from external databases and pretending them in context, constituting a framework known as retrieval-augmented language models (RALMs) (Mao et al., 2020; Lewis et al., 2020).  \n \nHowever, RALMs also present significant limitations. Previous studies (Yoran et al., 2023; Yu et al., 2023; Shi et al., 2023) have empirically demonstrated that retrieved noisy passages are problematic for LLMs, resulting in performance degradation. We term this issue as the noise robustness problem of RALMs. As illustrated in Figure 1, the model can provide correct answers when the retrieving context is accurate and related to the query. However, when the retrieved context contains misleading or inaccurate information, the model may yield incorrect answers. As the retriever inherently cannot achieve complete accuracy, the presence of noise in the retrieved context is inevitable. Therefore, designing robust algorithms against retrieved noises is of great practical importance.  \n \nRecently, several studies (Yoran et al., 2023; Li et al., 2022) have attempted to enhance the noise robustness of RALMs through noisy training, which involves incorporating retrieved noisy contexts into fine-tuning data. While noisy training exhibits promise, its effectiveness heavily relies on the composition of the training dataset. Incorrectly introducing noises to the training data can lead to model overfitting, adversely affecting generalization. In practice, meticulous adjustment of the type and intensity of noises is essential to ensure the model\u2019s proficiency across various tasks and datasets. This demands significant experimentation and tuning, adding complexity to the development process. Moreover, the lack of clear classification for retrieval noises in current studies stands in contrast to the diverse range of noises encountered in real retrieval environments.  \n \nThis paper systematically explores three types of retrieval noises: (i) contexts that are superficially related to the query but lack the correct answer (Relevant retrieval noise), (ii) contexts that are irrelevant to the query (Irrelevant retrieval noise), and (iii) contexts that are topically related to the query but contain incorrect information (Counterfactual retrieval noise). Our empirical study indicates that LLMs exhibit varying robustness to these three types of noise. Compared to entirely irrelevant texts, texts that are superficially related to the query or those containing counterfactual details often lead to more misinformation.  \n \nIn response to diverse types of noises, we propose a novel approach named Retrieval-augmented Adaptive Adversarial Training (RAAT), which employs adaptive adversarial training to dynamically regulate the model\u2019s training process in response to retrieved noisy texts. Concretely, RAAT generates adversarial samples (noises) by considering the model\u2019s sensitivity to different types of noises during training, which aligns with the min-max paradigm of adversarial training (Morris et al., 2020; Ivgi and Berant, 2021). Moreover, RAAT utilizes multi-task learning (Ruder, 2017) to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance.  \n \nThe main contributions of this paper can be summarized as follows: \n \n \u2022 \n \nWe systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises.   \n \u2022 \n \nWe propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises.    \n \u2022 \n \nWe set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments.       \n 2 Related Work  \nRetrieval-Augmented Generation with Noisy Context \n \nRetrieval-Augmented Language Models (RALMs) have shown impressive performance in various NLP tasks (Gao et al., 2023; Zhu et al., 2023). However, limited by the capabilities of the retriever, retrieval-augmented systems inevitably introduce irrelevant or partially relevant knowledge to the models (Yin et al., 2023). Recent studies (Yu et al., 2023; Yoran et al., 2023; Chen et al., 2023) have increasingly focused on the impact of noisy information on retrieval-augmented generation. For example, Jia and Liang (2017); Creswell et al. (2022) observed that adding irrelevant noise to the context could detrimentally affect model performance. Chen et al. (2023) demonstrated that as the proportion of noise in the retrieval context increases, the performance of LLMs experiences a notable decline. Similar phenomena have been reported by  Yoran et al. (2023) and  Thakur et al. (2023).    \nAdversarial Training \n \nAdversarial training is recognized as a crucial method for enhancing model robustness, initially proposed by Goodfellow et al. (2014). Early studies widely investigated adversarial training in the computer vision domain (Kurakin et al., 2016; Madry et al., 2017). In the NLP domain,  Miyato et al. (2016) applied perturbations to word embeddings, making the model less prone to overfitting. Similarly, perturbations on different granularities have been extensively studied, encompassing various aspects of NLP tasks (Yasunaga et al., 2017; Wu et al., 2017; Zhu et al., 2019; Wang et al., 2020; Ni et al., 2023; Liang et al., 2023).  \n \nRecently, several studies have concentrated on generating adversarial examples designed to induce LLMs to generate harmful or non-factual content (Zou et al., 2023; Shen et al., 2023) instead of merely causing the model to make inaccurate predictions. Shen et al. (2023) employed decision-based perturbation at different levels to craft adversarial examples, revealing vulnerabilities in ChatGPT to both sentence-level and character-level adversarial attacks. Shi et al. (2023) added irrelevant context to an arithmetic reasoning dataset, finding that including irrelevant information distracted the model\u2019s predictions. Zou et al. (2023) proposed a method that could reliably generate adversarial attack suffixes, yielding adversarial prompts that exhibit high transferability.  \n \nIn this study, we investigate adversarial training concerning LLMs in response to various retrieval noises, aiming to efficiently obtain adversarial examples that enhance model robustness while reducing training overhead. We construct noisy adversarial examples by sampling or paraphrasing the original dataset. This approach ensures more dependable and precise outputs even when confronted with imperfect retrieved contexts.     \n 3 Methodology  \n 3.1 Problem Setup \n \nIn the standard RALM, given input query x\ud835\udc65xitalic_x, a retriever r\ud835\udc5fritalic_r is designed to retrieve relevant contexts C={c1,c2,\u2026}\ud835\udc36subscript\ud835\udc501subscript\ud835\udc502\u2026C=\\{c_{1},c_{2},\\ldots\\}italic_C = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 } from an external database. During inference, the content of retrieval context is concatenated with x\ud835\udc65xitalic_x to form d\ud835\udc51ditalic_d, which is then fed into the pre-trained language model M\ud835\udc40Mitalic_M, yielding a response y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG regarding x\ud835\udc65xitalic_x. If the retrieved context c\ud835\udc50citalic_c contains the correct answer y\ud835\udc66yitalic_y about x\ud835\udc65xitalic_x, we can denote c\ud835\udc50citalic_c as cg\u2062o\u2062l\u2062d\u2062e\u2062nsubscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bc_{golden}italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT, representing the golden retrieval context. However, if c\ud835\udc50citalic_c does not contain the correct answer y\ud835\udc66yitalic_y or contains partially irrelevant content, we can denote c\ud835\udc50citalic_c as cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT.  \n \nIn our study, we transform open-domain question answering (QA) into a reading comprehension task to meet the RAG settings. Formally, given the objective f\ud835\udc53fitalic_f of an open domain question answering task is f:{x}\u2192y:\ud835\udc53\u2192\ud835\udc65\ud835\udc66f:\\left\\{x\\right\\}\\rightarrow yitalic_f : { italic_x } \u2192 italic_y, we can formulate the objective of the reading comprehension task as f:{cg\u2062o\u2062l\u2062d\u2062e\u2062n,x}\u2192y:\ud835\udc53\u2192subscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc65\ud835\udc66f:\\left\\{c_{golden},x\\right\\}\\rightarrow yitalic_f : { italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT , italic_x } \u2192 italic_y. In examining the challenge of the retrieval noise robustness problem of RALM, we aim to obtain a fine-tuned model M\u2032superscript\ud835\udc40\u2032M^{\\prime}italic_M start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT that can not only fulfill the function f:{cg\u2062o\u2062l\u2062d\u2062e\u2062n,x}\u2192y:\ud835\udc53\u2192subscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc65\ud835\udc66f:\\left\\{c_{golden},x\\right\\}\\rightarrow yitalic_f : { italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT , italic_x } \u2192 italic_y but also produce accurate answers even in the presence of additional retrieval noise cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT and achieve function f:{cg\u2062o\u2062l\u2062d\u2062e\u2062n,cn\u2062o\u2062i\u2062s\u2062y,x}\u2192y:\ud835\udc53\u2192subscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bsubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66\ud835\udc65\ud835\udc66f:\\left\\{c_{golden},c_{noisy},x\\right\\}\\rightarrow yitalic_f : { italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT , italic_x } \u2192 italic_y.  \n \nFigure 2: Exact match (EM) scores of various models under different types of retrieval noises. \u201cGolden Context\u201d denotes instances where LLMs respond to questions with reference to the golden retrieval context. \u201cNo Noise\u201d indicates instances where LLMs answer questions without any retrieval. The experimental configurations of other models involve the introduction of different types of noises on the foundation of the \u201cGolden Context\u201d.    \n 3.2 Diverse Retrieval Noises \n \nWe systematically classify the retrieval noise present in cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT to closely mimic real-world conditions. Existing studies (Yoran et al., 2023; Yu et al., 2023) on retrieval noise robustness often dichotomize noise into relevant and irrelevant categories. However, we contend that such a classification may not fully align with the retrieval noise robustness of RALMs. In this work, we propose a more nuanced classification of retrieval noise, differentiating it into three distinct types: Relevant retrieval noise, Irrelevant retrieval noise, and Counterfactual retrieval noise. Specifically, Relevant retrieval noise (denoted as crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT) pertains to contexts that exhibit superficial relevance to the query x\ud835\udc65xitalic_x but lack the information necessary for the correct answer y\ud835\udc66yitalic_y. These contexts may appear relevant at first glance but ultimately mislead the model. Irrelevant retrieval noise (denoted as cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) encompasses contexts with low relevance to the query x\ud835\udc65xitalic_x, often arising from erroneous retrievals and generally being off-topic. Counterfactual retrieval noise (denoted as ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT) encompasses contexts that are topically related to x\ud835\udc65xitalic_x but contain incorrect and misleading information, often attributed to inaccuracies in the retriever\u2019s database.  \n \nTo examine the influence of three distinct types of retrieval noise on LLMs, we establish a benchmark for assessing retrieval noise robustness in LLMs by employing three open-domain question-answering datasets: Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and WebQ (Berant et al., 2013). Leveraging this benchmark, we evaluated the susceptibility of various open-source large language models to the effects of the three identified types of noise. The details of the construction of this benchmark can be found in Section 4.1. Leveraging this benchmark, we evaluate the sensitivity of various LLMs to the effects of the three types of noise. Specifically, we conduct experiments on six LLMs, including ChatGPT3.5, LLaMA27B (Touvron et al., 2023), LLaMA213B (Touvron et al., 2023), LLaMA270B (Touvron et al., 2023), Qwen7B (Bai et al., 2023), and Qwen14B (Bai et al., 2023). For each model, our experiments encompass two distinct settings: one with the exclusive presence of the golden retrieval context cg\u2062o\u2062l\u2062d\u2062e\u2062nsubscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bc_{golden}italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT and another incorporating the introduction of three different types of retrieval noise cn\u2062o\u2062i\u2062s\u2062ysubscript\ud835\udc50\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc66c_{noisy}italic_c start_POSTSUBSCRIPT italic_n italic_o italic_i italic_s italic_y end_POSTSUBSCRIPT. As shown in Figure 2, all LLMs experience varying degrees of impact from the three types of noise. The performance of LLMs exhibits a decline ranging from 0.2% to 13.43%. Through a comparative analysis of the effects of the three types of noise, we observe that irrelevant retrieval noise has a comparatively minor impact on LLMs with substantial capabilities.  \n \nFigure 3: The overview of our proposed RAAT method, which incorporates three distinct types of retrieval noises and the golden retrieval context during the training process.    \n 3.3 Retrieval-augmented Adaptive Adversarial Training \n \nRecently, several studies (Yoran et al., 2023; Li et al., 2022) attempted to enhance the noise robustness of LLMs through noisy training, which involves incorporating retrieved noisy contexts into fine-tuning data. The essence of noisy training involves the exploration of offline data augmentation, while in contrast, adversarial training leverages online data augmentation for a similar purpose (Ivgi and Berant, 2021). The core idea of adversarial training is to fortify the models against adversarial conditions by introducing adversarial perturbations (Jain et al., 2023). In the construction of adversarial samples, also known as noise samples, the min-max optimization strategy assumes a pivotal role, encompassing two fundamental steps. Initially, the maximization process involves adjusting the input data to intentionally mislead the model, inducing the maximum prediction error. Then, the minimization process entails fine-tuning the model\u2019s parameters to enhance its resistance against these meticulously crafted input perturbations (Bai et al., 2021). This strategy seeks to strike a balance, allowing the model to accurately identify normal data while robustly defending against potential attacks from adversarial examples.  \n \nIn this study, we aim to refine the objective of adversarial training while exploring the noise robustness challenges of RALMs. Considering a given query x\ud835\udc65xitalic_x, we assume the existence of four types of data augmentation, namely, golden retrieval context only (d\u2062ag\ud835\udc51subscript\ud835\udc4e\ud835\udc54da_{g}italic_d italic_a start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT), additional relevant retrieval noise (d\u2062ar\ud835\udc51subscript\ud835\udc4e\ud835\udc5fda_{r}italic_d italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT), additional irrelevant retrieval noise (d\u2062ai\ud835\udc51subscript\ud835\udc4e\ud835\udc56da_{i}italic_d italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), and additional counterfactual retrieval noise (d\u2062ac\ud835\udc51subscript\ud835\udc4e\ud835\udc50da_{c}italic_d italic_a start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT). The space of data augmentation is denoted as D\u2062A={d\u2062ag,d\u2062ar,d\u2062ai,d\u2062ac}\ud835\udc37\ud835\udc34\ud835\udc51subscript\ud835\udc4e\ud835\udc54\ud835\udc51subscript\ud835\udc4e\ud835\udc5f\ud835\udc51subscript\ud835\udc4e\ud835\udc56\ud835\udc51subscript\ud835\udc4e\ud835\udc50{DA}=\\{{da}_{g},{da}_{r},{da}_{i},{da}_{c}\\}italic_D italic_A = { italic_d italic_a start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , italic_d italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_d italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d italic_a start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT }. Then, the optimization problem can be formulated as follows:    min\u03b8\u2061\ud835\udd3c(x,y)\u223c\ud835\udc9f\u2062[maxd\u2062a\u2208D\u2062A\u2061\u2112\u2062(\u03b8,d\u2062a\u2062(x),y)]subscript\ud835\udf03subscript\ud835\udd3csimilar-to\ud835\udc65\ud835\udc66\ud835\udc9fdelimited-[]subscript\ud835\udc51\ud835\udc4e\ud835\udc37\ud835\udc34\u2112\ud835\udf03\ud835\udc51\ud835\udc4e\ud835\udc65\ud835\udc66\\min_{\\theta}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\left[\\max_{da\\in DA}\\mathcal{L}% (\\theta,da(x),y)\\right]roman_min start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_x , italic_y ) \u223c caligraphic_D end_POSTSUBSCRIPT [ roman_max start_POSTSUBSCRIPT italic_d italic_a \u2208 italic_D italic_A end_POSTSUBSCRIPT caligraphic_L ( italic_\u03b8 , italic_d italic_a ( italic_x ) , italic_y ) ]  (1)   \nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D denotes training data, \u2112\u2112\\mathcal{L}caligraphic_L is the loss function, \u03b8\ud835\udf03\\thetaitalic_\u03b8 denotes the parameters of LLMs, and d\u2062a\u2062(x)\ud835\udc51\ud835\udc4e\ud835\udc65da(x)italic_d italic_a ( italic_x ) represents the data augmentation of x\ud835\udc65xitalic_x.  \n \nBuilding upon the optimization problem outlined above, we introduce adaptive adversarial training as a tailored approach to enhance the robustness of RALMs against retrieval noise. Within adaptive adversarial training, the model refrains from updating parameters across all adversarial samples. Instead, it initiates the process by computing the generation loss for each adversarial sample, quantifying its adaptability to varying noise environments. Notably, a higher generation loss implies reduced adaptability of the model to the noisy environment. Given that each query involves one sample with a golden retrieval context and three adversarial samples, the model generates four distinct generation losses in each iteration. Following a min-max optimization strategy, the model prioritizes the selection of the largest loss to guide subsequent parameter update. Formally, we define the generation loss function \u2112\u2032superscript\u2112\u2032\\mathcal{L}^{\\prime}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT for the augmented input x\u2032superscript\ud835\udc65\u2032x^{\\prime}italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT as:    \u2112\u2032\u2062(\u03b8,x\u2032,y)=\u22121|y|\u2062\u2211t=1|y|log\u2061P\u03b8\u2062(yt\u2223x\u2032,y<t)superscript\u2112\u2032\ud835\udf03superscript\ud835\udc65\u2032\ud835\udc661\ud835\udc66superscriptsubscript\ud835\udc611\ud835\udc66subscript\ud835\udc43\ud835\udf03conditionalsubscript\ud835\udc66\ud835\udc61superscript\ud835\udc65\u2032subscript\ud835\udc66absent\ud835\udc61\\mathcal{L}^{\\prime}\\left(\\theta,x^{\\prime},y\\right)=-\\frac{1}{\\left|y\\right|}% \\sum_{t=1}^{\\left|y\\right|}\\log P_{\\theta}\\left(y_{t}\\mid x^{\\prime},y_{<t}\\right)caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_\u03b8 , italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_y ) = - divide start_ARG 1 end_ARG start_ARG | italic_y | end_ARG \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_y | end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2223 italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT )  (2)   \nwhere x\u2032=d\u2062a\u2062(x)superscript\ud835\udc65\u2032\ud835\udc51\ud835\udc4e\ud835\udc65x^{\\prime}=da(x)italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_d italic_a ( italic_x ) represents the augmented noise context of x\ud835\udc65xitalic_x.  \n \nTo effectively enhance performance across diverse noise environments, adaptive adversarial training incorporates a regularization term into its loss function. This incorporation of a regularization term is designed to mitigate the risk of the model overfitting to a particular type of noise. The regularization term acts as a stabilizing factor, promoting generalization and preventing the model from becoming overly specialized in its response to a specific noise profile. To achieve this goal, we introduce a regularization term specifically designed to mitigate the variance between these generation losses. This regularization term operates by identifying the largest \u2112m\u2062a\u2062x\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc4e\ud835\udc65\\mathcal{L}^{\\prime}_{max}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT and the smallest \u2112m\u2062i\u2062n\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc56\ud835\udc5b\\mathcal{L}^{\\prime}_{min}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT of the four generation losses at each training step. \u2112m\u2062a\u2062x\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc4e\ud835\udc65\\mathcal{L}^{\\prime}_{max}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT is the generation loss with the highest numerical value among four losses being considered. Conversely, \u2112m\u2062i\u2062n\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc56\ud835\udc5b\\mathcal{L}^{\\prime}_{min}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT is the loss function with the lowest numerical value. Here, an increased loss value indicates a greater magnitude of error or disparity in the aspect of the model\u2019s performance being assessed. This suggests that the model exhibits heightened sensitivity to adversarial examples reflecting retrieval noise. These adversarial examples are designed to probe and exploit weaknesses in the model\u2019s processing capabilities, especially in how it deals with noisy information in its input data. The regularization term, calculated as the square of the difference between \u2112m\u2062a\u2062x\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc4e\ud835\udc65\\mathcal{L}^{\\prime}_{max}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT and \u2112m\u2062i\u2062n\u2032subscriptsuperscript\u2112\u2032\ud835\udc5a\ud835\udc56\ud835\udc5b\\mathcal{L}^{\\prime}_{min}caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT, aims to reduce the model\u2019s sensitivity to retrieval noise by encouraging a more balanced optimization. Formally, we design the regularization term \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT as:    \u2112reg=\u2016\u2112max\u2032\u2212\u2112min\u2032\u201622subscript\u2112regsuperscriptsubscriptnormsubscriptsuperscript\u2112\u2032maxsubscriptsuperscript\u2112\u2032min22\\mathcal{L}_{\\text{reg}}=\\|\\mathcal{L}^{\\prime}_{\\text{max}}-\\mathcal{L}^{% \\prime}_{\\text{min}}\\|_{2}^{2}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT = \u2225 caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT min end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  (3)    \n \nSubsequently, we define the adaptive adversarial training loss function \u2112adasubscript\u2112ada\\mathcal{L}_{\\text{ada}}caligraphic_L start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT as follows:    \u2112ada=\u2112max\u2032+wreg\u22c5\u2112regsubscript\u2112adasubscriptsuperscript\u2112\u2032max\u22c5subscript\ud835\udc64regsubscript\u2112reg\\mathcal{L}_{\\text{ada}}=\\mathcal{L}^{\\prime}_{\\text{max}}+w_{\\text{\\text{reg}% }}\\cdot\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT = caligraphic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT max end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT \u22c5 caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT  (4)   \nwhere wregsubscript\ud835\udc64regw_{\\text{reg}}italic_w start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT is a pre-defined hyperparameter to control the weight of \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT.    \n 3.4 Incorporating Noise Awareness \n \nAccurately identifying retrieval noise plays a pivotal role in fortifying the robustness of RALMs against the retrieval noise. Models endowed with the ability to discern different types of noise can more effectively choose and utilize training data, leading to an improvement in the overall quality of their generated outputs. This capacity to distinguish between various noise types contributes significantly to the model\u2019s adaptive learning process, enabling it to optimize performance in the presence of diverse noise scenarios. Inspired by the above motivation, we propose an auxiliary task designed to autonomously recognize the types of noisy retrieval texts, aiming to significantly bolster the retrieval robustness of RALMs. This auxiliary task serves as a valuable augmentation, contributing to the overall adaptability and effectiveness of the model in scenarios involving retrieval noise.   \n \nSpecifically, we attempt to enable the model to generate tokens that are sensitive to noise, thereby improving the model\u2019s capacity to discern various types of retrieval noise internally. Specifically, we first incorporate a linear layer beneath LLMs. Subsequently, a classification loss \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT is computed for each of the golden retrieval context and the three adversarial samples corresponding to each input x\ud835\udc65xitalic_x. One-hot encoding is employed in classification tasks, assigning values from 1 to 4 as labels to train the classifiers, where each classifier is tailored to a different retrieval noise type. The loss function \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT is computed using cross-entropy.  \n \nFinally, we formulate the final RAAT loss \u2112RAATsubscript\u2112RAAT\\mathcal{L}_{\\text{RAAT}}caligraphic_L start_POSTSUBSCRIPT RAAT end_POSTSUBSCRIPT by combining the adaptive adversarial training loss and the classification loss in the context of multi-task learning:    \u2112RAAT=wada\u22c5\u2112ada+wcls\u22c5\u2112clssubscript\u2112RAAT\u22c5subscript\ud835\udc64adasubscript\u2112ada\u22c5subscript\ud835\udc64clssubscript\u2112cls\\mathcal{L}_{\\text{RAAT}}=w_{\\text{ada}}\\cdot\\mathcal{L}_{\\text{ada}}+w_{\\text% {cls}}\\cdot\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT RAAT end_POSTSUBSCRIPT = italic_w start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT \u22c5 caligraphic_L start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT \u22c5 caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT  (5)   \nwhere wadasubscript\ud835\udc64adaw_{\\text{ada}}italic_w start_POSTSUBSCRIPT ada end_POSTSUBSCRIPT and wclssubscript\ud835\udc64clsw_{\\text{cls}}italic_w start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT represent pre-defined hyperparameters used to balance the importance of these two different tasks.     \n 4 Experiments  \n 4.1 Dataset Construction \n \nWe have formulated a benchmark named RAG-Bench that is specifically designed to evaluate the retrieval noise robustness of LLMs. RAG-Bench is established upon three widely available datasets that center around open-domain question answering (QA): Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and WebQ (Berant et al., 2013). For each dataset, we employ the retrieval model DPR (Karpukhin et al., 2020) as our retriever, which retrieves ten passages from Wikipedia for each query. Then, we apply filtering to the queries, ensuring that each query in the filtered subset contains at least two golden retrieval contexts, indicating the presence of correct answers. Detailed statistics for both the full set and the filtered subset can be found in Table 1.  \n \nEach sample in our dataset contains a golden retrieval context and is deliberately designed to incorporate three types of augmented retrieval noise. To introduce relevant retrieval noise, we choose the context most pertinent to the query from the set of ten retrieval texts, excluding the golden retrieval context. In the case of irrelevant retrieval noise, no selection is made from the retrieval texts associated with the current query. Instead, a passage is randomly chosen from the retrieval contents of other queries, ensuring its complete irrelevance to the current query. For the counterfactual retrieval noise, we randomly select one passage from the two golden retrieval contexts and substitute its answer entity with an incorrect one.  \n \nThe test set of RAG-Bench comprises 1000 randomly chosen samples from the test sets of three QA datasets, resulting in a total of 3000 samples. The training set consists of 1500 samples randomly selected from the training sets of the three datasets, totaling 4500 samples. The validation set, drawn from the training sets of three QA datasets, contains 300 samples. Notably, careful measures were taken to ensure no overlap with the training data of RAG-Bench.  \n \n   Datasets Train Test   #Full #Subset #Full #Subset   NQ 79,168 40,551 3,610 1,833   TriviaQA 78,785 51,202 11,313 7,010   WebQ 3,778 2,316 2,032 1,057    \nTable 1: The statistics of the three QA datasets.  \n \n   Method Golden Only Golden & cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT Golden & crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Golden & ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT Avg   F1 EM F1 EM F1 EM F1 EM F1 EM   LLaMA27B 65.56 51.80 56.14 42.87 53.10 39.73 51.81 38.37 56.68 43.19   Qwen7B 62.57 47.07 61.48 46.06 55.50 40.50 53.26 36.90 58.20 42.63   LLaMA213B 69.27 55.00 63.25 49.47 62.27 47.97 62.07 47.17 64.22 49.90   Qwen14B 67.45 51.43 66.71 51.20 61.88 46.16 58.65 41.30 63.67 47.52   LLaMA270B 71.43 56.56 70.05 55.13 65.97 51.33 63.91 48.27 67.84 52.82   ChatGPT3.5 73.98 60.50 72.24 60.30 70.65 56.89 69.00 54.64 71.47 58.10   RALMgolden 80.31 74.03 79.33 72.73 73.26 66.33 73.08 65.40 76.50 69.62   RetRobust 80.10 73.80 79.25 72.97 74.81 68.30 75.46 68.43 77.41 70.88   RALMretrieved 80.04 73.40 81.09 74.80 75.99 69.10 73.10 65.67 77.55 70.74   RALMmultiple 85.47 80.17 85.27 81.20 83.07 78.33 83.25 79.23 84.27 79.73   RAAT 87.15 83.07 86.80 82.73 85.14 81.00 86.29 82.10 86.35 82.23    \nTable 2: Experimental results on our RAG-Bench benchmark. \u201cGolden Only\u201d denotes a scenario where LLMs only consult the golden retrieval context. In \u201cGolden & cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT/crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT/ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT\u201d, LLMs consider both the golden retrieval context and irrelevant retrieval noise/relevant retrieval noise/counterfactual retrieval noise.    \n 4.2 Evaluation Metrics \n \nWe evaluate the effectiveness of our method using two metrics: exact match (EM) and F1 score (Chen et al., 2017). Concretely, EM assesses the extent to which the answer generated by the system aligns precisely with the standard answer without any disparities at the character level. In contrast, the F1 score incorporates precision and recall, accounting for the equilibrium between correctly identifying answers and avoiding omitting correct answers.    \n 4.3 Baseline Methods \n \nWe conduct a comparison of our RAAT method against zero-shot LLMs, as well as finetuning approaches applied to LLaMA27B, which shares a common backbone with RAAT.   \n Zero-Shot Methods \n \nWithin the open-source community, many foundational and supervised fine-tuning (SFT) models have emerged. In our experiments, we select six renowned LLMs as baselines: ChatGPT3.5, LLaMA27B (Touvron et al., 2023), LLaMA213B, LLaMA270B, Qwen7B (Bai et al., 2023), and Qwen14B.    \nFine-tuning Methods \n \nWe further compare RAAT with various fine-tuning methods. \n \n \u2022 \n \nRALMgolden This is a RALM with instruction tuning (Lin et al., 2023). It prepends a golden retrieval text cg\u2062o\u2062l\u2062d\u2062e\u2062nsubscript\ud835\udc50\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5bc_{golden}italic_c start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT in context to fine-tune LLaMA27B.   \n \u2022 \n \nRetRobust To ensure that the model is exposed to both golden retrieval texts and various retrieval noise during training,  Yoran et al. (2023) proposes RetRobust. For each query, RetRobust selects top-1, low-ranked, or random retrieved passages with equal probability for training.   \n \u2022 \n \nRALMretrieved This variant is a RALM incorporating instruction tuning. In contrast to RALMgolden, RALMretrieved does not manually design retrieval noise in the training set but directly uses the top-2 retrieved passages. This training method is more aligned with real retrieval environments.   \n \u2022 \n \nRALMmultiple This approach closely resembles RetRobust, differing only in the construction of the training dataset. In RALMmultiple, rather than introducing one type of retrieval noise randomly for each query, each type of retrieval noise is combined with the sample and incorporated into the dataset. That is, each query is associated with four augmented noisy samples.     \n \n   Method Golden Only Golden & cisubscript\ud835\udc50\ud835\udc56c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT Golden & crsubscript\ud835\udc50\ud835\udc5fc_{r}italic_c start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Golden & ccsubscript\ud835\udc50\ud835\udc50c_{c}italic_c start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT Avg   F1 EM F1 EM F1 EM F1 EM F1 EM   RAAT 87.15 83.07 86.80 82.73 85.14 81.00 86.29 82.10 86.35 82.23   RAAT w/o \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT 86.76 82.77 86.45 82.27 84.69 80.63 85.54 81.20 85.86 81.71   RAAT w/o \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT 86.87 83.03 83.92 79.86 84.69 80.57 87.02 82.80 85.63 81.57    \nTable 3: Ablation test results in terms of EM and F1 score.     \n 4.4 Implementation Details \n \nOur RAAT method relies on LLaMA2-7B as the foundational model. We set the weight parameters as follows: wregsubscript\ud835\udc64regw_{\\text{reg}}italic_w start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT to 0.1, wraatsubscript\ud835\udc64raatw_{\\text{raat}}italic_w start_POSTSUBSCRIPT raat end_POSTSUBSCRIPT to 2, and wclssubscript\ud835\udc64clsw_{\\text{cls}}italic_w start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT to 1. The sequence length, epoch, and learning rate are configured to 512, 2, and 5e-6, respectively. Our experiments are conducted on a computational cluster equipped with 4 NVIDIA A100 GPUs, each boasting a capacity of 80GB.     \n 5 Experimental Results   \n 5.1 Main Results \n \nTable 2 illustrates the efficacy of our RAAT method compared to the baselines in terms of F1 and EM scores. We observe that all models are affected by three different types of retrieval noise attacks. The influence of irrelevant retrieval noise is marginal, while counterfactual retrieval noise exerts the most significant impact. For the models sharing the same architecture, larger parameter sizes correlate with superior performance and better robustness against retrieval noise. For instance, LLaMA27B exhibits a 12.46% reduction in F1 score when confronted with relevant retrieval noise, whereas LLaMA213B only experiences a 7% decrease under identical conditions. This trend is also evident in Qwen.  \n \nFrom Table 2, we can also observe that fine-tuning enables LLMs to better utilize information from the retrieval texts. Fine-tuned models significantly outperform the zero-shot LLMs with varying parameter sizes. Moreover, RALMmultiple shows a significant improvement over RALMgolden, RALMretrieved and RetRobust, reflecting the sensitivity of retrieval noise to the training dataset and the importance of diversity in noise attacks during training. Our RAAT method achieves even better performance than RALMmultiple in all four environments, with an average increase of 2.08% in the F1 score and 2.5% in the EM score, demonstrating its superior ability to handle diverse retrieval noise.    \n 5.2 Ablation Study \n \nTo gain a comprehensive understanding of the individual contribution of each component within RAAT to the overall performance, we conducted an ablation study by removing the regularization term loss (denoted as w/o \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT) and the noise-aware classification loss (denoted as w/o \u2112clssubscript\u2112cls\\mathcal{L}_{\\text{cls}}caligraphic_L start_POSTSUBSCRIPT cls end_POSTSUBSCRIPT). The experimental results are shown in Table 3. After removing the classification loss, we observe that the average performance of the model decreased by 0.49% and 0.52% in terms of F1 score and EM score, respectively. While removing the regularization term, there was a significant performance decrease in handling irrelevant retrieval noise.  \n \nFigure 4: The number of queries and parameter updates are 4,500 and 9,000, respectively. The statistical content in this table pertains to different types of retrieval noises selected by RAAT each time the model parameters undergo an update.     \n 5.3 Further Discussion  \nWhat types of adversarial samples does RAAT employ during training? \n \nTo gain a comprehensive understanding of the underlying mechanisms of RAAT, particularly its utilization of specific retrieved data to augment model robustness, we undertook an in-depth examination of its training process, involving meticulously tracking the training iterations and conducting a thorough statistical analysis to quantify the number of different types of adversarial examples incorporated during the training phase. The statistical results are illustrated in Figure 4. We observe that RAAT prioritizes the selection of adversarial examples that can significantly improve model robustness, as reflected in its tendency to choose certain types of adversarial examples. This is consistent with our empirical findings described in Section 3.2. RAAT tends to select adversarial examples associated with relevant retrieval noise and counterfactual retrieval noise for training.      \n 6 Conclusion \n \nThis work initially investigated retrieval noises in RALMs and categorized them into three distinct types, reflecting real-world environments. In addition, we introduced RAAT as a solution to address the noise robustness challenges faced by RALMs, which leveraged adaptive adversarial learning and multi-task learning to enhance the model\u2019s capability. Moreover, we established a benchmark to verify the effectiveness of RAAT based on three open-domain QA datasets. Experimental results demonstrate substantial improvements in F1 and EM scores for the LLaMA2 7B model fine-tuned with RAAT across diverse noise conditions.    \n 7 Limitations \n \nIn this section, we delve into the limitations inherent in our work, with the objective of pinpointing areas for refinement and bolstering the performance of our model in future endeavors. Two principal limitations have been identified. Firstly, the benchmark constructed for our experiments relies exclusively on datasets sourced from three open-domain question answering repositories. Going forward, we intend to compile additional high-quality datasets from varying NLP tasks and endeavor to retrieve texts from a more extensive array of knowledge bases. This strategic expansion aims to facilitate the creation of a more diversified and expansive benchmark tailored for evaluating the retrieval noise robustness of large language models. Secondly, within the framework of RAAT, our efforts have been singularly concentrated on fortifying the retrieval noise robustness at the LLM end. However, the prospect of jointly training large language models and retrieval models emerges as a promising avenue for enhancing the overall robustness of RALMs. Although this dimension was not the primary focal point of our current work, in our subsequent investigations into retrieval noise robustness, we plan to delve into this avenue. This approach would facilitate the synchronized progress of both the large language model and the retrieval model, contributing to an overall improvement in their robustness.    \nAcknowledgments \n \nMin Yang was supported by National Key Research and Development Program of China (2022YFF0902100), National Natural Science Foundation of China (62376262), the Natural Science Foundation of Guangdong Province of China (2024A1515030166), Shenzhen Science and Technology Innovation Program (KQTD20190929172835662), Shenzhen Basic Research Foundation (JCYJ20210324115614039). This work was supported by Alibaba Group through Alibaba Innovative Research Program, Postdoctoral Fellowship Program of CPSF (GZC20232873), GuangDong Basic and Applied Basic Research Foundation (2023A1515110718 and 2024A1515012003).    \nReferences \n \n Bai et al. (2023)  Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023.  Qwen technical report.  arXiv preprint arXiv:2309.16609.   \n Bai et al. (2021)  Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021.  Recent advances in adversarial training for adversarial robustness.  arXiv preprint arXiv:2102.01356.   \n Berant et al. (2013)  Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.  Semantic parsing on freebase from question-answer pairs.  In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533\u20131544.   \n Brown et al. (2020)  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.  Language models are few-shot learners.  Advances in neural information processing systems, 33:1877\u20131901.   \n Bubeck et al. (2023)  S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.  Sparks of artificial general intelligence: Early experiments with gpt-4.  arXiv preprint arXiv:2303.12712.   \n Chen et al. (2017)  Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.  Reading wikipedia to answer open-domain questions.  arXiv preprint arXiv:1704.00051.   \n Chen et al. (2023)  Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023.  Benchmarking large language models in retrieval-augmented generation.  arXiv preprint arXiv:2309.01431.   \n Creswell et al. (2022)  Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022.  Selection-inference: Exploiting large language models for interpretable logical reasoning.  arXiv preprint arXiv:2205.09712.   \n Gao et al. (2023)  Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.  Retrieval-augmented generation for large language models: A survey.  arXiv preprint arXiv:2312.10997.   \n Goodfellow et al. (2014)  Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014.  Explaining and harnessing adversarial examples.  arXiv preprint arXiv:1412.6572.   \n Gueta et al. (2023)  Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. 2023.  Knowledge is a region in weight space for fine-tuned language models.  arXiv preprint arXiv:2302.04863.   \n Ivgi and Berant (2021)  Maor Ivgi and Jonathan Berant. 2021.  Achieving model robustness through discrete adversarial training.  arXiv preprint arXiv:2104.05062.   \n Jain et al. (2023)  Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. 2023.  Neftune: Noisy embeddings improve instruction finetuning.  arXiv preprint arXiv:2310.05914.   \n Jia and Liang (2017)  Robin Jia and Percy Liang. 2017.  Adversarial examples for evaluating reading comprehension systems.  arXiv preprint arXiv:1707.07328.   \n Joshi et al. (2017)  Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017.  Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.  arXiv preprint arXiv:1705.03551.   \n Kandpal et al. (2023)  Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.  Large language models struggle to learn long-tail knowledge.  In International Conference on Machine Learning, pages 15696\u201315707. PMLR.   \n Karpukhin et al. (2020)  Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.  Dense passage retrieval for open-domain question answering.  arXiv preprint arXiv:2004.04906.   \n Kurakin et al. (2016)  Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016.  Adversarial machine learning at scale.  arXiv preprint arXiv:1611.01236.   \n Kwiatkowski et al. (2019)  Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.  Natural questions: a benchmark for question answering research.  Transactions of the Association for Computational Linguistics, 7:453\u2013466.   \n Lewis et al. (2020)  Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020.  Retrieval-augmented generation for knowledge-intensive nlp tasks.  Advances in Neural Information Processing Systems, 33:9459\u20139474.   \n Li et al. (2022)  Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2022.  Large language models with controllable working memory.  arXiv preprint arXiv:2211.05110.   \n Liang et al. (2023)  Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, and Xinwang Liu. 2023.  Knowledge graph contrastive learning based on relation-symmetrical structure.  IEEE Transactions on Knowledge and Data Engineering.   \n Lin et al. (2023)  Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.  Ra-dit: Retrieval-augmented dual instruction tuning.  arXiv preprint arXiv:2310.01352.   \n Madry et al. (2017)  Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017.  Towards deep learning models resistant to adversarial attacks.  arXiv preprint arXiv:1706.06083.   \n Mao et al. (2020)  Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020.  Generation-augmented retrieval for open-domain question answering.  arXiv preprint arXiv:2009.08553.   \n Miyato et al. (2016)  Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016.  Adversarial training methods for semi-supervised text classification.  arXiv preprint arXiv:1605.07725.   \n Morris et al. (2020)  John X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020.  Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp.  arXiv preprint arXiv:2005.05909.   \n Ni et al. (2023)  Shiwen Ni, Jiawen Li, Min Yang, and Hung-Yu Kao. 2023.  Dropattack: A random dropped weight attack adversarial training for natural language understanding.  IEEE/ACM Transactions on Audio, Speech, and Language Processing.   \n Ruder (2017)  Sebastian Ruder. 2017.  An overview of multi-task learning in deep neural networks.  arXiv preprint arXiv:1706.05098.   \n Shen et al. (2023)  Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023.  In chatgpt we trust? measuring and characterizing the reliability of chatgpt.  arXiv preprint arXiv:2304.08979.   \n Shi et al. (2023)  Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023.  Large language models can be easily distracted by irrelevant context.  In International Conference on Machine Learning, pages 31210\u201331227. PMLR.   \n Thakur et al. (2023)  Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, et al. 2023.  Nomiracl: Knowing when you don\u2019t know for robust multilingual retrieval-augmented generation.  arXiv preprint arXiv:2312.11361.   \n Touvron et al. (2023)  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.  Llama 2: Open foundation and fine-tuned chat models.  arXiv preprint arXiv:2307.09288.   \n Wang et al. (2020)  Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020.  Infobert: Improving robustness of language models from an information theoretic perspective.  arXiv preprint arXiv:2010.02329.   \n Wu et al. (2017)  Yi Wu, David Bamman, and Stuart Russell. 2017.  Adversarial training for relation extraction.  In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783.   \n Yasunaga et al. (2017)  Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017.  Robust multilingual part-of-speech tagging via adversarial training.  arXiv preprint arXiv:1711.04903.   \n Yin et al. (2023)  Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023.  Alcuna: large language models meet new knowledge.  arXiv preprint arXiv:2310.14820.   \n Yoran et al. (2023)  Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.  Making retrieval-augmented language models robust to irrelevant context.  arXiv preprint arXiv:2310.01558.   \n Yu et al. (2023)  Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.  Chain-of-note: Enhancing robustness in retrieval-augmented language models.  arXiv preprint arXiv:2311.09210.   \n Zhu et al. (2019)  Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2019.  Freelb: Enhanced adversarial training for natural language understanding.  arXiv preprint arXiv:1909.11764.   \n Zhu et al. (2023)  Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023.  Large language models for information retrieval: A survey.  arXiv preprint arXiv:2308.07107.   \n Zou et al. (2023)  Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.  Universal and transferable adversarial attacks on aligned language models.  arXiv preprint arXiv:2307.15043.     \n \n \nFigure 5: The results of T-SNE visualization. Following the introduction of four types of adversarial samples (i.e., retrieval noises) into models tuned by various methods, the hidden state of the last token is extracted. Subsequently, dimensionality reduction using t-SNE, clustering, and visualization are performed. This visual representation includes three methods, namely RALMg\u2062o\u2062l\u2062d\u2062e\u2062nsubscriptRALM\ud835\udc54\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc52\ud835\udc5b\\text{RALM}_{golden}RALM start_POSTSUBSCRIPT italic_g italic_o italic_l italic_d italic_e italic_n end_POSTSUBSCRIPT, RetRobust, and RAAT.   \n Appendix A Has the Model Truly Attained Noise Awareness? \n \nOur preliminary investigation focused on the intrinsic capability of RALMgolden and RetRobust to classify the types of retrieval noises. Drawing inspiration from previous work Gueta et al. (2023), we approached this matter through the application of clustering algorithms. The results, illustrated in Figure  5, reveal suboptimal clustering of text vectors from RALMgolden and RetRobust, suggesting that the internal representations for noise classification in these models may lack clarity. Consequently, we introduced a noise classification loss \u2112c\u2062l\u2062ssubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\\mathcal{L}_{{cls}}caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT into our RAAT method. The experimental results demonstrated tangible benefits with the incorporation of the classification loss. Additionally, we assessed the clustering effectiveness in models fine-tuned with RAAT, observing minimal distances among samples of irrelevant, relevant, and no retrieval noises, in contrast to the considerable distance from counterfactual retrieval noise samples. In particular, counterfactual retrieval noise posed the most significant challenge to LLMs; however, after RAAT tuning, it exhibited superior clustering and representation learning outcomes, indirectly validating the efficacy of RAAT.     \n \nGenerated on Fri May 31 16:18:59 2024 by LaTeXML   ", "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "description": "<strong>Adversarial</strong> <strong>training</strong> <strong>for</strong> relation extraction. In Proceedings of the 2017 Conference on Empirical <strong>Methods</strong> in Natural <strong>Language</strong> Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. <strong>Robust</strong> <strong>multilingual</strong> <strong>part</strong>-<strong>of</strong>-<strong>speech</strong> <strong>tagging</strong> <strong>via</strong> <strong>adversarial</strong> ...", "usage": {"tokens": 15092}, "retrieved_text": ["2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Yin et al. (2023) Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. Alcuna: large language models meet new knowledge. arXiv preprint arXiv:2310.14820. Yoran et al. (2023) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558. Yu et al. (2023) Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210. Zhu et al. (2019)  Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2019. Freelb: Enhanced adversarial training for natural language understanding. arXiv preprint arXiv:1909.11764. Zhu et al. (2023)  Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107. Zou et al. (2023)  Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.", "(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wang et al. (2020) Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020. Infobert: Improving robustness of language models from an information theoretic perspective. arXiv preprint arXiv:2010.02329. Wu et al. (2017) Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Yin et al. (2023)  Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. Alcuna: large language models meet new knowledge. arXiv preprint arXiv:2310.14820. Yoran et al. (2023)  Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558. Yu et al. (2023)  Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210.", "the min-max paradigm of adversarial training (Morris et al., 2020; Ivgi and Berant, 2021). Moreover, RAAT utilizes multi-task learning (Ruder, 2017) to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance. The main contributions of this paper can be summarized as follows: \u2022 We systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises. \u2022 \n \nWe propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises. \u2022 \n \nWe set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments. 2 Related Work  \nRetrieval-Augmented Generation with Noisy Context \n \nRetrieval-Augmented Language Models (RALMs) have shown impressive performance in various NLP tasks (Gao et al., 2023; Zhu et al., 2023)."]}], "The paper titled \"Unsupervised Cross-lingual Representation Learning at Scale\" discusses an unsupervised approach to learning cross-lingual representations that can be used for tasks such as transfer learning and zero-shot learning.": [{"text": " \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.  \n \nI Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1911.02116 (cs)   \n \n \n [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] \nTitle:Unsupervised Cross-lingual Representation Learning at Scale \nAuthors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \nView a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF \n Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.   \n   Comments: ACL 2020 (+ updated results)   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1911.02116 [cs.CL]     (or  arXiv:1911.02116v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1911.02116\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Alexis Conneau [view email] \n [v1] Tue, 5 Nov 2019 22:42:00 UTC (569 KB)\n [v2] Wed, 8 Apr 2020 01:02:17 UTC (561 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2019-11 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 3 blog links (what is this?)   \n \nDBLP - CS Bibliography \n listing | bibtex  \n Alexis Conneau\nNaman Goyal\nVishrav Chaudhary\nGuillaume Wenzek\nFrancisco Guzm\u00e1n \n\u2026  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale", "description": "This <strong>paper</strong> shows that pretraining multilingual language models <strong>at</strong> <strong>scale</strong> leads to significant performance gains for a wide range of <strong>cross</strong>-<strong>lingual</strong> <strong>transfer</strong> <strong>tasks</strong>. We train a Transformer-based masked language model on one hundred languages, <strong>using</strong> more than two terabytes of filtered CommonCrawl data.", "usage": {"tokens": 1454}, "retrieved_text": [" \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1911.02116 (cs)   \n \n \n [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] \nTitle:Unsupervised Cross-lingual Representation Learning at Scale \nAuthors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \nView a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF \n Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1911.02116 (cs) [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] Title:Unsupervised Cross-lingual Representation Learning at Scale Authors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov View a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.", "Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\n\nAnthology ID:\n2020.acl-main.747\nVolume:\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nMonth:\nJuly\nYear:\n2020\nAddress:\nOnline\nEditors:\nDan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n8440\u20138451\nLanguage:\n\nURL:\nhttps://aclanthology.org/2020.acl-main.747/\nDOI:\n10.18653/v1/2020.acl-main.747\nBibkey:\n\nconneau-etal-2020-unsupervised\nCite (ACL):\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\n\nCite (Informal):\nUnsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/2020.acl-main.747.pdf\nVideo:\n http://slideslive.com/38928776\nCode\n facebookresearch/XLM +  additional community code\nData\nCC100, CoNLL 2002, GLUE, MLQA, QNLI, SQuAD, SST\nPDF Cite Search  Code  Video  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{conneau-etal-2020-unsupervised, title = \"Unsupervised Cross-lingual Representation Learning at Scale\", author = \"Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin\", editor = \"Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = jul, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.747/\", doi = \"10.18653/v1/2020.acl-main.747\", pages = \"8440--8451\", abstract = \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\\%} average accuracy on XNLI, +13{\\%} average F1 score on MLQA, and +2.4{\\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\\%} in XNLI accuracy for Swahili and 11.4{\\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"conneau-etal-2020-unsupervised\"> <titleInfo> <title>Unsupervised Cross-lingual Representation Learning at Scale</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Alexis</namePart> <namePart type=\"family\">Conneau</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Kartikay</namePart> <namePart type=\"family\">Khandelwal</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Naman</namePart> <namePart type=\"family\">Goyal</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Vishrav</namePart> <namePart type=\"family\">Chaudhary</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Guillaume</namePart> <namePart type=\"family\">Wenzek</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Francisco</namePart> <namePart type=\"family\">Guzm\u00e1n</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Edouard</namePart> <namePart type=\"family\">Grave</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Myle</namePart> <namePart type=\"family\">Ott</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Luke</namePart> <namePart type=\"family\">Zettlemoyer</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Veselin</namePart> <namePart type=\"family\">Stoyanov</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2020-07</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Dan</namePart> <namePart type=\"family\">Jurafsky</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Joyce</namePart> <namePart type=\"family\">Chai</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Natalie</namePart> <namePart type=\"family\">Schluter</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Joel</namePart> <namePart type=\"family\">Tetreault</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Online</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.</abstract> <identifier type=\"citekey\">conneau-etal-2020-unsupervised</identifier> <identifier type=\"doi\">10.18653/v1/2020.acl-main.747</identifier> <location> <url>https://aclanthology.org/2020.acl-main.747/</url> </location> <part> <date>2020-07</date> <extent unit=\"page\"> <start>8440</start> <end>8451</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Unsupervised Cross-lingual Representation Learning at Scale %A Conneau, Alexis %A Khandelwal, Kartikay %A Goyal, Naman %A Chaudhary, Vishrav %A Wenzek, Guillaume %A Guzm\u00e1n, Francisco %A Grave, Edouard %A Ott, Myle %A Zettlemoyer, Luke %A Stoyanov, Veselin %Y Jurafsky, Dan %Y Chai, Joyce %Y Schluter, Natalie %Y Tetreault, Joel %S Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics %D 2020 %8 July %I Association for Computational Linguistics %C Online %F conneau-etal-2020-unsupervised %X This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available. %R 10.18653/v1/2020.acl-main.747 %U https://aclanthology.org/2020.acl-main.747/ %U https://doi.org/10.18653/v1/2020.acl-main.747 %P 8440-8451 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Unsupervised Cross-lingual Representation Learning at Scale](https://aclanthology.org/2020.acl-main.747/) (Conneau et al., ACL 2020)\n\nUnsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020)\nACL\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Unsupervised Cross-lingual Representation Learning at Scale - ACL Anthology", "description": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.", "usage": {"tokens": 3716}, "retrieved_text": [" ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.", "ACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit AbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "(ACL): Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics. Cite (Informal): Unsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/2020.acl-main.747.pdf Video: http://slideslive.com/38928776 Code facebookresearch/XLM + additional community code Data CC100, CoNLL 2002, GLUE, MLQA, QNLI, SQuAD, SST PDF Cite Search Code Video Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{conneau-etal-2020-unsupervised, title = \"Unsupervised Cross-lingual Representation Learning at Scale\", author = \"Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin\", editor = \"Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = jul, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.747/\", doi = \"10.18653/v1/2020.acl-main.747\", pages = \"8440--8451\", abstract = \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data."]}, {"text": "Home\n\nChat with PDF\n\nLiterature Review\n\nAI Writer\n\nFind Topics\n\nParaphraser\n\nCitation Generator\n\nExtract Data\n\nAI Detector\n\nPDF to Video\n\nAffiliate Program\n\nChrome Extension\n\n\nUse on ChatGPT\n\n\nContact Us\n\n\n\n\nHome\nPapers\nUnsupervised Cross-lingual Representation Learning at Scale\n\n\n\nOpen AccessProceedings ArticleDOI\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University\n - 01 Jul 2020\n\n - pp 8440-8451\nShow Less\n\n\n\nPodcast\n3.7K\nPDF\n\nSave\nCite\n\n\n\n\nTL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time.\nread more\n\n\nAbstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\nread more\nShow Related Papers\n\n\n\n\nChat with Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitations\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nMAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\nJonas Pfeiffer,Ivan Vuli\u0107,Iryna Gurevych,Sebastian Ruder +3 moreTechnische Universit\u00e4t Darmstadt,University of Mannheim,Google\n - 30 Apr 2020\n\nShow Less\n\n\n\nTL;DR: This paper proposed MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations, and introduced a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language....read moreread less\n\n169\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Journal Article\u2022DOI\nPre-Trained Models: Past, Present and Future\nXu Han,Zhengyan Zhang,Ning Ding,Yuxian Gu,Xiao Liu,Yuqi Huo,Jiezhong Qiu,Liang Zhang,Wentao Han,Minlie Huang,Qin Jin,Yanyan Lan,Yang Liu,Zhiyuan Liu,Zhiwu Lu,Xipeng Qiu,Ruihua Song,Jie Tang,Ji-Rong Wen,Jinhui Yuan,Wayne Xin Zhao,Jun Zhu +21 moreTsinghua University,Renmin University of China,Fudan University\n - 14 Jun 2021\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum....read moreread less\n\n135\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nAdapterHub: A Framework for Adapting Transformers\nJonas Pfeiffer,Andreas R\u00fcckl\u00e9,Clifton Poth,Aishwarya Kamath,Ivan Vuli\u0107,Sebastian Ruder,Kyunghyun Cho,Iryna Gurevych +7 moreTechnische Universit\u00e4t Darmstadt,New York University,University of Oslo,Google,Canadian Institute for Advanced Research\n - 15 Jul 2020\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors propose a framework that allows dynamic \"stiching-in\" of pre-trained adapters for different tasks and languages, which enables extremely easy and quick adaptation of state-of-the-art pre-training models across tasks....read moreread less\n\n122\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nFlauBERT: Unsupervised Language Model Pre-training for French\nHang Le,Lo\u00efc Vial,Jibril Frej,Vincent Segonne,Maximin Coavoux,Benjamin Lecouteux,Alexandre Allauzen,Beno\u00eet Crabb\u00e9,Laurent Besacier,Didier Schwab +9 moreUniversity of Grenoble,Centre national de la recherche scientifique\n - 11 Dec 2019\n\nShow Less\n\n\n\nTL;DR: The authors proposed FlauBERT, a model learned on a very large and heterogeneous French corpus and applied it to various NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and showed that most of the time they outperformed other pre-training approaches....read moreread less\n\n120\nPodcast\n\n\n\n\n\n\n\n\n\u2022Posted Content\nmT5: A massively multilingual pre-trained text-to-text transformer\nLinting Xue,Noah Constant,Adam Roberts,Mihir Kale,Rami Al-Rfou,Aditya Siddhant,Aditya Barua,Colin Raffel +7 moreGoogle,University of Chester\n - 22 Oct 2020\n - arXiv: Computation and Language \nShow Less\n\n\n\nTL;DR: This article proposed a multilingual variant of T5, mT5, which was pre-trained on a new Common Crawl-based dataset covering 101 languages and achieved state-of-the-art performance on many multilingual benchmarks....read moreread less\n\n99\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n...\n\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\nProceedings Article\u2022DOI\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 moreGoogle\n - 11 Oct 2018\n\nShow Less\n\n\n\nTL;DR: BERT as mentioned in this paper pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks....read moreread less\n\n24.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nDistributed Representations of Words and Phrases and their Compositionality\nTomas Mikolov,Ilya Sutskever,Kai Chen,Greg S. Corrado,Jeffrey Dean +4 moreGoogle\n - 05 Dec 2013\n\nShow Less\n\n\n\nTL;DR: This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling....read moreread less\n\n24K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nDeep contextualized word representations\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 moreAllen Institute for Artificial Intelligence,University of Massachusetts Amherst,Google,University of Washington\n - 15 Feb 2018\n\nShow Less\n\n\n\nTL;DR: This paper introduced a new type of deep contextualized word representation that models both complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic contexts (i.e., to model polysemy)....read moreread less\n\n7.4K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nPranav Rajpurkar,Jian Zhang,Konstantin Lopyrev,Percy Liang +3 moreStanford University\n - 16 Jun 2016\n\nShow Less\n\n\n\nTL;DR: The Stanford Question Answering Dataset (SQuAD) as mentioned in this paper is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage....read moreread less\n\n3.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nAlex Wang,Amanpreet Singh,Julian Michael,Felix Hill,Omer Levy,Samuel R. Bowman +5 moreNew York University,University of Washington,Google\n - 20 Apr 2018\n\nShow Less\n\n\n\nTL;DR: A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks....read moreread less\n\n2.1K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n...\n\n\n\n\n\nRelated Papers (5)\n\n\n\n\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n[...]\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 more\n - 11 Oct 2018\n\nShow Less\n\n\n\n\nDeep contextualized word representations\n[...]\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 more\n - 15 Feb 2018\n\nShow Less\n\n\n\n\nBeto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\n[...]\nShijie Wu,Mark Dredze +1 more\n - 19 Apr 2019\n\nShow Less\n\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n[...]\nColin Raffel,Noam Shazeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena,Yanqi Zhou,Wei Li,Peter J. Liu +8 more\n - 01 Jan 2020\n - Journal of Machine Learning Research \nShow Less\n\n\n\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\n[...]\nTaku Kudo,John Richardson +1 more\n - 19 Aug 2018\n\nShow Less\n\n\n\n\n\n\nTools\nChat with PDFLiterature ReviewAI WriterFind TopicsParaphraserCitation GeneratorExtract DataAI DetectorCitation BoosterExtensions\nSciSpace Chrome Extension\nSciSpace\nAboutCareersResourcesSupportBrowse PapersPricingSciSpace Affiliate ProgramCancellation & Refund PolicyTermsPrivacyData Sources\nDirectories\nPapersTopicsJournalsAuthorsConferencesInstitutionsCitation Styles\nContact\nsupport@typeset.io+1 (760) 284-7800+91 9916292973\n\n\u00a9 2025 | PubGenius Inc. | Suite # 217 691 S Milpitas Blvd Milpitas CA 95035, USA", "title": "(PDF) Unsupervised Cross-lingual Representation Learning at Scale (2020) | Alexis Conneau | 3721 Citations", "description": "(DOI: 10.18653/V1/2020.ACL-MAIN.747) This <strong>paper</strong> shows that pretraining multilingual language models <strong>at</strong> <strong>scale</strong> leads to significant performance gains for a wide range of <strong>cross</strong>-<strong>lingual</strong> <strong>transfer</strong> <strong>tasks</strong>. We train a Transformer-based masked language model on one hundred languages, <strong>using</strong> more than ...", "usage": {"tokens": 2990}, "retrieved_text": ["(mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available. read more\nShow Related Papers\n\n\n\n\nChat with Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitations\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nMAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\nJonas Pfeiffer,Ivan Vuli\u0107,Iryna Gurevych,Sebastian Ruder +3 moreTechnische Universit\u00e4t Darmstadt,University of Mannheim,Google\n - 30 Apr 2020\n\nShow Less\n\n\n\nTL;DR: This paper proposed MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations, and introduced a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language....read moreread less\n\n169\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Journal Article\u2022DOI\nPre-Trained Models: Past, Present and Future\nXu Han,Zhengyan Zhang,Ning Ding,Yuxian Gu,Xiao Liu,Yuqi Huo,Jiezhong Qiu,Liang Zhang,Wentao Han,Minlie Huang,Qin Jin,Yanyan Lan,Yang Liu,Zhiyuan Liu,Zhiwu Lu,Xipeng Qiu,Ruihua Song,Jie Tang,Ji-Rong Wen,Jinhui Yuan,Wayne Xin Zhao,Jun Zhu +21 moreTsinghua University,Renmin University of China,Fudan University\n - 14 Jun 2021\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum....read moreread less\n\n135\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nAdapterHub: A Framework for Adapting Transformers\nJonas Pfeiffer,Andreas R\u00fcckl\u00e9,Clifton Poth,Aishwarya Kamath,Ivan Vuli\u0107,Sebastian Ruder,Kyunghyun Cho,Iryna Gurevych +7 moreTechnische Universit\u00e4t Darmstadt,New York University,University of Oslo,Google,Canadian Institute for Advanced Research\n - 15 Jul 2020\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors propose a framework that allows dynamic \"stiching-in\" of pre-trained adapters for different tasks and languages, which enables extremely easy and quick adaptation of state-of-the-art pre-training models across tasks....read moreread less\n\n122\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nFlauBERT: Unsupervised Language Model Pre-training for French\nHang Le,Lo\u00efc Vial,Jibril Frej,Vincent Segonne,Maximin Coavoux,Benjamin Lecouteux,Alexandre Allauzen,Beno\u00eet Crabb\u00e9,Laurent Besacier,Didier Schwab +9 moreUniversity of Grenoble,Centre national de la recherche scientifique\n - 11 Dec 2019\n\nShow Less\n\n\n\nTL;DR: The authors proposed FlauBERT, a model learned on a very large and heterogeneous French corpus and applied it to various NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and showed that most of the time they outperformed other pre-training approaches....read moreread less\n\n120\nPodcast\n\n\n\n\n\n\n\n\n\u2022Posted Content\nmT5: A massively multilingual pre-trained text-to-text transformer\nLinting Xue,Noah Constant,Adam Roberts,Mihir Kale,Rami Al-Rfou,Aditya Siddhant,Aditya Barua,Colin Raffel +7 moreGoogle,University of Chester\n - 22 Oct 2020\n - arXiv: Computation and Language \nShow Less\n\n\n\nTL;DR: This article proposed a multilingual variant of T5, mT5, which was pre-trained on a new Common Crawl-based dataset covering 101 languages and achieved state-of-the-art performance on many multilingual benchmarks....read moreread less\n\n99\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n...\n\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\nProceedings Article\u2022DOI\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 moreGoogle\n - 11 Oct 2018\n\nShow Less\n\n\n\nTL;DR: BERT as mentioned in this paper pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks....read moreread less\n\n24.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nDistributed Representations of Words and Phrases and their Compositionality\nTomas Mikolov,Ilya Sutskever,Kai Chen,Greg S. Corrado,Jeffrey Dean +4 moreGoogle\n - 05 Dec 2013\n\nShow Less\n\n\n\nTL;DR: This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling....read moreread less\n\n24K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nDeep contextualized word representations\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 moreAllen Institute for Artificial Intelligence,University of Massachusetts Amherst,Google,University of Washington\n - 15 Feb 2018\n\nShow Less\n\n\n\nTL;DR: This paper introduced a new type of deep contextualized word representation that models both complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic contexts (i.e., to model polysemy)....read moreread less\n\n7.4K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nPranav Rajpurkar,Jian Zhang,Konstantin Lopyrev,Percy Liang +3 moreStanford University\n - 16 Jun 2016\n\nShow Less\n\n\n\nTL;DR: The Stanford Question Answering Dataset (SQuAD) as mentioned in this paper is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage....read moreread less\n\n3.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nAlex Wang,Amanpreet Singh,Julian Michael,Felix Hill,Omer Levy,Samuel R. Bowman +5 moreNew York University,University of Washington,Google\n - 20 Apr 2018\n\nShow Less\n\n\n\nTL;DR: A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks....read moreread less\n\n2.1K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n...\n\n\n\n\n\nRelated Papers (5)\n\n\n\n\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n[...]\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 more\n - 11 Oct 2018\n\nShow Less\n\n\n\n\nDeep contextualized word representations\n[...]\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 more\n - 15 Feb 2018\n\nShow Less\n\n\n\n\nBeto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\n[...]\nShijie Wu,Mark Dredze +1 more\n - 19 Apr 2019\n\nShow Less\n\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n[...]\nColin Raffel,Noam Shazeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena,Yanqi Zhou,Wei Li,Peter J. Liu +8 more\n - 01 Jan 2020\n - Journal of Machine Learning Research \nShow Less\n\n\n\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\n[...]\nTaku Kudo,John Richardson +1 more\n - 19 Aug 2018\n\nShow Less\n\n\n\n\n\n\nTools\nChat with PDFLiterature ReviewAI WriterFind TopicsParaphraserCitation GeneratorExtract DataAI DetectorCitation BoosterExtensions\nSciSpace Chrome Extension\nSciSpace\nAboutCareersResourcesSupportBrowse PapersPricingSciSpace Affiliate ProgramCancellation & Refund PolicyTermsPrivacyData Sources\nDirectories\nPapersTopicsJournalsAuthorsConferencesInstitutionsCitation Styles\nContact\nsupport@typeset.io+1 (760) 284-7800+91 9916292973\n\n\u00a9 2025 | PubGenius Inc. | Suite # 217 691 S Milpitas Blvd Milpitas CA 95035, USA", "Home Chat with PDF Literature Review AI Writer Find Topics Paraphraser Citation Generator Extract Data AI Detector PDF to Video Affiliate Program Chrome Extension Use on ChatGPT Contact Us Home Papers Unsupervised Cross-lingual Representation Learning at Scale Open AccessProceedings ArticleDOI Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University - 01 Jul 2020 - pp 8440-8451 Show Less Podcast 3.7K PDF Save Cite TL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time. read more Abstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "Home\n\nChat with PDF\n\nLiterature Review\n\nAI Writer\n\nFind Topics\n\nParaphraser\n\nCitation Generator\n\nExtract Data\n\nAI Detector\n\nPDF to Video\n\nAffiliate Program\n\nChrome Extension\n\n\nUse on ChatGPT\n\n\nContact Us\n\n\n\n\nHome\nPapers\nUnsupervised Cross-lingual Representation Learning at Scale\n\n\n\nOpen AccessProceedings ArticleDOI\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University\n - 01 Jul 2020\n\n - pp 8440-8451\nShow Less\n\n\n\nPodcast\n3.7K\nPDF\n\nSave\nCite\n\n\n\n\nTL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time. read more\n\n\nAbstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER."]}, {"text": "         \n\n\n\ndblp\n\n\nBlog\nStatistics\n\nUpdate feed\nXML dump\nRDF dump\n\nbrowse\n\n\nPersons\n\nConferences\nJournals\nSeries\nRepositories\n\nsearch\n\n\nSearch dblp\nSPARQL queries \n\nLookup by ID\n\nabout\n\n\nF.A.Q.\nTeam\nAdvisory Board\n\nLicense\nPrivacy\nImprint\n\nnfdi\n\n\nNFDI - National Research Data Infrastructure Germany\nconsortia\n\nNFDIxCS\nevents | twitter\n\nNFDI4DataScience\nORKG\nCEUR\nMyBinder\nevents | twitter | publications \n\n\nstop the war!\n\n\n\u0417\u0443\u043f\u0438\u043d\u0456\u0442\u044c \u0432\u0456\u0439\u043d\u0443!\n\u041e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0435 \u0432\u043e\u0439\u043d\u0443!\nsolidarity - (ua) - (ru)\nnews - (ua) - (ru)\ndonate - donate - donate\n\nfor scientists\nERA4Ukraine\nAssistance in Germany\nUkrainian Global University\n#ScienceForUkraine\n   \nWe've just launched a new service: our brand new dblp SPARQL query service. Read more about it in our latest blog post or try out some of the SPARQL queries linked on the dblp web pages below.\n \n \n \n\n\n\n\ndefault search action\n\ncombined dblp search\nauthor search\nvenue search\npublication search\n\n\nAuthors:\n\nno matches\n\n\nVenues:\n\nno matches\n\n\nPublications:\n\nno matches\n\n\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nORCID    \n\n\n\n\n \n\nDagstuhl\n\n\nTrier \n\n\n> Home  \n\n\n\n\n\nrun query for this record\n\nreferenced publications\ncited by publications\nco-cited publications\nciting venues\nco-cited venues\nor build your own?  \n\n\n\nDOI: 10.18653/V1/2020.ACL-MAIN.747\naccess: open\ntype: Conference or Workshop Paper\nmetadata version: 2022-03-16\n\n\nload more data from openalex.org\nPrivacy notice:\nBy enabling the option above, your browser will contact the API of openalex.org to load information about this publication. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex.\nYour decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time.\nlooking up data   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\ncontained concepts\n\n\nrecent citations by year\n   \n\n\n\n\n\n\n\n\nview\n\nelectronic edition via DOI (open access)\nelectronic edition @ aclanthology.org (open access)\nauthority control:\n\n \n\n\n\nexport record\n\nBibTeX\nRIS\nRDF N-Triples\nRDF Turtle\nRDF/XML\nXML\ndblp key:\n\nconf/acl/ConneauKGCWGGOZ20\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nPubPeer\n\n\n\nshare record\n\nTwitter\nReddit\nBibSonomy\nLinkedIn\nFacebook\npersistent URL:\n\nhttps://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov:\n Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451\n\nPlease note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public. For more information please see the Initiative for Open Citations (I4OC). Please also note that there is no way of submitting missing references or citation data directly to dblp.\n\n\nPlease also note that this feature is work in progress and that it is still far from being perfect. That is, in particular,\n\nthe lists below may be incomplete due to unavailable citation data,\nreference strings may not have been successfully mapped to the items listed in dblp, and\nwe do not have complete and curated metadata for all items given in these lists.\n\n\nJavaScript is requires in order to retrieve and display any references and citations for this record.\n\n\n\n\nload references from crossref.org, opencitations.net, and semanticscholar.org\nPrivacy notice:\nBy enabling the option above, your browser will contact the APIs of crossref.org, opencitations.net, and semanticscholar.org to load citation information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Crossref privacy policy and the OpenCitations privacy policy, as well as the AI2 Privacy Policy covering Semantic Scholar.\nYour decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time.\n\nlooking up references   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\n\n\n\nload citations from opencitations.net and semanticscholar.org\nPrivacy notice:\nBy enabling the option above, your browser will contact the APIs of opencitations.net and semanticscholar.org to load citation information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the OpenCitations privacy policy as well as the AI2 Privacy Policy covering Semantic Scholar.\nYour decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time.\n\nlooking up citations   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\n\n\n\nmanage site settings\n\nTo protect your privacy, all features that rely on external API calls from your browser are turned off by default. You need to opt-in for them to become active. All settings here will be stored as cookies with your web browser. For more information see our F.A.Q.\n\n\n[+][\u2013] Unpaywalled article links\n\nAdd open access links from  to the list of external document links (if available).\nload links from unpaywall.org\nPrivacy notice: By enabling the option above, your browser will contact the API of unpaywall.org to load hyperlinks to open access articles. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Unpaywall privacy policy.\n\n[+][\u2013] Archived links via Wayback Machine\n\nFor web page which are no longer available, try to retrieve content from the  of the Internet Archive (if available).\nload content from archive.org\nPrivacy notice: By enabling the option above, your browser will contact the API of archive.org to check for archived content of web pages that are no longer available. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Internet Archive privacy policy.\n\n[+][\u2013] Reference lists\n\nAdd a list of references from , , and  to record detail pages.\nload references from crossref.org and opencitations.net\nPrivacy notice: By enabling the option above, your browser will contact the APIs of crossref.org, opencitations.net, and semanticscholar.org to load article reference information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the Crossref privacy policy and the OpenCitations privacy policy, as well as the AI2 Privacy Policy covering Semantic Scholar.\n\n[+][\u2013] Citation data\n\nAdd a list of citing articles from  and  to record detail pages.\nload citations from opencitations.net\nPrivacy notice: By enabling the option above, your browser will contact the API of opencitations.net and semanticscholar.org to load citation information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the OpenCitations privacy policy as well as the AI2 Privacy Policy covering Semantic Scholar.\n\n[+][\u2013] OpenAlex data \n\nLoad additional information about publications from .\nload data from openalex.org\nPrivacy notice: By enabling the option above, your browser will contact the API of openalex.org to load additional information. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex.\n\n\n\n  last updated on 2022-03-16 23:55 CET by the dblp team\n all metadata released as open data under CC0 1.0 license\nsee also: Terms of Use | Privacy Policy | Imprint\n\ndblp was originally created in 1993 at:   \n\nsince 2018, dblp has been operated and maintained by:   \n\nthe dblp computer science bibliography is funded and supported by:                   \n  ", "title": "dblp: Unsupervised Cross-lingual Representation Learning at Scale.", "description": "Bibliographic details on <strong>Unsupervised</strong> <strong>Cross</strong>-<strong>lingual</strong> <strong>Representation</strong> <strong>Learning</strong> <strong>at</strong> <strong>Scale</strong>.", "usage": {"tokens": 2041}, "retrieved_text": ["conf/acl/ConneauKGCWGGOZ20 ask others Google Google Scholar Semantic Scholar Internet Archive Scholar CiteSeerX PubPeer share record Twitter Reddit BibSonomy LinkedIn Facebook persistent URL: https://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451 Please note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public. For more information please see the Initiative for Open Citations (I4OC). Please also note that there is no way of submitting missing references or citation data directly to dblp. Please also note that this feature is work in progress and that it is still far from being perfect. That is, in particular,\n\nthe lists below may be incomplete due to unavailable citation data,\nreference strings may not have been successfully mapped to the items listed in dblp, and\nwe do not have complete and curated metadata for all items given in these lists.", "uses your data. So please proceed with care and consider checking the information given by OpenAlex. Your decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time. looking up data no open data available, sorry service temporarily unavailable, please try again later contained concepts recent citations by year view electronic edition via DOI (open access) electronic edition @ aclanthology.org (open access) authority control: export record BibTeX RIS RDF N-Triples RDF Turtle RDF/XML XML dblp key: conf/acl/ConneauKGCWGGOZ20 ask others Google Google Scholar Semantic Scholar Internet Archive Scholar CiteSeerX PubPeer share record Twitter Reddit BibSonomy LinkedIn Facebook persistent URL: https://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451\n\nPlease note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public.", "publications citing venues co-cited venues or build your own? DOI: 10.18653/V1/2020.ACL-MAIN.747 access: open type: Conference or Workshop Paper metadata version: 2022-03-16 load more data from openalex.org Privacy notice: By enabling the option above, your browser will contact the API of openalex.org to load information about this publication. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex. Your decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time. looking up data   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\ncontained concepts\n\n\nrecent citations by year\n   \n\n\n\n\n\n\n\n\nview\n\nelectronic edition via DOI (open access)\nelectronic edition @ aclanthology.org (open access)\nauthority control:\n\n \n\n\n\nexport record\n\nBibTeX\nRIS\nRDF N-Triples\nRDF Turtle\nRDF/XML\nXML\ndblp key:\n\nconf/acl/ConneauKGCWGGOZ20\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nPubPeer\n\n\n\nshare record\n\nTwitter\nReddit\nBibSonomy\nLinkedIn\nFacebook\npersistent URL:\n\nhttps://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov:\n Unsupervised Cross-lingual Representation Learning at Scale."]}, {"text": "     \n     \n   \n \n \n \n    \n   \n  Browse State-of-the-Art   \n  Datasets   \n Methods  \n  More  \n Newsletter RC2022 \n About Trends  Portals   Libraries     \n \n     \n Sign In      \n \n \n \n \nSubscribe to the PwC Newsletter \n \u00d7   \n \n \n Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\n\n Read previous issues     \n \nSubscribe       \n \n \n \n \nJoin the community \n \u00d7   \n You need to log in to edit.\n You can create a new account if you don't have one.\n\n       \n \n \n \n \nEdit Social Preview \n \u00d7   \n    \n Description\n \n  Default  \n  Custom   \n \n Implemented in 31 code libraries.  \n    \n Image\n \n \n  Default  \n  Custom  \n  None    \n \n   \n \n File is too large  \n Upload an image to customize your repository\u2019s social media preview.\n Images should be at least 640\u00d7320px (1280\u00d7640px for best display).    \n    \n \n Close  \n Save       \n \n \n \n \nAdd a new code entry for this paper \n \u00d7   \n \n  \n  GitHub, GitLab or BitBucket URL:*  \n    \n \n   Official code from paper authors     \n \nSubmit        \n \n \n \n \nRemove a code repository from this paper \n \u00d7   \n \n \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n    \n \n   \n-     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n   \n \n   \n-     \n \n \n   huggingface/transformers    \n \n  137,405   \n \n    \n \n   \n-     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n    \n \n   \n-     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n    \n \n   \n-     \n \n \n   Unbabel/COMET    \n \n  525   \n \n    \n \n   \n-     \n \n \n   google-deepmind/xquad    \n \n  184   \n \n   \n \n   \n-     \n \n \n   deepmind/xquad    \n \n  184   \n \n   \n \n   \n-     \n \n \n   apple/ml-mkqa    \n \n  171   \n \n   \n \n   \n-     \n \n \n   inzva/turkish-glove    \n \n  67   \n \n   \n \n   \n-     \n \n \n   bitextor/bicleaner-ai    \n \n  39   \n \n    \n \n   \n-     \n \n \n   ilhamfp/indonesian-text-classification-multilingual    \n \n  29   \n \n    \n \n   \n-     \n \n \n   Tikquuss/meta_XLM    \n \n  20   \n \n    \n \n   \n-     \n \n \n   ashwanitanwar/nmt-transfer-learning-xlm-r    \n \n  18   \n \n    \n \n   \n-     \n \n \n   heraclex12/vietpunc    \n \n  13   \n \n    \n \n   \n-     \n \n \n   huhailinguist/ChineseNLIProbing    \n \n  10   \n \n    \n \n   \n-     \n \n \n   martincjespersen/DaAnonymization    \n \n  9   \n \n   \n \n   \n-     \n \n \n   YuweiYin/HLT-MT    \n \n  4   \n \n    \n \n   \n-     \n \n \n   stanleyjzheng/ignitionhacks    \n \n  3   \n \n    \n \n   \n-     \n \n \n   Stanley-Zheng/ignitionhacks    \n \n  3   \n \n    \n \n   \n-     \n \n \n   deterministic-algorithms-lab/Large-XLM    \n \n  2   \n \n    \n \n   \n-     \n \n \n   MindCode-4/code-5    \n \n  1   \n \n    \n \n   \n-     \n \n \n   somiltg/xquad    \n \n  1   \n \n   \n \n   \n-     \n \n \n   pwc-1/Paper-9    \n \n  1   \n \n    \n \n   \n-     \n \n \n   bcaitech1/p2-klue-Heeseok-Jeong    \n \n  1   \n \n    \n \n   \n-     \n \n \n   MindCode-4/code-13    \n \n  0   \n \n    \n \n   \n-     \n \n \n   aleksac99/SQuAD-SR    \n \n  0   \n \n   \n \n   \n-     \n \n \n   lsqdecodebox/NLP-Comptition-review    \n \n  0   \n \n   \n \n   \n-     \n \n \n   feyzaakyurek/XLM-LwLL    \n \n  0   \n \n    \n \n   \n-     \n \n \n   kheeong/XLM_OWN    \n \n  0   \n \n    \n \n   \n-     \n \n \n   1-punchMan/CLTS    \n \n  0   \n \n    \n \n   \n-            \n \n \n \n \n Mark the official implementation from paper authors  \n \u00d7   \n   \n \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n    \n     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n   \n     \n \n \n   huggingface/transformers    \n \n  137,405   \n \n    \n     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n    \n     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n    \n     \n \n \n   Unbabel/COMET    \n \n  525   \n \n    \n     \n \n \n   google-deepmind/xquad    \n \n  184   \n \n   \n     \n \n \n   deepmind/xquad    \n \n  184   \n \n   \n     \n \n \n   apple/ml-mkqa    \n \n  171   \n \n   \n     \n \n \n   inzva/turkish-glove    \n \n  67   \n \n   \n     \n \n \n   bitextor/bicleaner-ai    \n \n  39   \n \n    \n     \n \n \n   ilhamfp/indonesian-text-classification-multilingual    \n \n  29   \n \n    \n     \n \n \n   Tikquuss/meta_XLM    \n \n  20   \n \n    \n     \n \n \n   ashwanitanwar/nmt-transfer-learning-xlm-r    \n \n  18   \n \n    \n     \n \n \n   heraclex12/vietpunc    \n \n  13   \n \n    \n     \n \n \n   huhailinguist/ChineseNLIProbing    \n \n  10   \n \n    \n     \n \n \n   martincjespersen/DaAnonymization    \n \n  9   \n \n   \n     \n \n \n   YuweiYin/HLT-MT    \n \n  4   \n \n    \n     \n \n \n   stanleyjzheng/ignitionhacks    \n \n  3   \n \n    \n     \n \n \n   Stanley-Zheng/ignitionhacks    \n \n  3   \n \n    \n     \n \n \n   deterministic-algorithms-lab/Large-XLM    \n \n  2   \n \n    \n     \n \n \n   MindCode-4/code-5    \n \n  1   \n \n    \n     \n \n \n   somiltg/xquad    \n \n  1   \n \n   \n     \n \n \n   pwc-1/Paper-9    \n \n  1   \n \n    \n     \n \n \n   bcaitech1/p2-klue-Heeseok-Jeong    \n \n  1   \n \n    \n     \n \n \n   MindCode-4/code-13    \n \n  0   \n \n    \n     \n \n \n   aleksac99/SQuAD-SR    \n \n  0   \n \n   \n     \n \n \n   lsqdecodebox/NLP-Comptition-review    \n \n  0   \n \n   \n     \n \n \n   feyzaakyurek/XLM-LwLL    \n \n  0   \n \n    \n     \n \n \n   kheeong/XLM_OWN    \n \n  0   \n \n    \n     \n \n \n   1-punchMan/CLTS    \n \n  0   \n \n    \n     \n  \n \n \n There is no official implementation   \n      \n \n \n Multiple official implementations   \n \n        \n \nSubmit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n  \n  Task:*  \n  --------- Multilingual NLP XLM-R Cross-Lingual Transfer Language Modelling Representation Learning    \n Not in the list? Add a task.  \n  Dataset:*  \n  ---------    \n  Model name:*  \n    \n  Metric name:*  \n  ---------    \n  \n \n   Higher is better (for the metric)    \n  Metric value:*  \n    \n  \n \n   Uses extra training data    \n  Data evaluated on  \n     \n \n Submit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n \n   TASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE          \n \n \n \n \nAdd a task \n \u00d7   \n \nAttached tasks: \n \n  CROSS-LINGUAL TRANSFER    \n \n  LANGUAGE MODELLING    \n \n  MULTILINGUAL NLP    \n \n  REPRESENTATION LEARNING    \n \n  XLM-R    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new task.  \n \n  New task name:  \n    \n  Top-level area:  \n  --------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series    \n  Parent task (if any):  \n  ---------    \n  Description:  \n     \n \n Submit         \n \n \n \n \nRemove a task \n \u00d7   \n \n \n \n \n    Cross-Lingual Transfer     \n-     \n \n \n    Language Modelling     \n-     \n \n \n    Multilingual NLP     \n-     \n \n \n    Representation Learning     \n-     \n \n \n    XLM-R     \n-           \n \n \n \n \nAdd a method \n \u00d7   \n \nAttached methods: \n \n  ADAM    \n \n  ATTENTION DROPOUT    \n \n  BERT    \n \n  BPE    \n \n  DENSE CONNECTIONS    \n \n  DROPOUT    \n \n  GELU    \n \n  LAYER NORMALIZATION    \n \n  LINEAR LAYER    \n \n  LINEAR WARMUP WITH LINEAR DECAY    \n \n  MULTI-HEAD ATTENTION    \n \n  RESIDUAL CONNECTION    \n \n  SCALED DOT-PRODUCT ATTENTION    \n \n  SOFTMAX    \n \n  WEIGHT DECAY    \n \n  WORDPIECE    \n \n  XLM    \n \n  XLM-R    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new method.  \n \n  New method name (e.g. ReLU):  \n    \n  New method full name (e.g. Rectified Linear Unit):  \n    \n  Paper where method was first introduced:  \n  ---------    \n  Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.  \n  ---------    \n  Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.  \n     \n \n Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n \n \n \n  ADAM    \n-     \n \n \n  ATTENTION DROPOUT    \n-     \n \n \n  BERT    \n-     \n \n \n  BPE    \n-     \n \n \n  DENSE CONNECTIONS    \n-     \n \n \n  DROPOUT    \n-     \n \n \n  GELU    \n-     \n \n \n  LAYER NORMALIZATION    \n-     \n \n \n  LINEAR LAYER    \n-     \n \n \n  LINEAR WARMUP WITH LINEAR DECAY    \n-     \n \n \n  MULTI-HEAD ATTENTION    \n-     \n \n \n  RESIDUAL CONNECTION    \n-     \n \n \n  SCALED DOT-PRODUCT ATTENTION    \n-     \n \n \n  SOFTMAX    \n-     \n \n \n  WEIGHT DECAY    \n-     \n \n \n  WORDPIECE    \n-     \n \n \n  XLM    \n-     \n \n \n  XLM-R    \n-           \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  CC100 \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  CoNLL 2002 GLUE MLQA QNLI SQuAD SST \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? \n   Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Cross-lingual Representation Learning at Scale  \n \n   ACL 2020    \u00b7   Alexis Conneau,  Kartikay Khandelwal,  Naman Goyal,  Vishrav Chaudhary,  Guillaume Wenzek,  Francisco Guzm\u00e1n,  Edouard Grave,  Myle Ott,  Luke Zettlemoyer,  Veselin Stoyanov \u00b7 \n  Edit social preview       \n \n \n \n This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.    PDF    Abstract     ACL 2020 PDF     ACL 2020 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n    \n \n \n   huggingface/transformers    \n \n  137,405   \n \n     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n     \n See all 31 implementations   \n \n \n \n   facebookresearch/XLM  official    \n \n  2,897   \n \n     \n \n \n   facebookresearch/cc_net  official    \n \n  977   \n \n    \n \n \n   huggingface/transformers    \n \n  137,405   \n \n     \n \n \n   deepset-ai/FARM    \n \n  1,749   \n \n     \n \n \n   nlp-uoregon/trankit    \n \n  743   \n \n     \n \n \n   Unbabel/COMET    \n \n  525   \n \n     \n \n \n   google-deepmind/xquad    \n \n  184   \n \n    \n \n \n   deepmind/xquad    \n \n  184   \n \n    \n \n \n   apple/ml-mkqa    \n \n  171   \n \n    \n \n \n   inzva/turkish-glove    \n \n  67   \n \n    \n \n \n   bitextor/bicleaner-ai    \n \n  39   \n \n     \n \n \n   ilhamfp/indonesian-text-classificat\u2026    \n \n  29   \n \n     \n \n \n   Tikquuss/meta_XLM   \n   \u21b3 Quickstart in   \n   Colab         \n \n  20   \n \n     \n \n \n   ashwanitanwar/nmt-transfer-learning\u2026    \n \n  18   \n \n     \n \n \n   heraclex12/vietpunc    \n \n  13   \n \n     \n \n \n   huhailinguist/ChineseNLIProbing    \n \n  10   \n \n     \n \n \n   martincjespersen/DaAnonymization    \n \n  9   \n \n    \n \n \n   YuweiYin/HLT-MT    \n \n  4   \n \n     \n \n \n   stanleyjzheng/ignitionhacks    \n \n  3   \n \n     \n \n \n   Stanley-Zheng/ignitionhacks    \n \n  3   \n \n     \n \n \n   deterministic-algorithms-lab/Large-\u2026    \n \n  2   \n \n     \n \n \n   MindCode-4/code-5    \n \n  1   \n \n     \n \n \n   somiltg/xquad    \n \n  1   \n \n    \n \n \n   pwc-1/Paper-9    \n \n  1   \n \n     \n \n \n   bcaitech1/p2-klue-Heeseok-Jeong    \n \n  1   \n \n     \n \n \n   MindCode-4/code-13    \n \n  0   \n \n     \n \n \n   aleksac99/SQuAD-SR    \n \n  0   \n \n    \n \n \n   lsqdecodebox/NLP-Comptition-review    \n \n  0   \n \n    \n \n \n   feyzaakyurek/XLM-LwLL    \n \n  0   \n \n     \n \n \n   kheeong/XLM_OWN    \n \n  0   \n \n     \n \n \n   1-punchMan/CLTS    \n \n  0   \n \n     \n Collapse 31 implementations     \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Cross-Lingual Transfer      Language Modelling      Multilingual NLP      Representation Learning      XLM-R         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \nIntroduced in the Paper:    CC100   \nUsed in the Paper:    GLUE      SST      SQuAD      QNLI      MLQA      CoNLL 2002          \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.       \n \n \n    \n \n \n \n \n \n Methods \n \n \n  Edit  \n   Add   Remove           \n  Adam \u2022  Attention Dropout \u2022  BERT \u2022  BPE \u2022  Dense Connections \u2022  Dropout \u2022  GELU \u2022  Layer Normalization \u2022  Linear Layer \u2022  Linear Warmup With Linear Decay \u2022  Multi-Head Attention \u2022  Residual Connection \u2022  Scaled Dot-Product Attention \u2022  Softmax \u2022  Weight Decay \u2022  WordPiece \u2022  XLM \u2022  XLM-R    \n \n Contact us on:   hello@paperswithcode.com .  Papers With Code is a free resource with all data licensed under CC-BY-SA.   \n Terms Data policy Cookies policy  from       ", "title": "Unsupervised Cross-lingual Representation Learning at Scale | Papers With Code", "description": "Upload an image to customize your repository\u2019s social media preview. Images should be at least 640\u00d7320px (1280\u00d7640px for best display) \u00b7 ACL 2020 \u00b7 Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke ...", "usage": {"tokens": 3837}, "retrieved_text": ["in this paper: CC100 [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: CoNLL 2002 GLUE MLQA QNLI SQuAD SST [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Cross-lingual Representation Learning at Scale ACL 2020 \u00b7 Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \u00b7 Edit social preview This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.", "Rectified Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 ADAM - ATTENTION DROPOUT - BERT - BPE - DENSE CONNECTIONS - DROPOUT - GELU - LAYER NORMALIZATION - LINEAR LAYER - LINEAR WARMUP WITH LINEAR DECAY - MULTI-HEAD ATTENTION - RESIDUAL CONNECTION - SCALED DOT-PRODUCT ATTENTION - SOFTMAX - WEIGHT DECAY - WORDPIECE - XLM - XLM-R - Edit Datasets \u00d7 Add or remove datasets introduced in this paper: CC100 [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: CoNLL 2002 GLUE MLQA QNLI SQuAD SST [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Cross-lingual Representation Learning at Scale  \n \n   ACL 2020    \u00b7   Alexis Conneau,  Kartikay Khandelwal,  Naman Goyal,  Vishrav Chaudhary,  Guillaume Wenzek,  Francisco Guzm\u00e1n,  Edouard Grave,  Myle Ott,  Luke Zettlemoyer,  Veselin Stoyanov \u00b7 \n  Edit social preview       \n \n \n \n This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.", "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \u00b7 Edit social preview This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks."]}], "The paper titled \"Learning to Translate in Real-time with Neural Machine Translation\" describes a method for real-time neural machine translation that can handle streaming inputs without waiting for the entire input sequence.": [{"text": " \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1610.00388 (cs)   \n \n \n [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] \nTitle:Learning to Translate in Real-time with Neural Machine Translation \nAuthors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li \nView a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF \n Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.   \n   Comments: 10 pages, camera ready   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1610.00388 [cs.CL]     (or  arXiv:1610.00388v3 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1610.00388\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Jiatao Gu [view email] \n [v1] Mon, 3 Oct 2016 02:11:03 UTC (1,464 KB)\n [v2] Thu, 6 Oct 2016 00:46:39 UTC (1,467 KB)\n [v3] Tue, 10 Jan 2017 21:07:56 UTC (2,974 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2016-10 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Jiatao Gu\nGraham Neubig\nKyunghyun Cho\nVictor O. K. Li  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1610.00388] Learning to Translate in Real-time with Neural Machine Translation", "description": "<strong>Translating</strong> <strong>in</strong> <strong>real</strong>-<strong>time</strong>, <strong>a</strong>.k.a. simultaneous <strong>translation</strong>, outputs <strong>translation</strong> words before <strong>the</strong> <strong>input</strong> sentence ends, which is a challenging problem for conventional <strong>machine</strong> <strong>translation</strong> <strong>methods</strong>. We propose <strong>a</strong> <strong>neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) framework for simultaneous <strong>translation</strong> <strong>in</strong> which an ...", "usage": {"tokens": 1314}, "retrieved_text": [" \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1610.00388 (cs)   \n \n \n [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] \nTitle:Learning to Translate in Real-time with Neural Machine Translation \nAuthors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li \nView a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF \n Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.", "Computer Science > Computation and Language arXiv:1610.00388 (cs) [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] Title:Learning to Translate in Real-time with Neural Machine Translation Authors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. Comments: 10 pages, camera ready   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1610.00388 [cs.CL]     (or  arXiv:1610.00388v3 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1610.00388\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Jiatao Gu [view email] \n [v1] Mon, 3 Oct 2016 02:11:03 UTC (1,464 KB)\n [v2] Thu, 6 Oct 2016 00:46:39 UTC (1,467 KB)\n [v3] Tue, 10 Jan 2017 21:07:56 UTC (2,974 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2016-10 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Jiatao Gu\nGraham Neubig\nKyunghyun Cho\nVictor O. K. Li  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)", "(What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack"]}, {"warning": "Target URL returned error 403: Forbidden", "text": " \n     \n \n\n\n\n\n\n\n\nWe've noticed some unusual traffic coming from your network. To continue, please check the box below.\n\n\nEnable JavaScript and cookies to continue\n\n\n\n\n\nAre you a researcher?\nTo avoid being denied access, log in if you're a ResearchGate member or create an account if you're not.\n\n\n\n\nLog in\nCreate account\n\n\nRay ID: 8ffd03d73b82aa45\nClient IP: 2600:1900:0:2d07::c00\n\u00a9 2008-2025 ResearchGate GmbH. All rights reserved.  ", "title": "Learning to Translate in Real-time with Neural Machine Translation | Request PDF", "description": "Request PDF | On Jan 1, 2017, Jiatao Gu and others published <strong>Learning</strong> <strong>to</strong> <strong>Translate</strong> <strong>in</strong> <strong>Real</strong>-<strong>time</strong> <strong>with</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> | Find, read and cite all the research you need on ResearchGate", "usage": {"tokens": 219}, "retrieved_text": ["Request PDF | On Jan 1, 2017, Jiatao Gu and others published <strong>Learning</strong> <strong>to</strong> <strong>Translate</strong> <strong>in</strong> <strong>Real</strong>-<strong>time</strong> <strong>with</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> | Find, read and cite all the research you need on ResearchGate"]}, {"text": "     \n     \n   \n \n \n \n    \n   \n  Browse State-of-the-Art   \n  Datasets   \n Methods  \n  More  \n Newsletter RC2022 \n About Trends  Portals   Libraries     \n \n     \n Sign In      \n \n \n \n \nSubscribe to the PwC Newsletter \n \u00d7   \n \n \n Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\n\n Read previous issues     \n \nSubscribe       \n \n \n \n \nJoin the community \n \u00d7   \n You need to log in to edit.\n You can create a new account if you don't have one.\n\n       \n \n \n \n \nEdit Social Preview \n \u00d7   \n    \n Description\n \n  Default  \n  Custom   \n \n Implemented in one code library.  \n    \n Image\n \n \n  Default  \n  Custom  \n  None    \n \n   \n \n File is too large  \n Upload an image to customize your repository\u2019s social media preview.\n Images should be at least 640\u00d7320px (1280\u00d7640px for best display).    \n    \n \n Close  \n Save       \n \n \n \n \nAdd a new code entry for this paper \n \u00d7   \n \n  \n  GitHub, GitLab or BitBucket URL:*  \n    \n \n   Official code from paper authors     \n \nSubmit        \n \n \n \n \nRemove a code repository from this paper \n \u00d7   \n \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n   \n \n   \n-            \n \n \n \n \n Mark the official implementation from paper authors  \n \u00d7   \n   \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n   \n     \n  \n \n \n There is no official implementation   \n      \n \n \n Multiple official implementations   \n \n        \n \nSubmit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n  \n  Task:*  \n  --------- Sentence NMT Translation Machine Translation    \n Not in the list? Add a task.  \n  Dataset:*  \n  ---------    \n  Model name:*  \n    \n  Metric name:*  \n  ---------    \n  \n \n   Higher is better (for the metric)    \n  Metric value:*  \n    \n  \n \n   Uses extra training data    \n  Data evaluated on  \n     \n \n Submit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n \n   TASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE          \n \n \n \n \nAdd a task \n \u00d7   \n \nAttached tasks: \n \n  MACHINE TRANSLATION    \n \n  NMT    \n \n  SENTENCE    \n \n  TRANSLATION    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new task.  \n \n  New task name:  \n    \n  Top-level area:  \n  --------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series    \n  Parent task (if any):  \n  ---------    \n  Description:  \n     \n \n Submit         \n \n \n \n \nRemove a task \n \u00d7   \n \n \n \n \n    Machine Translation     \n-     \n \n \n    NMT     \n-     \n \n \n    Sentence     \n-     \n \n \n    Translation     \n-           \n \n \n \n \nAdd a method \n \u00d7   \n \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new method.  \n \n  New method name (e.g. ReLU):  \n    \n  New method full name (e.g. Rectified Linear Unit):  \n    \n  Paper where method was first introduced:  \n  ---------    \n  Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.  \n  ---------    \n  Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.  \n     \n \n Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? \n   Add a new dataset here     \n \n Save        \n  \n \n \n \n Learning to Translate in Real-time with Neural Machine Translation  \n \n   EACL 2017    \u00b7   Jiatao Gu,  Graham Neubig,  Kyunghyun Cho,  Victor O. K. Li \u00b7 \n  Edit social preview       \n \n \n \n Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.    PDF    Abstract     EACL 2017 PDF     EACL 2017 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n     \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n       \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Machine Translation      NMT      Sentence      Translation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n \n  Edit  \n   Add   Remove      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.       \n \n \n    \n \n \n \n \n \n Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com .  Papers With Code is a free resource with all data licensed under CC-BY-SA.   \n Terms Data policy Cookies policy  from       ", "title": "Papers with Code - Learning to Translate in Real-time with Neural Machine Translation", "description": "Upload an image to customize your repository\u2019s social media preview. Images should be at least 640\u00d7320px (1280\u00d7640px for best display) \u00b7 EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview", "usage": {"tokens": 1370}, "retrieved_text": ["to Translate in Real-time with Neural Machine Translation EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. PDF    Abstract     EACL 2017 PDF     EACL 2017 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n     \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n       \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Machine Translation      NMT      Sentence      Translation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n \n  Edit  \n   Add   Remove      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers. Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper.", "Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 Edit Datasets \u00d7 Add or remove datasets introduced in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Learning to Translate in Real-time with Neural Machine Translation EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment.", "task \u00d7 Machine Translation - NMT - Sentence - Translation - Add a method \u00d7 Add: --------- Not in the list? Create a new method. New method name (e.g. ReLU): New method full name (e.g. Rectified Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? Add a new dataset here     \n \n Save        \n  \n \n \n \n Learning to Translate in Real-time with Neural Machine Translation  \n \n   EACL 2017    \u00b7   Jiatao Gu,  Graham Neubig,  Kyunghyun Cho,  Victor O. K. Li \u00b7 \n  Edit social preview       \n \n \n \n Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods."]}, {"text": "   \n\n\n\nSkip to search formSkip to main contentSkip to account menu\n\n\n\n\n\n\n\n\n\nDOI:10.18653/V1/E17-1099\nCorpus ID: 2782776\nLearning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} }\n\nGraham Neubig, Kyunghyun Cho, \n+1 author V. Li\nPublished in Conference of the European\u2026 3 October 2016\nComputer Science, Linguistics\n\n\n\nTLDR\nA neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed.\nExpand\n\n\n[PDF] Semantic Reader\n\n\n\n\n\n\n\n\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n\n\n\n209 Citations\n\n\n\nHighly Influential Citations\n36\n\n\n\nBackground Citations\n142\n\n\n Methods Citations\n73\n\n\n Results Citations\n4\nView All\n\n\n\n\n\nFigures from this paper\n\n\n\n\n\n\n\nfigure 1\n\n\n\nfigure 2\n\n\n\nfigure 3\n\n\n\nfigure 4\n\n\n\nfigure 5\n\n\n\nfigure 6\n\n\n\nfigure 7\n\n\n\nfigure 8\n\nView All 8 Figures & Tables\n\n\n\n\nTopics\n\n\nAI-Generated\n\n\nSimultaneous Translation (opens in a new tab)Simultaneous Machine Translation (opens in a new tab)Translation Delays (opens in a new tab)Simultaneous Translation Task (opens in a new tab)READ/WRITE Decision (opens in a new tab)Simultaneous Translation System (opens in a new tab)Average Proportion (opens in a new tab)Simultaneous Mt (opens in a new tab)Neural Machine Translation (opens in a new tab)Translate (opens in a new tab)\n\n\n\n\n209 Citations\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nA General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation\nYun ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nAACL\n2020\n\n\n\nTLDR\nA general framework for adapting neural machine translation to translate simultaneously and contains a prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation.\nExpand\n\n\n\n\n2\n\nHighly Influenced\n\n\nPDF\n\n\n\n\n\n12 Excerpts\n\n\nSave\n\n\nHow to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?\nYuxiang ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nArXiv\n2019\n\n\n\nTLDR\nA general framework to improve simultaneous translation with a pretrained consecutive neural machine translation (CNMT) model is proposed that contains two parts: prefix translation that utilizes a Pretrained CNMT model to better translate source prefixes and a stopping criterion that determines when to stop the prefix translation.\nExpand\n\n\n\n\n2\n\nHighly Influenced[PDF]\n\n\n\n\n\n9 Excerpts\n\n\nSave\n\n\nNeural Simultaneous Speech Translation Using Alignment-Based Chunking\nP. WilkenTamer AlkhouliE. MatusovPavel Golik\nComputer Science\nIWSLT\n2020\n\n\n\nTLDR\nA neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words, and compares models with bidirectional and unidirectional encoders of different depths, both on real speech and text input.\nExpand\n\n\n\n\n14\n\nHighly Influenced[PDF]\n\n\n\n\n\n4 Excerpts\n\n\nSave\n\n\nSimultaneous Neural Machine Translation with Constituent Label Prediction\nYasumasa KanoKatsuhito SudohSatoshi Nakamura\nComputer Science, Linguistics\nWMT\n2021\n\n\n\nTLDR\nMotivated by the concept of pre-reordering, a couple of simple decision rules are proposed using the label of the next constituent predicted by incremental constituent label prediction to decide when to start the translation process.\nExpand\n\n\n\n\n3[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nA Generative Framework for Simultaneous Machine Translation\nYishu MiaoPhil BlunsomLucia Specia\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work formulate simultaneous translation as a structural sequence-to-sequence learning problem, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.\nExpand\n\n\n\n\n24\n\nHighly Influenced\n\n\nPDF\n\n\n\n\n\n8 Excerpts\n\n\nSave\n\n\nReinforcement Learning for on-line Sequence Transformation\nGrzegorz RypescLukasz LepakPawel Wawrzy'nski\nComputer Science\n2022 17th Conference on Computer Science and\u2026\n2022\n\n\n\nTLDR\nThis paper introduces an SMT system that learns with reinforcement and is able to find the optimal delay in training, and conducts experiments on Tatoeba and IWSLT2014 datasets against state-of-the-art translation architectures.\nExpand\n\n\n[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nTowards Multimodal Simultaneous Neural Machine Translation\nAizhan ImankulovaMasahiro KanekoTosho HirasawaMamoru Komachi\nComputer Science\nWMT\n2020\n\n\n\nTLDR\nThe experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency, and verified the importance of visual information during decoding by performing an adversarial evaluation ofMSNMT.\nExpand\n\n\n\n\n15[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nSimultaneous Neural Machine Translation using Connectionist Temporal Classification\nKatsuki ChousaKatsuhito SudohSatoshi Nakamura\nComputer Science\nArXiv\n2019\n\n\n\nTLDR\nA neural machine translation method is proposed to determine when to start the translation for observed inputs so far in an adaptive manner and introduces a special token ' ', which is generated when the translation model chooses to read the next input token instead of generating an output token.\nExpand\n\n\n\n\n5[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nTranslation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation\nAshkan AlinejadHassan S. ShavaraniAnoop Sarkar\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work proposes a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences.\nExpand\n\n\n\n\n10\n\n\nPDF\n\n\n\n\n\n3 Excerpts\n\n\nSave\n\n\nAnticipation-Free Training for Simultaneous Machine Translation\nChih-Chiang ChangShun-Po ChuangHung-yi Lee\nComputer Science\nIWSLT\n2022\n\n\n\nTLDR\nA new framework is proposed that decompose the translation process into the monotonic translation step and the reordering step, and the latter is model by the auxiliary sorting network (ASN), so that the SimulMT model could learn to translate more reasonably.\nExpand\n\n\n\n\n6[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\n\n\n\n\n...\n\n1\n\n2\n\n3\n\n4\n\n5\n\n...\n\n\n\n\n\n\n24 References\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nSimultaneous Machine Translation using Deep Reinforcement Learning\n\nComputer Science, Linguistics\n2016\n\n\n\nTLDR\nA framework which uses the existing neural machine translation systems to function as a simultaneous machine translation system and shows that the policy learnt by the system out-performs the monotone and the batch translation policies while maintaining a delay-accuracy trade-off.\nExpand\n\n\n\n\n40\n\n\nPDF\n\n\n\nSave\n\n\nNeural Machine Translation by Jointly Learning to Align and Translate\nDzmitry BahdanauKyunghyun ChoYoshua Bengio\nComputer Science\nICLR\n2015\n\n\n\nTLDR\nIt is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.\nExpand\n\n\n\n\n26,339[PDF]\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nCan neural machine translation do simultaneous translation?\nKyunghyun ChoMasha Esipova\nComputer Science, Linguistics\nArXiv\n2016\n\n\n\nTLDR\nA novel decoding algorithm is introduced that allows an existing neural machine translation model to begin translating before a full source sentence is received and is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality.\nExpand\n\n\n\n\n149\n\nHighly Influential[PDF]\n\n\n\n\n\n7 Excerpts\n\n\nSave\n\n\nDon't Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation\nAlvin Grissom IIHe HeJordan L. Boyd-GraberJohn MorganHal Daum\u00e9\nComputer Science, Linguistics\nEMNLP\n2014\n\n\n\nTLDR\nA reinforcement learningbased approach to simultaneous machine translation\u2014producing a translation while receiving input words\u2014 between languages with drastically different word orders, which shows that the new translation model outperforms batch and monotone translation strategies.\nExpand\n\n\n\n\n120\n\n\nPDF\n\n\n\nSave\n\n\nSimple, lexicalized choice of translation timing for simultaneous speech translation\nTomoki FujitaGraham NeubigS. SaktiT. TodaSatoshi Nakamura\nComputer Science\nINTERSPEECH\n2013\n\n\n\nTLDR\nThis work proposes a method that uses lexicalized information to perform translation unit segmentation considering the relationship between the source and target languages and shows that this system can achieve a delay reduction of 20% compared to pause segmentation with identical accuracy.\nExpand\n\n\n\n\n46\n\n\nPDF\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nIncremental Segmentation and Decoding Strategies for Simultaneous Translation\nM. YarmohammadiV. SridharS. BangaloreB. Sankaran\nComputer Science, Linguistics\nIJCNLP\n2013\n\n\n\nTLDR\nA novel input segmentation method is proposed using the phrase alignment structure of the language pair to model the human interpretation process and compare and contrast three incremental decoding and two differentinput segmentation strategies for simultaneous translation.\nExpand\n\n\n\n\n35\n\n\nPDF\n\n\n\n\n\n2 Excerpts\n\n\nSave\n\n\nLearning online alignments with continuous rewards policy gradient\nYuping LuoChung-Cheng ChiuN. JaitlyI. Sutskever\nComputer Science\n2017 IEEE International Conference on Acoustics\u2026\n2017\n\n\n\nTLDR\nThis work presents a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments, which achieves encouraging performance on TIMIT and Wall Street Journal speech recognition datasets.\nExpand\n\n\n\n\n46[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nSegmentation Strategies for Streaming Speech Translation\nV. SridharJohn ChenS. BangaloreA. LjoljeR. Chengalvarayan\nComputer Science, Linguistics\nNAACL\n2013\n\n\n\nTLDR\nThe study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics, and demonstrates that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma- based segmentation.\nExpand\n\n\n\n\n69\n\n\nPDF\n\n\n\nSave\n\n\nOnline Segment to Segment Neural Transduction\nLei YuJan BuysPhil Blunsom\nComputer Science\nEMNLP\n2016\n\n\n\nTLDR\nAn online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read that tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states.\nExpand\n\n\n\n\n82[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nSequence to Sequence Learning with Neural Networks\nI. SutskeverO. VinyalsQuoc V. Le\nComputer Science\nNIPS\n2014\n\n\n\nTLDR\nThis paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing the order of the words in all source sentences improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\nExpand\n\n\n\n\n19,796[PDF]\n\n\n\nSave\n\n\n\n\n\n\n...\n\n1\n\n2\n\n3\n\n...\n\n\n\nRelated Papers\n\nShowing 1 through 3 of 0 Related Papers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures\nTopics\n209 Citations\n24 References\nRelated Papers\n\n\n\n\nStay Connected With Semantic Scholar\n\nSign Up\n\n\nWhat Is Semantic Scholar?\nSemantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.Learn More\n\n\nAboutAbout UsMeet the TeamPublishersBlog (opens in a new tab)Ai2 Careers (opens in a new tab)\n\nProductProduct OverviewSemantic ReaderScholar's HubBeta ProgramRelease Notes\n\nAPIAPI OverviewAPI TutorialsAPI Documentation (opens in a new tab)API Gallery\n\nResearchPublicationsResearchersResearch CareersPrototypesResources\n\nHelpFAQLibrariansTutorials\nContact\n\n\nProudly built by Ai2 (opens in a new tab)\n\nCollaborators & Attributions \u2022Terms of Service (opens in a new tab)\u2022Privacy Policy (opens in a new tab)\u2022API License AgreementThe Allen Institute for AI (opens in a new tab)\n\n\n\nBy clicking accept or continuing to use the site, you agree to the terms outlined in our Privacy Policy (opens in a new tab), Terms of Service (opens in a new tab), and Dataset License (opens in a new tab)\n\nACCEPT & CONTINUE\nor Only Accept Required            ", "title": "[PDF] Learning to Translate in Real-time with Neural Machine Translation | Semantic Scholar", "description": "<strong>A</strong> <strong>neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) framework for simultaneous <strong>translation</strong> <strong>in</strong> which an agent <strong>learns</strong> <strong>to</strong> make decisions on when <strong>to</strong> <strong>translate</strong> from the interaction with a pre-trained NMT environment is proposed. <strong>Translating</strong> <strong>in</strong> <strong>real</strong>-<strong>time</strong>, <strong>a</strong>.k.a.simultaneous <strong>translation</strong>, outputs <strong>translation</strong> words ...", "usage": {"tokens": 3061}, "retrieved_text": ["   \n\n\n\nSkip to search formSkip to main contentSkip to account menu\n\n\n\n\n\n\n\n\n\nDOI:10.18653/V1/E17-1099\nCorpus ID: 2782776\nLearning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} }\n\nGraham Neubig, Kyunghyun Cho, \n+1 author V. Li\nPublished in Conference of the European\u2026 3 October 2016\nComputer Science, Linguistics\n\n\n\nTLDR\nA neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed.", "Machine Translation Aizhan ImankulovaMasahiro KanekoTosho HirasawaMamoru Komachi Computer Science WMT 2020 TLDR The experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency, and verified the importance of visual information during decoding by performing an adversarial evaluation ofMSNMT. Expand 15[PDF] 2 Excerpts Save Simultaneous Neural Machine Translation using Connectionist Temporal Classification Katsuki ChousaKatsuhito SudohSatoshi Nakamura Computer Science ArXiv 2019 TLDR A neural machine translation method is proposed to determine when to start the translation for observed inputs so far in an adaptive manner and introduces a special token ' ', which is generated when the translation model chooses to read the next input token instead of generating an output token. Expand\n\n\n\n\n5[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nTranslation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation\nAshkan AlinejadHassan S. ShavaraniAnoop Sarkar\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work proposes a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences.", "Skip to search formSkip to main contentSkip to account menu DOI:10.18653/V1/E17-1099 Corpus ID: 2782776 Learning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} } Graham Neubig, Kyunghyun Cho, +1 author V. Li Published in Conference of the European\u2026 3 October 2016 Computer Science, Linguistics TLDR A neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed. Expand\n\n\n[PDF] Semantic Reader\n\n\n\n\n\n\n\n\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n\n\n\n209 Citations\n\n\n\nHighly Influential Citations\n36\n\n\n\nBackground Citations\n142\n\n\n Methods Citations\n73\n\n\n Results Citations\n4\nView All\n\n\n\n\n\nFigures from this paper\n\n\n\n\n\n\n\nfigure 1\n\n\n\nfigure 2\n\n\n\nfigure 3\n\n\n\nfigure 4\n\n\n\nfigure 5\n\n\n\nfigure 6\n\n\n\nfigure 7\n\n\n\nfigure 8\n\nView All 8 Figures & Tables\n\n\n\n\nTopics\n\n\nAI-Generated\n\n\nSimultaneous Translation (opens in a new tab)Simultaneous Machine Translation (opens in a new tab)Translation Delays (opens in a new tab)Simultaneous Translation Task (opens in a new tab)READ/WRITE Decision (opens in a new tab)Simultaneous Translation System (opens in a new tab)Average Proportion (opens in a new tab)Simultaneous Mt (opens in a new tab)Neural Machine Translation (opens in a new tab)Translate (opens in a new tab)\n\n\n\n\n209 Citations\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nA General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation\nYun ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nAACL\n2020\n\n\n\nTLDR\nA general framework for adapting neural machine translation to translate simultaneously and contains a prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation."]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nLearning to Translate in Real-time with Neural Machine Translation\nJiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\n\nAnthology ID:\nE17-1099\nVolume:\nProceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\nMonth:\nApril\nYear:\n2017\nAddress:\nValencia, Spain\nEditors:\nMirella Lapata, Phil Blunsom, Alexander Koller\nVenue:\nEACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1053\u20131062\nLanguage:\n\nURL:\nhttps://aclanthology.org/E17-1099/\nDOI:\n\nBibkey:\n\ngu-etal-2017-learning\nCite (ACL):\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. 2017. Learning to Translate in Real-time with Neural Machine Translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1053\u20131062, Valencia, Spain. Association for Computational Linguistics.\n\nCite (Informal):\nLearning to Translate in Real-time with Neural Machine Translation (Gu et al., EACL 2017)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/E17-1099.pdf\nCode\n nyu-dl/dl4mt-simul-trans\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{gu-etal-2017-learning, title = \"Learning to Translate in Real-time with Neural Machine Translation\", author = \"Gu, Jiatao and Neubig, Graham and Cho, Kyunghyun and Li, Victor O.K.\", editor = \"Lapata, Mirella and Blunsom, Phil and Koller, Alexander\", booktitle = \"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\", month = apr, year = \"2017\", address = \"Valencia, Spain\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/E17-1099/\", pages = \"1053--1062\", abstract = \"Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"gu-etal-2017-learning\"> <titleInfo> <title>Learning to Translate in Real-time with Neural Machine Translation</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Jiatao</namePart> <namePart type=\"family\">Gu</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Graham</namePart> <namePart type=\"family\">Neubig</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Kyunghyun</namePart> <namePart type=\"family\">Cho</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Victor</namePart> <namePart type=\"given\">O.K.</namePart> <namePart type=\"family\">Li</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2017-04</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Mirella</namePart> <namePart type=\"family\">Lapata</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Phil</namePart> <namePart type=\"family\">Blunsom</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Alexander</namePart> <namePart type=\"family\">Koller</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Valencia, Spain</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.</abstract> <identifier type=\"citekey\">gu-etal-2017-learning</identifier> <location> <url>https://aclanthology.org/E17-1099/</url> </location> <part> <date>2017-04</date> <extent unit=\"page\"> <start>1053</start> <end>1062</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Learning to Translate in Real-time with Neural Machine Translation %A Gu, Jiatao %A Neubig, Graham %A Cho, Kyunghyun %A Li, Victor O.K. %Y Lapata, Mirella %Y Blunsom, Phil %Y Koller, Alexander %S Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers %D 2017 %8 April %I Association for Computational Linguistics %C Valencia, Spain %F gu-etal-2017-learning %X Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. %U https://aclanthology.org/E17-1099/ %P 1053-1062 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Learning to Translate in Real-time with Neural Machine Translation](https://aclanthology.org/E17-1099/) (Gu et al., EACL 2017)\n\nLearning to Translate in Real-time with Neural Machine Translation (Gu et al., EACL 2017)\nACL\n\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. 2017. Learning to Translate in Real-time with Neural Machine Translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1053\u20131062, Valencia, Spain. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Learning to Translate in Real-time with Neural Machine Translation - ACL Anthology", "description": "Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long <strong>Papers</strong>. 2017.", "usage": {"tokens": 2413}, "retrieved_text": [" ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nLearning to Translate in Real-time with Neural Machine Translation\nJiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting.", "ACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Learning to Translate in Real-time with Neural Machine Translation Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit AbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.", "MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/E17-1099.pdf Code nyu-dl/dl4mt-simul-trans PDF Cite Search Code Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{gu-etal-2017-learning, title = \"Learning to Translate in Real-time with Neural Machine Translation\", author = \"Gu, Jiatao and Neubig, Graham and Cho, Kyunghyun and Li, Victor O.K. \", editor = \"Lapata, Mirella and Blunsom, Phil and Koller, Alexander\", booktitle = \"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\", month = apr, year = \"2017\", address = \"Valencia, Spain\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/E17-1099/\", pages = \"1053--1062\", abstract = \"Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting."]}], "The paper titled \"Unsupervised Natural Language Generation with Denoising Autoencoders\" proposes a method for unsupervised natural language generation using denoising autoencoders, which can be used for tasks such as text summarization and paraphrasing.": [{"text": " \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1804.07899 (cs)   \n \n \n [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] \nTitle:Unsupervised Natural Language Generation with Denoising Autoencoders \nAuthors:Markus Freitag, Scott Roy \nView a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF \n Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.   \n   Comments: Accepted at EMNLP 2018   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1804.07899 [cs.CL]     (or  arXiv:1804.07899v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1804.07899\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Markus Freitag [view email] \n [v1] Sat, 21 Apr 2018 06:16:57 UTC (39 KB)\n [v2] Fri, 24 Aug 2018 19:53:33 UTC (40 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2018-04 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Markus Freitag\nScott Roy  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1804.07899] Unsupervised Natural Language Generation with Denoising Autoencoders", "description": "<strong>Generating</strong> <strong>text</strong> from structured data is important for various <strong>tasks</strong> <strong>such</strong> <strong>as</strong> question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled <strong>text</strong>, we are able to build <strong>a</strong> <strong>Natural</strong> <strong>Language</strong> <strong>Generation</strong> (NLG) system with higher performance ...", "usage": {"tokens": 1206}, "retrieved_text": [" \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1804.07899 (cs)   \n \n \n [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] \nTitle:Unsupervised Natural Language Generation with Denoising Autoencoders \nAuthors:Markus Freitag, Scott Roy \nView a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF \n Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.", "Computer Science > Computation and Language arXiv:1804.07899 (cs) [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] Title:Unsupervised Natural Language Generation with Denoising Autoencoders Authors:Markus Freitag, Scott Roy View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. Comments: Accepted at EMNLP 2018   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1804.07899 [cs.CL]     (or  arXiv:1804.07899v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1804.07899\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Markus Freitag [view email] \n [v1] Sat, 21 Apr 2018 06:16:57 UTC (39 KB)\n [v2] Fri, 24 Aug 2018 19:53:33 UTC (40 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2018-04 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Markus Freitag\nScott Roy  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)", "(What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Natural Language Generation with Denoising Autoencoders\nMarkus Freitag, Scott Roy\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractGenerating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.\n\nAnthology ID:\nD18-1426\nVolume:\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\nMonth:\nOctober-November\nYear:\n2018\nAddress:\nBrussels, Belgium\nEditors:\nEllen Riloff, David Chiang, Julia Hockenmaier, Jun\u2019ichi Tsujii\nVenue:\nEMNLP\nSIG:\nSIGDAT\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n3922\u20133929\nLanguage:\n\nURL:\nhttps://aclanthology.org/D18-1426/\nDOI:\n10.18653/v1/D18-1426\nBibkey:\n\nfreitag-roy-2018-unsupervised\nCite (ACL):\nMarkus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics.\n\nCite (Informal):\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/D18-1426.pdf\nPDF Cite Search  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"freitag-roy-2018-unsupervised\"> <titleInfo> <title>Unsupervised Natural Language Generation with Denoising Autoencoders</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Markus</namePart> <namePart type=\"family\">Freitag</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Scott</namePart> <namePart type=\"family\">Roy</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2018-oct-nov</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Ellen</namePart> <namePart type=\"family\">Riloff</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">David</namePart> <namePart type=\"family\">Chiang</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Julia</namePart> <namePart type=\"family\">Hockenmaier</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Jun\u2019ichi</namePart> <namePart type=\"family\">Tsujii</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Brussels, Belgium</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.</abstract> <identifier type=\"citekey\">freitag-roy-2018-unsupervised</identifier> <identifier type=\"doi\">10.18653/v1/D18-1426</identifier> <location> <url>https://aclanthology.org/D18-1426/</url> </location> <part> <date>2018-oct-nov</date> <extent unit=\"page\"> <start>3922</start> <end>3929</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Unsupervised Natural Language Generation with Denoising Autoencoders %A Freitag, Markus %A Roy, Scott %Y Riloff, Ellen %Y Chiang, David %Y Hockenmaier, Julia %Y Tsujii, Jun\u2019ichi %S Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing %D 2018 %8 oct nov %I Association for Computational Linguistics %C Brussels, Belgium %F freitag-roy-2018-unsupervised %X Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. %R 10.18653/v1/D18-1426 %U https://aclanthology.org/D18-1426/ %U https://doi.org/10.18653/v1/D18-1426 %P 3922-3929 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Unsupervised Natural Language Generation with Denoising Autoencoders](https://aclanthology.org/D18-1426/) (Freitag & Roy, EMNLP 2018)\n\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\nACL\n\nMarkus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Unsupervised Natural Language Generation with Denoising Autoencoders - ACL Anthology", "description": "Markus Freitag, Scott Roy. Proceedings of the 2018 Conference on Empirical <strong>Methods</strong> <strong>in</strong> <strong>Natural</strong> <strong>Language</strong> Processing. 2018.", "usage": {"tokens": 2340}, "retrieved_text": ["for Computational Linguistics Note: Pages: 3922\u20133929 Language: URL: https://aclanthology.org/D18-1426/ DOI: 10.18653/v1/D18-1426 Bibkey: freitag-roy-2018-unsupervised Cite (ACL): Markus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal): Unsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/D18-1426.pdf PDF Cite Search Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches.", " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Natural Language Generation with Denoising Autoencoders\nMarkus Freitag, Scott Roy\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractGenerating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence.", "corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. Anthology ID: D18-1426 Volume: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing Month: October-November Year: 2018 Address: Brussels, Belgium Editors: Ellen Riloff, David Chiang, Julia Hockenmaier, Jun\u2019ichi Tsujii Venue: EMNLP SIG: SIGDAT Publisher: Association for Computational Linguistics Note: Pages: 3922\u20133929 Language: URL: https://aclanthology.org/D18-1426/ DOI: 10.18653/v1/D18-1426 Bibkey: freitag-roy-2018-unsupervised Cite (ACL): Markus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal):\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/D18-1426.pdf\nPDF Cite Search  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems."]}, {"text": "       \n  \n \n Home   \n Publications     \n \n \nUnsupervised Natural Language Generation with Denoising Autoencoders \n \n Markus Freitag  \n \n Scott Roy  \n \n Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929   \n   Download   Google Scholar  \n  \n Copy Bibtex         \n \n \nAbstract  \n Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network.     \n \n \nResearch Areas  \n \n \n  \n \n \n \n Natural Language Processing   \n         \n  \n \n \n \n Machine Intelligence   \n              \n \n \nMeet the teams driving innovation \nOur teams advance the state of the art through research, systems engineering, and collaboration across Google.  See our teams   \n \n      \n \n  \n \n Follow us   \n \n     \n     \n     \n          \n   \n     \n \n  About Google   \n  Google Products   \n  Privacy   \n  Terms    \n \n   Help   \n \n Submit feedback            ", "title": "Unsupervised Natural Language Generation with Denoising Autoencoders", "description": "We strive to create an environment conducive to many different types of research across many different time scales and levels of risk \u00b7 Our researchers drive advancements in computer science through both fundamental and applied research", "usage": {"tokens": 368}, "retrieved_text": ["how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network. Research Areas Natural Language Processing Machine Intelligence Meet the teams driving innovation Our teams advance the state of the art through research, systems engineering, and collaboration across Google. See our teams   \n \n      \n \n  \n \n Follow us   \n \n     \n     \n     \n          \n   \n     \n \n  About Google   \n  Google Products   \n  Privacy   \n  Terms    \n \n   Help   \n \n Submit feedback", "Home Publications Unsupervised Natural Language Generation with Denoising Autoencoders Markus Freitag Scott Roy Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929 Download Google Scholar Copy Bibtex Abstract Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network. Research Areas  \n \n \n  \n \n \n \n Natural Language Processing   \n         \n  \n \n \n \n Machine Intelligence   \n              \n \n \nMeet the teams driving innovation \nOur teams advance the state of the art through research, systems engineering, and collaboration across Google.", "       \n  \n \n Home   \n Publications     \n \n \nUnsupervised Natural Language Generation with Denoising Autoencoders \n \n Markus Freitag  \n \n Scott Roy  \n \n Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929   \n   Download   Google Scholar  \n  \n Copy Bibtex         \n \n \nAbstract  \n Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network."]}, {"warning": "Target URL returned error 403: Forbidden", "text": " \n     \n \n\n\n\n\n\n\n\nWe've noticed some unusual traffic coming from your network. To continue, please check the box below.\n\n\nEnable JavaScript and cookies to continue\n\n\n\n\n\nAre you a researcher?\nTo avoid being denied access, log in if you're a ResearchGate member or create an account if you're not.\n\n\n\n\nLog in\nCreate account\n\n\nRay ID: 8ffd044408f86176\nClient IP: 2600:1900:0:2d01::3e01\n\u00a9 2008-2025 ResearchGate GmbH. All rights reserved.  ", "title": "Unsupervised Natural Language Generation with Denoising ...", "description": "Access 160+ million publication pages and connect with 25+ million researchers. Join for free and gain visibility by uploading your research.", "usage": {"tokens": 141}, "retrieved_text": ["Access 160+ million publication pages and connect with 25+ million researchers. Join for free and gain visibility by uploading your research."]}, {"text": "     \n     \n   \n \n \n \n    \n   \n  Browse State-of-the-Art   \n  Datasets   \n Methods  \n  More  \n Newsletter RC2022 \n About Trends  Portals   Libraries     \n \n     \n Sign In      \n \n \n \n \nSubscribe to the PwC Newsletter \n \u00d7   \n \n \n Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.\n\n Read previous issues     \n \nSubscribe       \n \n \n \n \nJoin the community \n \u00d7   \n You need to log in to edit.\n You can create a new account if you don't have one.\n\n       \n \n \n \n \nEdit Social Preview \n \u00d7   \n    \n Description\n \n  Default  \n  Custom   \n \n Implemented in one code library.  \n    \n Image\n \n \n  Default  \n  Custom  \n  None    \n \n   \n \n File is too large  \n Upload an image to customize your repository\u2019s social media preview.\n Images should be at least 640\u00d7320px (1280\u00d7640px for best display).    \n    \n \n Close  \n Save       \n \n \n \n \nAdd a new code entry for this paper \n \u00d7   \n \n  \n  GitHub, GitLab or BitBucket URL:*  \n    \n \n   Official code from paper authors     \n \nSubmit        \n \n \n \n \nRemove a code repository from this paper \n \u00d7   \n \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n    \n \n   \n-            \n \n \n \n \n Mark the official implementation from paper authors  \n \u00d7   \n   \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n    \n     \n  \n \n \n There is no official implementation   \n      \n \n \n Multiple official implementations   \n \n        \n \nSubmit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n  \n  Task:*  \n  --------- Sentence Text Generation Denoising Question Answering    \n Not in the list? Add a task.  \n  Dataset:*  \n  ---------    \n  Model name:*  \n    \n  Metric name:*  \n  ---------    \n  \n \n   Higher is better (for the metric)    \n  Metric value:*  \n    \n  \n \n   Uses extra training data    \n  Data evaluated on  \n     \n \n Submit        \n \n \n \n \nAdd a new evaluation result row \n \u00d7   \n \n \n   TASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE          \n \n \n \n \nAdd a task \n \u00d7   \n \nAttached tasks: \n \n  DENOISING    \n \n  QUESTION ANSWERING    \n \n  SENTENCE    \n \n  TEXT GENERATION    \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new task.  \n \n  New task name:  \n    \n  Top-level area:  \n  --------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series    \n  Parent task (if any):  \n  ---------    \n  Description:  \n     \n \n Submit         \n \n \n \n \nRemove a task \n \u00d7   \n \n \n \n \n    Denoising     \n-     \n \n \n    Question Answering     \n-     \n \n \n    Sentence     \n-     \n \n \n    Text Generation     \n-           \n \n \n \n \nAdd a method \n \u00d7   \n \n  \n  Add:  \n  ---------    \n Not in the list?\n   Create a new method.  \n \n  New method name (e.g. ReLU):  \n    \n  New method full name (e.g. Rectified Linear Unit):  \n    \n  Paper where method was first introduced:  \n  ---------    \n  Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.  \n  ---------    \n  Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.  \n     \n \n Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? \n   Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Natural Language Generation with Denoising Autoencoders  \n \n   EMNLP 2018    \u00b7   Markus Freitag,  Scott Roy \u00b7 \n  Edit social preview       \n \n \n \n Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.    PDF    Abstract     EMNLP 2018 PDF     EMNLP 2018 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n      \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n        \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Denoising      Question Answering      Sentence      Text Generation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.       \n \n \n    \n \n \n \n \n \n Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com .  Papers With Code is a free resource with all data licensed under CC-BY-SA.   \n Terms Data policy Cookies policy  from       ", "title": "Unsupervised Natural Language Generation with Denoising Autoencoders | Papers With Code", "description": "Upload an image to customize your repository\u2019s social media preview. Images should be at least 640\u00d7320px (1280\u00d7640px for best display) \u00b7 EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview", "usage": {"tokens": 1322}, "retrieved_text": ["true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Natural Language Generation with Denoising Autoencoders EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. PDF    Abstract     EMNLP 2018 PDF     EMNLP 2018 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n      \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n        \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Denoising      Question Answering      Sentence      Text Generation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.", "data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. PDF Abstract EMNLP 2018 PDF EMNLP 2018 Abstract Code Edit Add Remove Mark official mcleonard/NLG_Autoencoder 16 mcleonard/NLG_Autoencoder 16 Tasks Edit Add Remove Denoising Question Answering Sentence Text Generation Datasets Edit Add Datasets introduced or used in this paper Results from the Paper Edit Submit results from this paper to get state-of-the-art GitHub badges and help the community compare results to other papers. Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com . Papers With Code is a free resource with all data licensed under CC-BY-SA.", "Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 Edit Datasets \u00d7 Add or remove datasets introduced in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Natural Language Generation with Denoising Autoencoders EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches."]}], "The paper titled \"Neural Machine Translation of Rare Words with Subword Units\" discusses the use of subword units for neural machine translation, which improves translation quality for words that are rare or out-of-vocabulary.": [{"text": " \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.  \n \nI Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.   \n   Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)   \n \nDBLP - CS Bibliography \n listing | bibtex  \n Rico Sennrich\nBarry Haddow\nAlexandra Birch  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1508.07909] Neural Machine Translation of Rare Words with Subword Units", "description": "<strong>Neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) models typically operate with a fixed <strong>vocabulary</strong>, <strong>but</strong> <strong>translation</strong> is an open-<strong>vocabulary</strong> problem. Previous work addresses <strong>the</strong> <strong>translation</strong> <strong>of</strong> <strong>out</strong>-<strong>of</strong>-<strong>vocabulary</strong> <strong>words</strong> by backing off to a dictionary. In this <strong>paper</strong>, we introduce a simpler and more effective approach, ...", "usage": {"tokens": 1451}, "retrieved_text": [" \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).", "Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1508.07909 (cs) [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] Title:Neural Machine Translation of Rare Words with Subword Units Authors:Rico Sennrich, Barry Haddow, Alexandra Birch View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.", "by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively. Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)"]}, {"text": " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nNeural Machine Translation of Rare Words with Subword Units\nRico Sennrich, Barry Haddow, Alexandra Birch\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF.\nTitle Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF.\n\nAdd Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. \n\n ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\nAnthology ID:\nP16-1162\nVolume:\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\nMonth:\nAugust\nYear:\n2016\nAddress:\nBerlin, Germany\nEditors:\nKatrin Erk, Noah A. Smith\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1715\u20131725\nLanguage:\n\nURL:\nhttps://aclanthology.org/P16-1162/\nDOI:\n10.18653/v1/P16-1162\nBibkey:\n\nsennrich-etal-2016-neural\nCite (ACL):\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\n\nCite (Informal):\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/P16-1162.pdf\nSoftware:\n  P16-1162.Software.zip\nCode\n rsennrich/subword-nmt +  additional community code\nData\nWMT 2015\nPDF Cite Search  Code  Software  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", editor = \"Erk, Katrin and Smith, Noah A.\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/P16-1162/\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\" } \nDownload as File \nCopy to Clipboard\n <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"sennrich-etal-2016-neural\"> <titleInfo> <title>Neural Machine Translation of Rare Words with Subword Units</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Rico</namePart> <namePart type=\"family\">Sennrich</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Barry</namePart> <namePart type=\"family\">Haddow</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Alexandra</namePart> <namePart type=\"family\">Birch</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2016-08</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Katrin</namePart> <namePart type=\"family\">Erk</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Noah</namePart> <namePart type=\"given\">A</namePart> <namePart type=\"family\">Smith</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Berlin, Germany</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <identifier type=\"citekey\">sennrich-etal-2016-neural</identifier> <identifier type=\"doi\">10.18653/v1/P16-1162</identifier> <location> <url>https://aclanthology.org/P16-1162/</url> </location> <part> <date>2016-08</date> <extent unit=\"page\"> <start>1715</start> <end>1725</end> </extent> </part> </mods> </modsCollection> \nDownload as File \nCopy to Clipboard\n %0 Conference Proceedings %T Neural Machine Translation of Rare Words with Subword Units %A Sennrich, Rico %A Haddow, Barry %A Birch, Alexandra %Y Erk, Katrin %Y Smith, Noah A. %S Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) %D 2016 %8 August %I Association for Computational Linguistics %C Berlin, Germany %F sennrich-etal-2016-neural %R 10.18653/v1/P16-1162 %U https://aclanthology.org/P16-1162/ %U https://doi.org/10.18653/v1/P16-1162 %P 1715-1725 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/) (Sennrich et al., ACL 2016)\n\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\nACL\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\n\nCopy Markdown to Clipboard \nCopy ACL to Clipboard\n\n\n ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License.\nThe ACL Anthology is managed and built by the ACL Anthology team of volunteers.\nSite last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", "title": "Neural Machine Translation of Rare Words with Subword Units - ACL Anthology", "description": "Rico Sennrich, Barry Haddow, Alexandra Birch. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long <strong>Papers</strong>). 2016.", "usage": {"tokens": 1787}, "retrieved_text": ["1715-1725 Download as File Copy to Clipboard Markdown (Informal) [Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/) (Sennrich et al., ACL 2016) Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016) ACL Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License. The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 09 January 2025 at 14:24 UTC with commit 61a2621.", " ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nNeural Machine Translation of Rare Words with Subword Units\nRico Sennrich, Barry Haddow, Alexandra Birch\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\nAnthology ID:\nP16-1162\nVolume:\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\nMonth:\nAugust\nYear:\n2016\nAddress:\nBerlin, Germany\nEditors:\nKatrin Erk, Noah A. Smith\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1715\u20131725\nLanguage:\n\nURL:\nhttps://aclanthology.org/P16-1162/\nDOI:\n10.18653/v1/P16-1162\nBibkey:\n\nsennrich-etal-2016-neural\nCite (ACL):\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.", "ACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Neural Machine Translation of Rare Words with Subword Units Rico Sennrich, Barry Haddow, Alexandra Birch Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit Anthology ID: P16-1162 Volume: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Month: August Year: 2016 Address: Berlin, Germany Editors: Katrin Erk, Noah A. Smith Venue: ACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 1715\u20131725 Language: URL: https://aclanthology.org/P16-1162/ DOI: 10.18653/v1/P16-1162 Bibkey: sennrich-etal-2016-neural Cite (ACL): Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. Cite (Informal):\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/P16-1162.pdf\nSoftware:\n  P16-1162.Software.zip\nCode\n rsennrich/subword-nmt +  additional community code\nData\nWMT 2015\nPDF Cite Search  Code  Software  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", editor = \"Erk, Katrin and Smith, Noah A."]}, {"text": " \n \n Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. \n  Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    rsennrich   /  subword-nmt  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 464   \n \n   Star  2.2k      \n \n \n Unsupervised Word Segmentation for Neural Machine Translation and Text Generation  \nLicense \n   MIT license   \n   2.2k stars    464 forks    Branches    Tags    Activity   \n \n \n   Star    \n  Notifications  You must be signed in to change notification settings        \n \n   Code   \n   Issues 3  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Wiki   \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Wiki    \n      Security    \n      Insights             \n \nrsennrich/subword-nmt \n \n \n \n              \n   \n\n\n\n\n\n\n\n\n\n master\n\nBranchesTags\n\n\n\n\n\n\nGo to file\n\n\nCode\n\n\n\n\n\n\nFolders and filesNameName\nLast commit message\nLast commit date\n\nLatest commit\n \n\n\n\nHistory144 Commits\n\n\n\n\n\n.github/workflows\n\n\n\n.github/workflows\n \n \n\n\n\nsubword_nmt\n\n\n\nsubword_nmt\n \n \n\n\n\n.gitignore\n\n\n\n.gitignore\n \n \n\n\n\nCHANGELOG.md\n\n\n\nCHANGELOG.md\n \n \n\n\n\nLICENSE\n\n\n\nLICENSE\n \n \n\n\n\nREADME.md\n\n\n\nREADME.md\n \n \n\n\n\napply_bpe.py\n\n\n\napply_bpe.py\n \n \n\n\n\nget_vocab.py\n\n\n\nget_vocab.py\n \n \n\n\n\nlearn_bpe.py\n\n\n\nlearn_bpe.py\n \n \n\n\n\nlearn_joint_bpe_and_vocab.py\n\n\n\nlearn_joint_bpe_and_vocab.py\n \n \n\n\n\nsetup.py\n\n\n\nsetup.py\n \n \n\nView all files\n\n\n\nRepository files navigation\n\nREADME\nMIT license\n\n\n\n\nSubword Neural Machine Translation \nThis repository contains preprocessing scripts to segment text into subword units. The primary purpose is to facilitate the reproduction of our experiments on Neural Machine Translation with subword units (see below for reference). \n\nINSTALLATION \ninstall via pip (from PyPI): \npip install subword-nmt  \ninstall via pip (from Github): \npip install https://github.com/rsennrich/subword-nmt/archive/master.zip  \nalternatively, clone this repository; the scripts are executable stand-alone. \n\nUSAGE INSTRUCTIONS \nCheck the individual files for usage instructions. \nTo apply byte pair encoding to word segmentation, invoke these commands: \nsubword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file} subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}  \nTo segment rare words into character n-grams, do the following: \nsubword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file} subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}  \nThe original segmentation can be restored with a simple replacement: \nsed -r 's/(@@ )|(@@ ?$)//g'  \nIf you cloned the repository and did not install a package, you can also run the individual commands as scripts: \n./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}  \n\nBEST PRACTICE ADVICE FOR BYTE PAIR ENCODING IN NMT \nWe found that for languages that share an alphabet, learning BPE on the concatenation of the (two or more) involved languages increases the consistency of segmentation, and reduces the problem of inserting/deleting characters when copying/transliterating names. \nHowever, this introduces undesirable edge cases in that a word may be segmented in a way that has only been observed in the other language, and is thus unknown at test time. To prevent this, apply_bpe.py accepts a --vocabulary and a --vocabulary-threshold option so that the script will only produce symbols which also appear in the vocabulary (with at least some frequency). \nTo use this functionality, we recommend the following recipe (assuming L1 and L2 are the two languages): \nLearn byte pair encoding on the concatenation of the training text, and get resulting vocabulary for each: \ncat {train_file}.L1 {train_file}.L2 | subword-nmt learn-bpe -s {num_operations} -o {codes_file} subword-nmt apply-bpe -c {codes_file} < {train_file}.L1 | subword-nmt get-vocab > {vocab_file}.L1 subword-nmt apply-bpe -c {codes_file} < {train_file}.L2 | subword-nmt get-vocab > {vocab_file}.L2  \nmore conventiently, you can do the same with with this command: \nsubword-nmt learn-joint-bpe-and-vocab --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2  \nre-apply byte pair encoding with vocabulary filter: \nsubword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L1 --vocabulary-threshold 50 < {train_file}.L1 > {train_file}.BPE.L1 subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L2 --vocabulary-threshold 50 < {train_file}.L2 > {train_file}.BPE.L2  \nas a last step, extract the vocabulary to be used by the neural network. Example with Nematus: \nnematus/data/build_dictionary.py {train_file}.BPE.L1 {train_file}.BPE.L2  \n[you may want to take the union of all vocabularies to support multilingual systems] \nfor test/dev data, re-use the same options for consistency: \nsubword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L1 --vocabulary-threshold 50 < {test_file}.L1 > {test_file}.BPE.L1  \n\nADVANCED FEATURES \nOn top of the basic BPE implementation, this repository supports: \n \n \nBPE dropout (Provilkov, Emelianenko and Voita, 2019): https://arxiv.org/abs/1910.13267 use the argument --dropout 0.1 for subword-nmt apply-bpe to randomly drop out possible merges. Doing this on the training corpus can improve quality of the final system; at test time, use BPE without dropout. In order to obtain reproducible results, argument --seed can be used to set the random seed. \nNote: In the original paper, the authors used BPE-Dropout on each new batch separately. You can copy the training corpus several times to get similar behavior to obtain multiple segmentations for the same sentence.  \n \nsupport for glossaries: use the argument --glossaries for subword-nmt apply-bpe to provide a list of subwords and/or regular expressions that should always be passed to the output without subword segmentation   \necho \"I am flying to <country>Switzerland</country> at noon .\" | subword-nmt apply-bpe --codes subword_nmt/tests/data/bpe.ref I am fl@@ y@@ ing to <@@ coun@@ tr@@ y@@ >@@ S@@ w@@ it@@ z@@ er@@ l@@ and@@ <@@ /@@ coun@@ tr@@ y@@ > at no@@ on . echo \"I am flying to <country>Switzerland</country> at noon .\" | subword-nmt apply-bpe --codes subword_nmt/tests/data/bpe.ref --glossaries \"<country>\\w*</country>\" \"fly\" I am fly@@ ing to <country>Switzerland</country> at no@@ on .  \n \nbyte-level BPE: while BPE uses characters as basic units in Sennrich et al., 2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file.  \n\nPUBLICATIONS \nThe segmentation methods are described in: \n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and  Haddow, Barry and  Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } \nThe best practice advice is described in: \n@inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and  Birch, Alexandra and  Currey, Anna and  Germann, Ulrich and  Haddow, Barry and  Heafield, Kenneth and  Miceli Barone, Antonio Valerio and  Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } \n\nHOW IMPLEMENTATION DIFFERS FROM Sennrich et al. (2016) \nThis repository implements the subword segmentation as described in Sennrich et al. (2016), but since version 0.2, there is one core difference related to end-of-word tokens. \nIn Sennrich et al. (2016), the end-of-word token </w> is initially represented as a separate token, which can be merged with other subwords over time: \nu n d </w> f u n d </w>  \nSince 0.2, end-of-word tokens are initially concatenated with the word-final character: \nu n d</w> f u n d</w>  \nThe new representation ensures that when BPE codes are learned from the above examples and then applied to new text, it is clear that a subword unit und is unambiguously word-final, and un is unambiguously word-internal, preventing the production of up to two different subword units from each BPE merge operation. \napply_bpe.py is backward-compatible and continues to accept old-style BPE files. New-style BPE files are identified by having the following first line: #version: 0.2 \n\nACKNOWLEDGMENTS \nThis project has received funding from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland, and from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 645452 (QT21).        \n \n \n \n \n \nAbout \n Unsupervised Word Segmentation for Neural Machine Translation and Text Generation  \nTopics \n \n  machine-translation   segmentation   neural-machine-translation   nmt   subword-units   bpe    \nResources \n   Readme   \nLicense \n   MIT license     \n  Activity  \nStars \n  2.2k stars  \nWatchers \n  55 watching  \nForks \n  464 forks  \n  Report repository      \n \n \n Releases 1   \n \n 0.3.8  Latest   \nDec 8, 2021     \n \n \n Packages 0 \n No packages published \n    \n \n     \n \n \n Contributors 14 \n \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n        \n \n \nLanguages \n    \n \n   Python 100.0%               \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time.      \n     \n    \n \n    \n            \n            \n \n ", "title": "GitHub - rsennrich/subword-nmt: Unsupervised Word Segmentation for Neural Machine Translation and Text Generation", "description": "Unsupervised <strong>Word</strong> Segmentation <strong>for</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> and Text Generation - rsennrich/<strong>subword</strong>-nmt", "usage": {"tokens": 2711}, "retrieved_text": ["2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file. PUBLICATIONS The segmentation methods are described in: @inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } The best practice advice is described in: @inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and Birch, Alexandra and Currey, Anna and Germann, Ulrich and Haddow, Barry and Heafield, Kenneth and Miceli Barone, Antonio Valerio and Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } HOW IMPLEMENTATION DIFFERS FROM Sennrich et al. (2016) \nThis repository implements the subword segmentation as described in Sennrich et al.", "--codes subword_nmt/tests/data/bpe.ref --glossaries \"<country>\\w*</country>\" \"fly\" I am fly@@ ing to <country>Switzerland</country> at no@@ on . byte-level BPE: while BPE uses characters as basic units in Sennrich et al., 2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file. PUBLICATIONS \nThe segmentation methods are described in: \n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and  Haddow, Barry and  Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } \nThe best practice advice is described in: \n@inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and  Birch, Alexandra and  Currey, Anna and  Germann, Ulrich and  Haddow, Barry and  Heafield, Kenneth and  Miceli Barone, Antonio Valerio and  Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } \n\nHOW IMPLEMENTATION DIFFERS FROM Sennrich et al.", "setup.py setup.py View all files Repository files navigation README MIT license Subword Neural Machine Translation This repository contains preprocessing scripts to segment text into subword units. The primary purpose is to facilitate the reproduction of our experiments on Neural Machine Translation with subword units (see below for reference). INSTALLATION install via pip (from PyPI): pip install subword-nmt install via pip (from Github): pip install https://github.com/rsennrich/subword-nmt/archive/master.zip alternatively, clone this repository; the scripts are executable stand-alone. USAGE INSTRUCTIONS Check the individual files for usage instructions. To apply byte pair encoding to word segmentation, invoke these commands: \nsubword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file} subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}  \nTo segment rare words into character n-grams, do the following: \nsubword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file} subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}  \nThe original segmentation can be restored with a simple replacement: \nsed -r 's/(@@ )|(@@ ?$)//g'  \nIf you cloned the repository and did not install a package, you can also run the individual commands as scripts: \n./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}  \n\nBEST PRACTICE ADVICE FOR BYTE PAIR ENCODING IN NMT \nWe found that for languages that share an alphabet, learning BPE on the concatenation of the (two or more) involved languages increases the consistency of segmentation, and reduces the problem of inserting/deleting characters when copying/transliterating names."]}, {"text": " \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy.  \n \nI Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909v5 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.   \n   Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)   \n \nDBLP - CS Bibliography \n listing | bibtex  \n Rico Sennrich\nBarry Haddow\nAlexandra Birch  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)   \n \n    Connected Papers Toggle   \n Connected Papers (What is Connected Papers?)  \n \n    Litmaps Toggle   \n Litmaps (What is Litmaps?)   \n \n    scite.ai Toggle   \n scite Smart Citations (What are Smart Citations?)    \n \n \n \n   Code, Data, Media \n \nCode, Data and Media Associated with this Article \n \n \n    alphaXiv Toggle   \n alphaXiv (What is alphaXiv?)   \n \n    Links to Code Toggle   \n CatalyzeX Code Finder for Papers (What is CatalyzeX?)   \n \n    DagsHub Toggle   \n DagsHub (What is DagsHub?)   \n \n    GotitPub Toggle   \n Gotit.pub (What is GotitPub?)   \n \n    Huggingface Toggle   \n Hugging Face (What is Huggingface?)   \n \n    Links to Code Toggle   \n Papers with Code (What is Papers with Code?)   \n \n    ScienceCast Toggle   \n ScienceCast (What is ScienceCast?)    \n \n \n \n \n \n \n \n   Demos \n \nDemos \n \n \n    Replicate Toggle   \n Replicate (What is Replicate?)   \n \n    Spaces Toggle   \n Hugging Face Spaces (What is Spaces?)   \n \n    Spaces Toggle   \n TXYZ.AI (What is TXYZ.AI?)    \n \n \n   Related Papers \n \nRecommenders and Search Tools \n \n \n    Link to Influence Flower   \n Influence Flower (What are Influence Flowers?)   \n \n    Core recommender toggle   \n CORE Recommender (What is CORE?)   \n \n \n \nAuthor \nVenue \nInstitution \nTopic  \n \n \n \n \n   \n \n    About arXivLabs  \n \n \n \narXivLabs: experimental projects with community collaborators \narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.  \n \n       \n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)       \n \n  \n \n \n \n \nAbout \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack             ", "title": "[1508.07909v5] Neural Machine Translation of Rare Words with Subword Units", "description": "<strong>Neural</strong> <strong>machine</strong> <strong>translation</strong> (NMT) models typically operate with a fixed <strong>vocabulary</strong>, <strong>but</strong> <strong>translation</strong> is an open-<strong>vocabulary</strong> problem. Previous work addresses <strong>the</strong> <strong>translation</strong> <strong>of</strong> <strong>out</strong>-<strong>of</strong>-<strong>vocabulary</strong> <strong>words</strong> by backing off to a dictionary. In this <strong>paper</strong>, we introduce a simpler and more effective approach, ...", "usage": {"tokens": 1453}, "retrieved_text": [" \n \n \n \nChange to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909v5 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).", "Change to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1508.07909v5 (cs) [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] Title:Neural Machine Translation of Rare Words with Subword Units Authors:Rico Sennrich, Barry Haddow, Alexandra Birch View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.", "by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively. Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)"]}, {"warning": "Target URL returned error 429: Too Many Requests", "text": "www.researchgate.net\nVerifying you are human. This may take a few seconds.\n\n\n\n\n\n\n\nwww.researchgate.net needs to review the security of your connection before proceeding.\n\nVerification successful\nWaiting for www.researchgate.net to respond...\nEnable JavaScript and cookies to continue\n\n\n\nRay ID: 8ffd053bbf0b8114\nPerformance & security by Cloudflare", "title": "Neural Machine Translation of Rare Words with Subword Units | Request PDF", "description": "Request PDF | On Jan 1, 2016, Rico Sennrich and others published <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> <strong>of</strong> <strong>Rare</strong> <strong>Words</strong> <strong>with</strong> <strong>Subword</strong> <strong>Units</strong> | Find, read and cite all the research you need on ResearchGate", "usage": {"tokens": 173}, "retrieved_text": ["Request PDF | On Jan 1, 2016, Rico Sennrich and others published <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> <strong>of</strong> <strong>Rare</strong> <strong>Words</strong> <strong>with</strong> <strong>Subword</strong> <strong>Units</strong> | Find, read and cite all the research you need on ResearchGate"]}]}, "claim_verification_result": [{"claim": "The paper titled \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\" presents a method for improving part-of-speech tagging across multiple languages by using adversarial training to generate more robust models.", "search_results": "Evidence 1\nTitle: [1711.04903] Robust Multilingual Part-of-Speech Tagging via Adversarial Training\nContent: Computer Science > Computation and Language  \n arXiv:1711.04903 (cs)   \n \n \n [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] \nTitle:Robust Multilingual Part-of-Speech Tagging via Adversarial Training \nAuthors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev \nView a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF \n Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations.\nComputer Science > Computation and Language arXiv:1711.04903 (cs) [Submitted on 14 Nov 2017 (v1), last revised 20 Apr 2018 (this version, v2)] Title:Robust Multilingual Part-of-Speech Tagging via Adversarial Training Authors:Michihiro Yasunaga, Jungo Kasai, Dragomir Radev View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors View PDF Abstract:Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks.\nfor rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. Comments: NAACL 2018   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1711.04903 [cs.CL]     (or  arXiv:1711.04903v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1711.04903\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Michihiro Yasunaga [view email] \n [v1] Tue, 14 Nov 2017 01:50:30 UTC (359 KB)\n [v2] Fri, 20 Apr 2018 15:49:22 UTC (187 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Robust Multilingual Part-of-Speech Tagging via Adversarial Training, by Michihiro Yasunaga and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2017-11 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Michihiro Yasunaga\nJungo Kasai\nDragomir R. Radev  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)\n\nEvidence 2\nTitle: Robust Multilingual Part-of-Speech Tagging via Adversarial Training - ACL Anthology\nContent: (ACL): Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics. Cite (Informal): Robust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/N18-1089.pdf Code michiyasunaga/pos_adv Data CoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies PDF Cite Search Code Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing.\nYear: 2018 Address: New Orleans, Louisiana Editors: Marilyn Walker, Heng Ji, Amanda Stent Venue: NAACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 976\u2013986 Language: URL: https://aclanthology.org/N18-1089/ DOI: 10.18653/v1/N18-1089 Bibkey: yasunaga-etal-2018-robust Cite (ACL): Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 976\u2013986, New Orleans, Louisiana. Association for Computational Linguistics. Cite (Informal):\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/N18-1089.pdf\nCode\n michiyasunaga/pos_adv\nData\nCoNLL, CoNLL 2003, CoNLL-2000, Penn Treebank, Universal Dependencies\nPDF Cite Search  Code  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{yasunaga-etal-2018-robust, title = \"Robust Multilingual Part-of-Speech Tagging via Adversarial Training\", author = \"Yasunaga, Michihiro and Kasai, Jungo and Radev, Dragomir\", editor = \"Walker, Marilyn and Ji, Heng and Stent, Amanda\", booktitle = \"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)\", month = jun, year = \"2018\", address = \"New Orleans, Louisiana\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/N18-1089/\", doi = \"10.18653/v1/N18-1089\", pages = \"976--986\", abstract = \"Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations.\n%C New Orleans, Louisiana %F yasunaga-etal-2018-robust %X Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare / unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. %R 10.18653/v1/N18-1089 %U https://aclanthology.org/N18-1089/ %U https://doi.org/10.18653/v1/N18-1089 %P 976-986 \nDownload as File \nCopy to Clipboard\n\nMarkdown (Informal)\n[Robust Multilingual Part-of-Speech Tagging via Adversarial Training](https://aclanthology.org/N18-1089/) (Yasunaga et al., NAACL 2018)\n\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training (Yasunaga et al., NAACL 2018)\nACL\n\nMichihiro Yasunaga, Jungo Kasai, and Dragomir Radev.\n\nEvidence 3\nTitle: Robust Multilingual Part-of-Speech Tagging via Adversarial Training \u2013 arXiv Vanity\nContent: Robust Multilingual Part-of-Speech Tagging via Adversarial Training \n  Michihiro Yasunaga Jungo Kasai Dragomir Radev \nDepartment of Computer Science, Yale University \n  {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu    \n \nAbstract \nAdversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. 1 Introduction \n \nFigure 1: Illustration of our architecture for adversarial POS tagging.\nRobust Multilingual Part-of-Speech Tagging via Adversarial Training Michihiro Yasunaga Jungo Kasai Dragomir Radev Department of Computer Science, Yale University {michihiro.yasunaga,jungo.kasai,dragomir.radev}@yale.edu Abstract Adversarial training (AT)111We distinguish AT from Generative Adversarial Networks (GANs). is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits AT. In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages), we find that AT not only improves the overall tagging accuracy, but also 1) prevents over-fitting well in low resource languages and 2) boosts tagging accuracy for rare \u200b/\u200b unseen words. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word representations. 5) The proposed AT model is generally effective in different sequence labeling tasks. These positive results motivate further use of AT for natural language tasks. 1 Introduction Figure 1: Illustration of our architecture for adversarial POS tagging. Given a sentence, we input the normalized word embeddings (\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0,\ud835\udc98\ud835\udfd1subscript\ud835\udc981subscript\ud835\udc982subscript\ud835\udc983\\mbox{\\boldmath$\\uline{w}_{1}$},\\mbox{\\boldmath$\\uline{w}_{2}$},\\mbox{\\boldmath$\\uline{w}_{3}$}) and character embeddings (showing \ud835\udc84\ud835\udfcf,\ud835\udc84\ud835\udfd0,\ud835\udc84\ud835\udfd1subscript\ud835\udc841subscript\ud835\udc842subscript\ud835\udc843\\mbox{\\boldmath$\\uline{c}_{1}$},\\mbox{\\boldmath$\\uline{c}_{2}$},\\mbox{\\boldmath$\\uline{c}_{3}$} for w1subscript\ud835\udc641w_{1}).\ncore problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training. With a BiLSTM-CRF model Huang et al. (2015); Ma and Hovy (2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word \u200b/\u200b character embeddings. In order to demystify the effects of adversarial training in the context of NLP, we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus (Englsih) and the Universal Dependencies dataset (27 languages), with thorough analyses of the following points: \u2022 Effects on different target languages \u2022 Vocabulary statistics and tagging accuracy \u2022 Influence on downstream tasks \u2022 Representation learning of words In our experiments, we find that our adversarial training model consistently outperforms the baseline POS tagger, and even achieves state-of-the-art results on 22 languages. Furthermore, our analyses reveal the following insights into adversarial training in the context of NLP:  \n \n \n \u2022 \n \nThe regularization effects of adversarial training (AT) are general across different languages. AT can prevent overfitting especially well when training examples are scarce, providing an effective tool to process low resource languages.\n\nEvidence 4\nTitle: DL-NLP-Readings/Robust Multilingual Part-of-Speech Tagging via ...\nContent: Skip to content         \n     \n  You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert   \n \n  \n \n \n     \n \n{{ message }}      \n \n  \n \n \n \n    26hzhang   /  DL-NLP-Readings  Public   \n \n \n  Notifications  You must be signed in to change notification settings  \n  Fork 268   \n \n   Star  850      \n   \n \n   Code   \n   Issues 0  \n   Pull requests 0  \n   Actions   \n   Projects 0  \n   Security   \n   Insights    \n   \n  Additional navigation options  \n \n  \n \n \n      Code    \n      Issues    \n      Pull requests    \n      Actions    \n      Projects    \n      Security    \n      Insights             \n   \n       \n \nFooter \n \n     \u00a9 2025 GitHub, Inc.    \nFooter navigation \n \n Terms  \n Privacy  \n Security  \n Status  \n Docs  \n Contact  \n  \n Manage cookies    \n  \n Do not share my personal information         \n  \n   You can\u2019t perform that action at this time.\n\nEvidence 5\nTitle: Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training\nContent: 2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Yin et al. (2023) Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. Alcuna: large language models meet new knowledge. arXiv preprint arXiv:2310.14820. Yoran et al. (2023) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558. Yu et al. (2023) Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210. Zhu et al. (2019)  Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2019. Freelb: Enhanced adversarial training for natural language understanding. arXiv preprint arXiv:1909.11764. Zhu et al. (2023)  Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107. Zou et al. (2023)  Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.\n(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wang et al. (2020) Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020. Infobert: Improving robustness of language models from an information theoretic perspective. arXiv preprint arXiv:2010.02329. Wu et al. (2017) Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial training for relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778\u20131783. Yasunaga et al. (2017) Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. 2017. Robust multilingual part-of-speech tagging via adversarial training. arXiv preprint arXiv:1711.04903. Yin et al. (2023)  Xunjian Yin, Baizhou Huang, and Xiaojun Wan. 2023. Alcuna: large language models meet new knowledge. arXiv preprint arXiv:2310.14820. Yoran et al. (2023)  Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558. Yu et al. (2023)  Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210.\nthe min-max paradigm of adversarial training (Morris et al., 2020; Ivgi and Berant, 2021). Moreover, RAAT utilizes multi-task learning (Ruder, 2017) to encourage the LLMs to generate tokens that are aware of noises, thereby enabling the model to internally recognize retrieved noisy contexts and improve the overall generation performance. The main contributions of this paper can be summarized as follows: \u2022 We systematically explore three types of retrieval noises and investigate the sensitivity of LLMs to these diverse types of noises. \u2022 \n \nWe propose a novel adaptive adversarial training method (called RAAT) to enhance the robustness of RALMs against various retrieval noises. RAAT dynamically adjusts the training process of the model in diverse noise environments. In addition, it integrates multi-task learning to encourage the model to improve its ability to discern different types of noises. \u2022 \n \nWe set up a benchmark (named RAG-Bench) for assessing the noise robustness problem of RALMs based on three open-domain question-answering datasets. Experimental results demonstrate that our RAAT method enhances robustness across diverse retrieval noise environments. 2 Related Work  \nRetrieval-Augmented Generation with Noisy Context \n \nRetrieval-Augmented Language Models (RALMs) have shown impressive performance in various NLP tasks (Gao et al., 2023; Zhu et al., 2023).\n\n", "verification_result": "supported"}, {"claim": "The paper titled \"Unsupervised Cross-lingual Representation Learning at Scale\" discusses an unsupervised approach to learning cross-lingual representations that can be used for tasks such as transfer learning and zero-shot learning.", "search_results": "Evidence 1\nTitle: [1911.02116] Unsupervised Cross-lingual Representation Learning at Scale\nContent: Change to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1911.02116 (cs)   \n \n \n [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] \nTitle:Unsupervised Cross-lingual Representation Learning at Scale \nAuthors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \nView a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF \n Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.\nChange to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1911.02116 (cs) [Submitted on 5 Nov 2019 (v1), last revised 8 Apr 2020 (this version, v2)] Title:Unsupervised Cross-lingual Representation Learning at Scale Authors:Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov View a PDF of the paper titled Unsupervised Cross-lingual Representation Learning at Scale, by Alexis Conneau and 9 other authors View PDF Abstract:This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.\nToggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack\n\nEvidence 2\nTitle: Unsupervised Cross-lingual Representation Learning at Scale - ACL Anthology\nContent: ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.\nACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit AbstractThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.\n(ACL): Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics. Cite (Informal): Unsupervised Cross-lingual Representation Learning at Scale (Conneau et al., ACL 2020) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/2020.acl-main.747.pdf Video: http://slideslive.com/38928776 Code facebookresearch/XLM + additional community code Data CC100, CoNLL 2002, GLUE, MLQA, QNLI, SQuAD, SST PDF Cite Search Code Video Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{conneau-etal-2020-unsupervised, title = \"Unsupervised Cross-lingual Representation Learning at Scale\", author = \"Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin\", editor = \"Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel\", booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\", month = jul, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2020.acl-main.747/\", doi = \"10.18653/v1/2020.acl-main.747\", pages = \"8440--8451\", abstract = \"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data.\n\nEvidence 3\nTitle: (PDF) Unsupervised Cross-lingual Representation Learning at Scale (2020) | Alexis Conneau | 3721 Citations\nContent: (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available. read more\nShow Related Papers\n\n\n\n\nChat with Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCitations\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nMAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\nJonas Pfeiffer,Ivan Vuli\u0107,Iryna Gurevych,Sebastian Ruder +3 moreTechnische Universit\u00e4t Darmstadt,University of Mannheim,Google\n - 30 Apr 2020\n\nShow Less\n\n\n\nTL;DR: This paper proposed MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations, and introduced a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language....read moreread less\n\n169\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Journal Article\u2022DOI\nPre-Trained Models: Past, Present and Future\nXu Han,Zhengyan Zhang,Ning Ding,Yuxian Gu,Xiao Liu,Yuqi Huo,Jiezhong Qiu,Liang Zhang,Wentao Han,Minlie Huang,Qin Jin,Yanyan Lan,Yang Liu,Zhiyuan Liu,Zhiwu Lu,Xipeng Qiu,Ruihua Song,Jie Tang,Ji-Rong Wen,Jinhui Yuan,Wayne Xin Zhao,Jun Zhu +21 moreTsinghua University,Renmin University of China,Fudan University\n - 14 Jun 2021\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum....read moreread less\n\n135\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nAdapterHub: A Framework for Adapting Transformers\nJonas Pfeiffer,Andreas R\u00fcckl\u00e9,Clifton Poth,Aishwarya Kamath,Ivan Vuli\u0107,Sebastian Ruder,Kyunghyun Cho,Iryna Gurevych +7 moreTechnische Universit\u00e4t Darmstadt,New York University,University of Oslo,Google,Canadian Institute for Advanced Research\n - 15 Jul 2020\n\nShow Less\n\n\n\nTL;DR: In this paper, the authors propose a framework that allows dynamic \"stiching-in\" of pre-trained adapters for different tasks and languages, which enables extremely easy and quick adaptation of state-of-the-art pre-training models across tasks....read moreread less\n\n122\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nFlauBERT: Unsupervised Language Model Pre-training for French\nHang Le,Lo\u00efc Vial,Jibril Frej,Vincent Segonne,Maximin Coavoux,Benjamin Lecouteux,Alexandre Allauzen,Beno\u00eet Crabb\u00e9,Laurent Besacier,Didier Schwab +9 moreUniversity of Grenoble,Centre national de la recherche scientifique\n - 11 Dec 2019\n\nShow Less\n\n\n\nTL;DR: The authors proposed FlauBERT, a model learned on a very large and heterogeneous French corpus and applied it to various NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and showed that most of the time they outperformed other pre-training approaches....read moreread less\n\n120\nPodcast\n\n\n\n\n\n\n\n\n\u2022Posted Content\nmT5: A massively multilingual pre-trained text-to-text transformer\nLinting Xue,Noah Constant,Adam Roberts,Mihir Kale,Rami Al-Rfou,Aditya Siddhant,Aditya Barua,Colin Raffel +7 moreGoogle,University of Chester\n - 22 Oct 2020\n - arXiv: Computation and Language \nShow Less\n\n\n\nTL;DR: This article proposed a multilingual variant of T5, mT5, which was pre-trained on a new Common Crawl-based dataset covering 101 languages and achieved state-of-the-art performance on many multilingual benchmarks....read moreread less\n\n99\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n...\n\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\nSort by: Citation Count\n\n\n\n\n\nPDF\n\n\n\nOpen Access\n More filters\n\n\n\n\n\n\nProceedings Article\u2022DOI\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 moreGoogle\n - 11 Oct 2018\n\nShow Less\n\n\n\nTL;DR: BERT as mentioned in this paper pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks....read moreread less\n\n24.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nDistributed Representations of Words and Phrases and their Compositionality\nTomas Mikolov,Ilya Sutskever,Kai Chen,Greg S. Corrado,Jeffrey Dean +4 moreGoogle\n - 05 Dec 2013\n\nShow Less\n\n\n\nTL;DR: This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling....read moreread less\n\n24K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nDeep contextualized word representations\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 moreAllen Institute for Artificial Intelligence,University of Massachusetts Amherst,Google,University of Washington\n - 15 Feb 2018\n\nShow Less\n\n\n\nTL;DR: This paper introduced a new type of deep contextualized word representation that models both complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic contexts (i.e., to model polysemy)....read moreread less\n\n7.4K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\u2022DOI\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nPranav Rajpurkar,Jian Zhang,Konstantin Lopyrev,Percy Liang +3 moreStanford University\n - 16 Jun 2016\n\nShow Less\n\n\n\nTL;DR: The Stanford Question Answering Dataset (SQuAD) as mentioned in this paper is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage....read moreread less\n\n3.6K\nPodcast\n\n\n\n\n\n\n\n\n\u2022Proceedings Article\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nAlex Wang,Amanpreet Singh,Julian Michael,Felix Hill,Omer Levy,Samuel R. Bowman +5 moreNew York University,University of Washington,Google\n - 20 Apr 2018\n\nShow Less\n\n\n\nTL;DR: A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks....read moreread less\n\n2.1K\nPDF\nPodcast\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n...\n\n\n\n\n\nRelated Papers (5)\n\n\n\n\n\n\n\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n[...]\nJacob Devlin,Ming-Wei Chang,Kenton Lee,Kristina Toutanova +3 more\n - 11 Oct 2018\n\nShow Less\n\n\n\n\nDeep contextualized word representations\n[...]\nMatthew E. Peters,Mark Neumann,Mohit Iyyer,Matt Gardner,Christopher Clark,Kenton Lee,Luke Zettlemoyer +6 more\n - 15 Feb 2018\n\nShow Less\n\n\n\n\nBeto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\n[...]\nShijie Wu,Mark Dredze +1 more\n - 19 Apr 2019\n\nShow Less\n\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n[...]\nColin Raffel,Noam Shazeer,Adam Roberts,Katherine Lee,Sharan Narang,Michael Matena,Yanqi Zhou,Wei Li,Peter J. Liu +8 more\n - 01 Jan 2020\n - Journal of Machine Learning Research \nShow Less\n\n\n\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\n[...]\nTaku Kudo,John Richardson +1 more\n - 19 Aug 2018\n\nShow Less\n\n\n\n\n\n\nTools\nChat with PDFLiterature ReviewAI WriterFind TopicsParaphraserCitation GeneratorExtract DataAI DetectorCitation BoosterExtensions\nSciSpace Chrome Extension\nSciSpace\nAboutCareersResourcesSupportBrowse PapersPricingSciSpace Affiliate ProgramCancellation & Refund PolicyTermsPrivacyData Sources\nDirectories\nPapersTopicsJournalsAuthorsConferencesInstitutionsCitation Styles\nContact\nsupport@typeset.io+1 (760) 284-7800+91 9916292973\n\n\u00a9 2025 | PubGenius Inc. | Suite # 217 691 S Milpitas Blvd Milpitas CA 95035, USA\nHome Chat with PDF Literature Review AI Writer Find Topics Paraphraser Citation Generator Extract Data AI Detector PDF to Video Affiliate Program Chrome Extension Use on ChatGPT Contact Us Home Papers Unsupervised Cross-lingual Representation Learning at Scale Open AccessProceedings ArticleDOI Unsupervised Cross-lingual Representation Learning at Scale Alexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University - 01 Jul 2020 - pp 8440-8451 Show Less Podcast 3.7K PDF Save Cite TL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time. read more Abstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.\nHome\n\nChat with PDF\n\nLiterature Review\n\nAI Writer\n\nFind Topics\n\nParaphraser\n\nCitation Generator\n\nExtract Data\n\nAI Detector\n\nPDF to Video\n\nAffiliate Program\n\nChrome Extension\n\n\nUse on ChatGPT\n\n\nContact Us\n\n\n\n\nHome\nPapers\nUnsupervised Cross-lingual Representation Learning at Scale\n\n\n\nOpen AccessProceedings ArticleDOI\nUnsupervised Cross-lingual Representation Learning at Scale\nAlexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary,Guillaume Wenzek,Francisco Guzm\u00e1n,Edouard Grave,Myle Ott,Luke Zettlemoyer,Veselin Stoyanov +9 moreFacebook,Microsoft,Johns Hopkins University\n - 01 Jul 2020\n\n - pp 8440-8451\nShow Less\n\n\n\nPodcast\n3.7K\nPDF\n\nSave\nCite\n\n\n\n\nTL;DR: It is shown that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks, and the possibility of multilingual modeling without sacrificing per-language performance is shown for the first time. read more\n\n\nAbstract: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.\n\nEvidence 4\nTitle: dblp: Unsupervised Cross-lingual Representation Learning at Scale.\nContent: conf/acl/ConneauKGCWGGOZ20 ask others Google Google Scholar Semantic Scholar Internet Archive Scholar CiteSeerX PubPeer share record Twitter Reddit BibSonomy LinkedIn Facebook persistent URL: https://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451 Please note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public. For more information please see the Initiative for Open Citations (I4OC). Please also note that there is no way of submitting missing references or citation data directly to dblp. Please also note that this feature is work in progress and that it is still far from being perfect. That is, in particular,\n\nthe lists below may be incomplete due to unavailable citation data,\nreference strings may not have been successfully mapped to the items listed in dblp, and\nwe do not have complete and curated metadata for all items given in these lists.\nuses your data. So please proceed with care and consider checking the information given by OpenAlex. Your decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time. looking up data no open data available, sorry service temporarily unavailable, please try again later contained concepts recent citations by year view electronic edition via DOI (open access) electronic edition @ aclanthology.org (open access) authority control: export record BibTeX RIS RDF N-Triples RDF Turtle RDF/XML XML dblp key: conf/acl/ConneauKGCWGGOZ20 ask others Google Google Scholar Semantic Scholar Internet Archive Scholar CiteSeerX PubPeer share record Twitter Reddit BibSonomy LinkedIn Facebook persistent URL: https://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: Unsupervised Cross-lingual Representation Learning at Scale. ACL 2020: 8440-8451\n\nPlease note: Providing information about references and citations is only possible thanks to to the open metadata APIs provided by crossref.org and opencitations.net. If citation data of your publications is not openly available yet, then please consider asking your publisher to release your citation data to the public.\npublications citing venues co-cited venues or build your own? DOI: 10.18653/V1/2020.ACL-MAIN.747 access: open type: Conference or Workshop Paper metadata version: 2022-03-16 load more data from openalex.org Privacy notice: By enabling the option above, your browser will contact the API of openalex.org to load information about this publication. Although we do not have any reason to believe that your call will be tracked, we do not have any control over how the remote server uses your data. So please proceed with care and consider checking the information given by OpenAlex. Your decision whether to load or to not load citation data from external APIs will be stored by us in a cookie with your web browser and can be revoked at any time. looking up data   \nno open data available, sorry\nservice temporarily unavailable, please try again later\n\ncontained concepts\n\n\nrecent citations by year\n   \n\n\n\n\n\n\n\n\nview\n\nelectronic edition via DOI (open access)\nelectronic edition @ aclanthology.org (open access)\nauthority control:\n\n \n\n\n\nexport record\n\nBibTeX\nRIS\nRDF N-Triples\nRDF Turtle\nRDF/XML\nXML\ndblp key:\n\nconf/acl/ConneauKGCWGGOZ20\n\n\n\nask others\n\nGoogle\nGoogle Scholar\nSemantic Scholar\nInternet Archive Scholar\nCiteSeerX\nPubPeer\n\n\n\nshare record\n\nTwitter\nReddit\nBibSonomy\nLinkedIn\nFacebook\npersistent URL:\n\nhttps://dblp.org/rec/conf/acl/ConneauKGCWGGOZ20Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov:\n Unsupervised Cross-lingual Representation Learning at Scale.\n\nEvidence 5\nTitle: Unsupervised Cross-lingual Representation Learning at Scale | Papers With Code\nContent: in this paper: CC100 [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: CoNLL 2002 GLUE MLQA QNLI SQuAD SST [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Cross-lingual Representation Learning at Scale ACL 2020 \u00b7 Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \u00b7 Edit social preview This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.\nRectified Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 ADAM - ATTENTION DROPOUT - BERT - BPE - DENSE CONNECTIONS - DROPOUT - GELU - LAYER NORMALIZATION - LINEAR LAYER - LINEAR WARMUP WITH LINEAR DECAY - MULTI-HEAD ATTENTION - RESIDUAL CONNECTION - SCALED DOT-PRODUCT ATTENTION - SOFTMAX - WEIGHT DECAY - WORDPIECE - XLM - XLM-R - Edit Datasets \u00d7 Add or remove datasets introduced in this paper: CC100 [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: CoNLL 2002 GLUE MLQA QNLI SQuAD SST [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here     \n \n Save        \n  \n \n \n \n Unsupervised Cross-lingual Representation Learning at Scale  \n \n   ACL 2020    \u00b7   Alexis Conneau,  Kartikay Khandelwal,  Naman Goyal,  Vishrav Chaudhary,  Guillaume Wenzek,  Francisco Guzm\u00e1n,  Edouard Grave,  Myle Ott,  Luke Zettlemoyer,  Veselin Stoyanov \u00b7 \n  Edit social preview       \n \n \n \n This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov \u00b7 Edit social preview This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks.\n\n", "verification_result": "supported"}, {"claim": "The paper titled \"Learning to Translate in Real-time with Neural Machine Translation\" describes a method for real-time neural machine translation that can handle streaming inputs without waiting for the entire input sequence.", "search_results": "Evidence 1\nTitle: [1610.00388] Learning to Translate in Real-time with Neural Machine Translation\nContent: Computer Science > Computation and Language  \n arXiv:1610.00388 (cs)   \n \n \n [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] \nTitle:Learning to Translate in Real-time with Neural Machine Translation \nAuthors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li \nView a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF \n Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\nComputer Science > Computation and Language arXiv:1610.00388 (cs) [Submitted on 3 Oct 2016 (v1), last revised 10 Jan 2017 (this version, v3)] Title:Learning to Translate in Real-time with Neural Machine Translation Authors:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors View PDF Abstract:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. Comments: 10 pages, camera ready   Subjects:  Computation and Language (cs.CL); Machine Learning (cs.LG)  Cite as: arXiv:1610.00388 [cs.CL]     (or  arXiv:1610.00388v3 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1610.00388\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Jiatao Gu [view email] \n [v1] Mon, 3 Oct 2016 02:11:03 UTC (1,464 KB)\n [v2] Thu, 6 Oct 2016 00:46:39 UTC (1,467 KB)\n [v3] Tue, 10 Jan 2017 21:07:56 UTC (2,974 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Learning to Translate in Real-time with Neural Machine Translation, by Jiatao Gu and 2 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2016-10 \n Change to browse by: \n cs\n cs.LG\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Jiatao Gu\nGraham Neubig\nKyunghyun Cho\nVictor O. K. Li  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)\n(What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack\n\nEvidence 2\nTitle: Learning to Translate in Real-time with Neural Machine Translation | Request PDF\nContent: Request PDF | On Jan 1, 2017, Jiatao Gu and others published <strong>Learning</strong> <strong>to</strong> <strong>Translate</strong> <strong>in</strong> <strong>Real</strong>-<strong>time</strong> <strong>with</strong> <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> | Find, read and cite all the research you need on ResearchGate\n\nEvidence 3\nTitle: Papers with Code - Learning to Translate in Real-time with Neural Machine Translation\nContent: to Translate in Real-time with Neural Machine Translation EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. PDF    Abstract     EACL 2017 PDF     EACL 2017 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n     \n \n \n \n   nyu-dl/dl4mt-simul-trans  official    \n \n  33   \n \n       \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Machine Translation      NMT      Sentence      Translation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n \n  Edit  \n   Add   Remove      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers. Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper.\nUnit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 Edit Datasets \u00d7 Add or remove datasets introduced in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Learning to Translate in Real-time with Neural Machine Translation EACL 2017 \u00b7 Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li \u00b7 Edit social preview Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment.\ntask \u00d7 Machine Translation - NMT - Sentence - Translation - Add a method \u00d7 Add: --------- Not in the list? Create a new method. New method name (e.g. ReLU): New method full name (e.g. Rectified Linear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit         \n \n \n \n \nRemove a method \n \u00d7   \n \n       \n \n \n     \n \n \n \n \nEdit Datasets \n \u00d7   \n \n  \n  Add or remove datasets introduced in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n  Add or remove other datasets used in this paper:  \n  \n[{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}]   \n Paper introduces a new dataset? Add a new dataset here     \n \n Save        \n  \n \n \n \n Learning to Translate in Real-time with Neural Machine Translation  \n \n   EACL 2017    \u00b7   Jiatao Gu,  Graham Neubig,  Kyunghyun Cho,  Victor O. K. Li \u00b7 \n  Edit social preview       \n \n \n \n Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods.\n\nEvidence 4\nTitle: [PDF] Learning to Translate in Real-time with Neural Machine Translation | Semantic Scholar\nContent: Skip to search formSkip to main contentSkip to account menu\n\n\n\n\n\n\n\n\n\nDOI:10.18653/V1/E17-1099\nCorpus ID: 2782776\nLearning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} }\n\nGraham Neubig, Kyunghyun Cho, \n+1 author V. Li\nPublished in Conference of the European\u2026 3 October 2016\nComputer Science, Linguistics\n\n\n\nTLDR\nA neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed.\nMachine Translation Aizhan ImankulovaMasahiro KanekoTosho HirasawaMamoru Komachi Computer Science WMT 2020 TLDR The experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency, and verified the importance of visual information during decoding by performing an adversarial evaluation ofMSNMT. Expand 15[PDF] 2 Excerpts Save Simultaneous Neural Machine Translation using Connectionist Temporal Classification Katsuki ChousaKatsuhito SudohSatoshi Nakamura Computer Science ArXiv 2019 TLDR A neural machine translation method is proposed to determine when to start the translation for observed inputs so far in an adaptive manner and introduces a special token ' ', which is generated when the translation model chooses to read the next input token instead of generating an output token. Expand\n\n\n\n\n5[PDF]\n\n\n\n\n\n1 Excerpt\n\n\nSave\n\n\nTranslation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation\nAshkan AlinejadHassan S. ShavaraniAnoop Sarkar\nComputer Science\nEMNLP\n2021\n\n\n\nTLDR\nThis work proposes a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences.\nSkip to search formSkip to main contentSkip to account menu DOI:10.18653/V1/E17-1099 Corpus ID: 2782776 Learning to Translate in Real-time with Neural Machine Translation@article{Neubig2016LearningTT, title={Learning to Translate in Real-time with Neural Machine Translation}, author={Graham Neubig and Kyunghyun Cho and Jiatao Gu and Victor O. K. Li}, journal={ArXiv}, year={2016}, volume={abs/1610.00388}, url={https://api.semanticscholar.org/CorpusID:2782776} } Graham Neubig, Kyunghyun Cho, +1 author V. Li Published in Conference of the European\u2026 3 October 2016 Computer Science, Linguistics TLDR A neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment is proposed. Expand\n\n\n[PDF] Semantic Reader\n\n\n\n\n\n\n\n\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n\n\n\n209 Citations\n\n\n\nHighly Influential Citations\n36\n\n\n\nBackground Citations\n142\n\n\n Methods Citations\n73\n\n\n Results Citations\n4\nView All\n\n\n\n\n\nFigures from this paper\n\n\n\n\n\n\n\nfigure 1\n\n\n\nfigure 2\n\n\n\nfigure 3\n\n\n\nfigure 4\n\n\n\nfigure 5\n\n\n\nfigure 6\n\n\n\nfigure 7\n\n\n\nfigure 8\n\nView All 8 Figures & Tables\n\n\n\n\nTopics\n\n\nAI-Generated\n\n\nSimultaneous Translation (opens in a new tab)Simultaneous Machine Translation (opens in a new tab)Translation Delays (opens in a new tab)Simultaneous Translation Task (opens in a new tab)READ/WRITE Decision (opens in a new tab)Simultaneous Translation System (opens in a new tab)Average Proportion (opens in a new tab)Simultaneous Mt (opens in a new tab)Neural Machine Translation (opens in a new tab)Translate (opens in a new tab)\n\n\n\n\n209 Citations\n\n\n\n\n\n\n\n\nCitation Type\n\n\nHas PDF\n\n\nAuthor\n\n\nMore Filters\n\nMore Filters\n\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n\n\n\nA General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation\nYun ChenLiangyou LiXin JiangXiao ChenQun Liu\nComputer Science\nAACL\n2020\n\n\n\nTLDR\nA general framework for adapting neural machine translation to translate simultaneously and contains a prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation.\n\nEvidence 5\nTitle: Learning to Translate in Real-time with Neural Machine Translation - ACL Anthology\nContent: ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nLearning to Translate in Real-time with Neural Machine Translation\nJiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting.\nACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Learning to Translate in Real-time with Neural Machine Translation Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O.K. Li Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit AbstractTranslating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.\nMODS XML Endnote More options\u2026 PDF: https://aclanthology.org/E17-1099.pdf Code nyu-dl/dl4mt-simul-trans PDF Cite Search Code Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{gu-etal-2017-learning, title = \"Learning to Translate in Real-time with Neural Machine Translation\", author = \"Gu, Jiatao and Neubig, Graham and Cho, Kyunghyun and Li, Victor O.K. \", editor = \"Lapata, Mirella and Blunsom, Phil and Koller, Alexander\", booktitle = \"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers\", month = apr, year = \"2017\", address = \"Valencia, Spain\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/E17-1099/\", pages = \"1053--1062\", abstract = \"Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting.\n\n", "verification_result": "supported"}, {"claim": "The paper titled \"Unsupervised Natural Language Generation with Denoising Autoencoders\" proposes a method for unsupervised natural language generation using denoising autoencoders, which can be used for tasks such as text summarization and paraphrasing.", "search_results": "Evidence 1\nTitle: [1804.07899] Unsupervised Natural Language Generation with Denoising Autoencoders\nContent: Computer Science > Computation and Language  \n arXiv:1804.07899 (cs)   \n \n \n [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] \nTitle:Unsupervised Natural Language Generation with Denoising Autoencoders \nAuthors:Markus Freitag, Scott Roy \nView a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF \n Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.\nComputer Science > Computation and Language arXiv:1804.07899 (cs) [Submitted on 21 Apr 2018 (v1), last revised 24 Aug 2018 (this version, v2)] Title:Unsupervised Natural Language Generation with Denoising Autoencoders Authors:Markus Freitag, Scott Roy View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors View PDF Abstract:Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. Comments: Accepted at EMNLP 2018   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1804.07899 [cs.CL]     (or  arXiv:1804.07899v2 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1804.07899\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Markus Freitag [view email] \n [v1] Sat, 21 Apr 2018 06:16:57 UTC (39 KB)\n [v2] Fri, 24 Aug 2018 19:53:33 UTC (40 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Unsupervised Natural Language Generation with Denoising Autoencoders, by Markus Freitag and 1 other authors\nView PDF\nTeX Source\nOther Formats \nview license   \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2018-04 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \nDBLP - CS Bibliography \n listing | bibtex  \n Markus Freitag\nScott Roy  \n a export BibTeX citation Loading...  \n \n \n \nBibTeX formatted citation \u00d7  \n loading...  \n Data provided by:     \n \n\nBookmark          \n \n Bibliographic Tools \n \nBibliographic and Citation Tools \n \n \n    Bibliographic Explorer Toggle   \n Bibliographic Explorer (What is the Explorer?)\n(What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About \nHelp   \n \n \n   Contact  \n   Subscribe        \n \n \n \n \nCopyright \nPrivacy Policy   \n \n \nWeb Accessibility Assistance \n \n arXiv Operational Status \n Get status notifications via email or slack\n\nEvidence 2\nTitle: Unsupervised Natural Language Generation with Denoising Autoencoders - ACL Anthology\nContent: for Computational Linguistics Note: Pages: 3922\u20133929 Language: URL: https://aclanthology.org/D18-1426/ DOI: 10.18653/v1/D18-1426 Bibkey: freitag-roy-2018-unsupervised Cite (ACL): Markus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal): Unsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018) Copy Citation: BibTeX Markdown MODS XML Endnote More options\u2026 PDF: https://aclanthology.org/D18-1426.pdf PDF Cite Search Fix data Export citation \u00d7 BibTeX MODS XML Endnote Preformatted @inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches.\n ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nUnsupervised Natural Language Generation with Denoising Autoencoders\nMarkus Freitag, Scott Roy\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\n\nAbstractGenerating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence.\ncorrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. Anthology ID: D18-1426 Volume: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing Month: October-November Year: 2018 Address: Brussels, Belgium Editors: Ellen Riloff, David Chiang, Julia Hockenmaier, Jun\u2019ichi Tsujii Venue: EMNLP SIG: SIGDAT Publisher: Association for Computational Linguistics Note: Pages: 3922\u20133929 Language: URL: https://aclanthology.org/D18-1426/ DOI: 10.18653/v1/D18-1426 Bibkey: freitag-roy-2018-unsupervised Cite (ACL): Markus Freitag and Scott Roy. 2018. Unsupervised Natural Language Generation with Denoising Autoencoders. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3922\u20133929, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal):\nUnsupervised Natural Language Generation with Denoising Autoencoders (Freitag & Roy, EMNLP 2018)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/D18-1426.pdf\nPDF Cite Search  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{freitag-roy-2018-unsupervised, title = \"Unsupervised Natural Language Generation with Denoising Autoencoders\", author = \"Freitag, Markus and Roy, Scott\", editor = \"Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun{'}ichi\", booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\", month = oct # \"-\" # nov, year = \"2018\", address = \"Brussels, Belgium\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/D18-1426/\", doi = \"10.18653/v1/D18-1426\", pages = \"3922--3929\", abstract = \"Generating text from structured data is important for various tasks such as question answering and dialog systems.\n\nEvidence 3\nTitle: Unsupervised Natural Language Generation with Denoising Autoencoders\nContent: how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network. Research Areas Natural Language Processing Machine Intelligence Meet the teams driving innovation Our teams advance the state of the art through research, systems engineering, and collaboration across Google. See our teams   \n \n      \n \n  \n \n Follow us   \n \n     \n     \n     \n          \n   \n     \n \n  About Google   \n  Google Products   \n  Privacy   \n  Terms    \n \n   Help   \n \n Submit feedback\nHome Publications Unsupervised Natural Language Generation with Denoising Autoencoders Markus Freitag Scott Roy Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929 Download Google Scholar Copy Bibtex Abstract Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network. Research Areas  \n \n \n  \n \n \n \n Natural Language Processing   \n         \n  \n \n \n \n Machine Intelligence   \n              \n \n \nMeet the teams driving innovation \nOur teams advance the state of the art through research, systems engineering, and collaboration across Google.\n       \n  \n \n Home   \n Publications     \n \n \nUnsupervised Natural Language Generation with Denoising Autoencoders \n \n Markus Freitag  \n \n Scott Roy  \n \n Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (2018), pp. 3922-3929   \n   Download   Google Scholar  \n  \n Copy Bibtex         \n \n \nAbstract  \n Generating text from structured data is important for various tasks such as question answering and dialog systems. The task of Natural Language Generation (NLG) is to generate fluent sentences including all of the information given by some structured data. We show that without any supervision and only based on unlabeled text, we are able to build a NLG system with similar performance compared to supervised approaches. In our approach, we treat the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into the training data to build a denoising auto-encoder that is able to generate correct sentences out of structured data. Further, by using bilingual out-of-domain data, we show how to train an unsupervised NLG system that can generate sentences in different languages within one network.\n\nEvidence 4\nTitle: Unsupervised Natural Language Generation with Denoising ...\nContent: Access 160+ million publication pages and connect with 25+ million researchers. Join for free and gain visibility by uploading your research.\n\nEvidence 5\nTitle: Unsupervised Natural Language Generation with Denoising Autoencoders | Papers With Code\nContent: true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Natural Language Generation with Denoising Autoencoders EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. PDF    Abstract     EMNLP 2018 PDF     EMNLP 2018 Abstract     \n \n \n \n \n \nCode \n \n \n  Edit  \n   Add   Remove   Mark official         \n \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n      \n \n \n \n   mcleonard/NLG_Autoencoder    \n \n  16   \n \n        \n \n \n \n \nTasks \n \n \n  Edit  \n   Add   Remove        \n \n \n    Denoising      Question Answering      Sentence      Text Generation         \n \n \n \n \n \nDatasets \n \n \n  Edit        \n \n \n \n       Add Datasets  introduced or used in this paper         \n \n \n \n \n \nResults from the Paper \n \n   Edit      \n    Submit  results from this paper  to get state-of-the-art GitHub badges and help the community compare results to other papers.\ndata is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data. PDF Abstract EMNLP 2018 PDF EMNLP 2018 Abstract Code Edit Add Remove Mark official mcleonard/NLG_Autoencoder 16 mcleonard/NLG_Autoencoder 16 Tasks Edit Add Remove Denoising Question Answering Sentence Text Generation Datasets Edit Add Datasets introduced or used in this paper Results from the Paper Edit Submit results from this paper to get state-of-the-art GitHub badges and help the community compare results to other papers. Methods \n \n \n  Edit  \n   Add   Remove           \n No methods listed for this paper. Add  relevant methods here     \n \n Contact us on:   hello@paperswithcode.com . Papers With Code is a free resource with all data licensed under CC-BY-SA.\nLinear Unit): Paper where method was first introduced: --------- Method category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards. --------- Markdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct. Submit Remove a method \u00d7 Edit Datasets \u00d7 Add or remove datasets introduced in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Add or remove other datasets used in this paper: [{\"type\": \"const\", \"val\": true, \"dst\": \"canonical_only\"}, {\"type\": \"const\", \"val\": true, \"dst\": \"disable_create_option\"}] Paper introduces a new dataset? Add a new dataset here Save Unsupervised Natural Language Generation with Denoising Autoencoders EMNLP 2018 \u00b7 Markus Freitag, Scott Roy \u00b7 Edit social preview Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches.\n\n", "verification_result": "supported"}, {"claim": "The paper titled \"Neural Machine Translation of Rare Words with Subword Units\" discusses the use of subword units for neural machine translation, which improves translation quality for words that are rare or out-of-vocabulary.", "search_results": "Evidence 1\nTitle: [1508.07909] Neural Machine Translation of Rare Words with Subword Units\nContent: Change to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\nChange to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1508.07909 (cs) [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] Title:Neural Machine Translation of Rare Words with Subword Units Authors:Rico Sennrich, Barry Haddow, Alexandra Birch View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.\nby encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively. Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)\n\nEvidence 2\nTitle: Neural Machine Translation of Rare Words with Subword Units - ACL Anthology\nContent: 1715-1725 Download as File Copy to Clipboard Markdown (Informal) [Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162/) (Sennrich et al., ACL 2016) Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016) ACL Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. Copy Markdown to Clipboard Copy ACL to Clipboard ACL materials are Copyright \u00a9 1963\u20132025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License. The ACL Anthology is managed and built by the ACL Anthology team of volunteers. Site last built on 09 January 2025 at 14:24 UTC with commit 61a2621.\n ACL Anthology \n \n\n\nNews(current)\nFAQ(current)\nCorrections(current)\nSubmissions(current)\nGithub\n \n\n\n\nNeural Machine Translation of Rare Words with Subword Units\nRico Sennrich, Barry Haddow, Alexandra Birch\n\n\n\n\nCorrect Metadata for \n \u00d7\n\n\nImportant: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author\nAbstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF\nSubmit\n\n\n\nAnthology ID:\nP16-1162\nVolume:\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\nMonth:\nAugust\nYear:\n2016\nAddress:\nBerlin, Germany\nEditors:\nKatrin Erk, Noah A. Smith\nVenue:\nACL\nSIG:\n\nPublisher:\nAssociation for Computational Linguistics\nNote:\n\nPages:\n1715\u20131725\nLanguage:\n\nURL:\nhttps://aclanthology.org/P16-1162/\nDOI:\n10.18653/v1/P16-1162\nBibkey:\n\nsennrich-etal-2016-neural\nCite (ACL):\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\nACL Anthology News(current) FAQ(current) Corrections(current) Submissions(current) Github Neural Machine Translation of Rare Words with Subword Units Rico Sennrich, Barry Haddow, Alexandra Birch Correct Metadata for \u00d7 Important: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. ALL author names, the title, and the abstract match the PDF Submit Anthology ID: P16-1162 Volume: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Month: August Year: 2016 Address: Berlin, Germany Editors: Katrin Erk, Noah A. Smith Venue: ACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 1715\u20131725 Language: URL: https://aclanthology.org/P16-1162/ DOI: 10.18653/v1/P16-1162 Bibkey: sennrich-etal-2016-neural Cite (ACL): Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. Cite (Informal):\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et al., ACL 2016)\n\nCopy Citation:\n\nBibTeX \nMarkdown \nMODS XML \nEndnote \nMore options\u2026\nPDF:\nhttps://aclanthology.org/P16-1162.pdf\nSoftware:\n  P16-1162.Software.zip\nCode\n rsennrich/subword-nmt +  additional community code\nData\nWMT 2015\nPDF Cite Search  Code  Software  Fix data\n\n\n\n\nExport citation\n \u00d7\n\n\nBibTeX\nMODS XML\nEndnote\nPreformatted\n\n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", editor = \"Erk, Katrin and Smith, Noah A.\n\nEvidence 3\nTitle: GitHub - rsennrich/subword-nmt: Unsupervised Word Segmentation for Neural Machine Translation and Text Generation\nContent: 2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file. PUBLICATIONS The segmentation methods are described in: @inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and Haddow, Barry and Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } The best practice advice is described in: @inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and Birch, Alexandra and Currey, Anna and Germann, Ulrich and Haddow, Barry and Heafield, Kenneth and Miceli Barone, Antonio Valerio and Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } HOW IMPLEMENTATION DIFFERS FROM Sennrich et al. (2016) \nThis repository implements the subword segmentation as described in Sennrich et al.\n--codes subword_nmt/tests/data/bpe.ref --glossaries \"<country>\\w*</country>\" \"fly\" I am fly@@ ing to <country>Switzerland</country> at no@@ on . byte-level BPE: while BPE uses characters as basic units in Sennrich et al., 2016), Radford et al., 2019 use bytes as basic units. This can be enabled with the argument --bytes for subword-nmt learn-bpe. When applying BPE with subword-nmt apply-bpe, no argument is necessary: whether characters or bytes are the basic units is stored in the first line of the BPE file. PUBLICATIONS \nThe segmentation methods are described in: \n@inproceedings{sennrich-etal-2016-neural, title = \"Neural Machine Translation of Rare Words with Subword Units\", author = \"Sennrich, Rico and  Haddow, Barry and  Birch, Alexandra\", booktitle = \"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = aug, year = \"2016\", address = \"Berlin, Germany\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/P16-1162\", doi = \"10.18653/v1/P16-1162\", pages = \"1715--1725\", } \nThe best practice advice is described in: \n@inproceedings{sennrich-etal-2017-university, title = \"The University of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17\", author = \"Sennrich, Rico and  Birch, Alexandra and  Currey, Anna and  Germann, Ulrich and  Haddow, Barry and  Heafield, Kenneth and  Miceli Barone, Antonio Valerio and  Williams, Philip\", booktitle = \"Proceedings of the Second Conference on Machine Translation\", month = sep, year = \"2017\", address = \"Copenhagen, Denmark\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/W17-4739\", doi = \"10.18653/v1/W17-4739\", pages = \"389--399\", } \n\nHOW IMPLEMENTATION DIFFERS FROM Sennrich et al.\nsetup.py setup.py View all files Repository files navigation README MIT license Subword Neural Machine Translation This repository contains preprocessing scripts to segment text into subword units. The primary purpose is to facilitate the reproduction of our experiments on Neural Machine Translation with subword units (see below for reference). INSTALLATION install via pip (from PyPI): pip install subword-nmt install via pip (from Github): pip install https://github.com/rsennrich/subword-nmt/archive/master.zip alternatively, clone this repository; the scripts are executable stand-alone. USAGE INSTRUCTIONS Check the individual files for usage instructions. To apply byte pair encoding to word segmentation, invoke these commands: \nsubword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file} subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}  \nTo segment rare words into character n-grams, do the following: \nsubword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file} subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}  \nThe original segmentation can be restored with a simple replacement: \nsed -r 's/(@@ )|(@@ ?$)//g'  \nIf you cloned the repository and did not install a package, you can also run the individual commands as scripts: \n./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}  \n\nBEST PRACTICE ADVICE FOR BYTE PAIR ENCODING IN NMT \nWe found that for languages that share an alphabet, learning BPE on the concatenation of the (two or more) involved languages increases the consistency of segmentation, and reduces the problem of inserting/deleting characters when copying/transliterating names.\n\nEvidence 4\nTitle: [1508.07909v5] Neural Machine Translation of Rare Words with Subword Units\nContent: Change to arXiv's privacy policy \nThe arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand    \n   \n \n \n \n \nComputer Science > Computation and Language  \n arXiv:1508.07909v5 (cs)   \n \n \n [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] \nTitle:Neural Machine Translation of Rare Words with Subword Units \nAuthors:Rico Sennrich, Barry Haddow, Alexandra Birch \nView a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF \n Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\nChange to arXiv's privacy policy The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. I Understand Computer Science > Computation and Language arXiv:1508.07909v5 (cs) [Submitted on 31 Aug 2015 (v1), last revised 10 Jun 2016 (this version, v5)] Title:Neural Machine Translation of Rare Words with Subword Units Authors:Rico Sennrich, Barry Haddow, Alexandra Birch View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors View PDF Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.\nby encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively. Comments: accepted at ACL 2016; new in this version: figure 3   Subjects:  Computation and Language (cs.CL)  Cite as: arXiv:1508.07909 [cs.CL]     (or  arXiv:1508.07909v5 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.1508.07909\n \n  Focus to learn more   \n  arXiv-issued DOI via DataCite       \n \nSubmission history From: Rico Sennrich [view email] \n [v1] Mon, 31 Aug 2015 16:37:31 UTC (209 KB)\n [v2] Fri, 27 Nov 2015 15:41:25 UTC (122 KB)\n [v3] Thu, 17 Mar 2016 14:56:06 UTC (125 KB)\n [v4] Fri, 3 Jun 2016 15:01:02 UTC (125 KB)\n [v5] Fri, 10 Jun 2016 14:45:08 UTC (197 KB)\n    \n \n  Full-text links: \nAccess Paper: \n \n View a PDF of the paper titled Neural Machine Translation of Rare Words with Subword Units, by Rico Sennrich and 1 other authors\nView PDF\nTeX Source\nOther Formats \n  view license    \n Current browse context: \ncs.CL \n  < prev    |    next > \n \n new  |  recent  | 2015-08 \n Change to browse by: \n cs\n    \n \nReferences & Citations \n \nNASA ADS\nGoogle Scholar \nSemantic Scholar  \n  \n \n \n 4 blog links (what is this?)\n\nEvidence 5\nTitle: Neural Machine Translation of Rare Words with Subword Units | Request PDF\nContent: Request PDF | On Jan 1, 2016, Rico Sennrich and others published <strong>Neural</strong> <strong>Machine</strong> <strong>Translation</strong> <strong>of</strong> <strong>Rare</strong> <strong>Words</strong> <strong>with</strong> <strong>Subword</strong> <strong>Units</strong> | Find, read and cite all the research you need on ResearchGate\n\n", "verification_result": "supported"}], "response_stats": {"sentences": 1, "verified_claims": 5, "verified_supported_claims": 5, "pre_supported_claims": 2, "pre_verified_claims": 2, "all_claims": 7, "P": 1.0}}
